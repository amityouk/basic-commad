
Question 1: Incorrect
When creating a machine learning service to recognize text in images, what type of server should you choose to manage my computing resources?
•	 
virtual machine
(Incorrect)
•	 
cluster of virtual machines
•	 
Professional services are serverless, so they don't use servers
(Correct)
•	 
Virtual machines running Linux only
Explanation
Option 3 is the correct answer.
Google Cloud's dedicated services are serverless. Google manages computing resources for the Service.
There is no need for users to allocate or monitor virtual machines.
Question 2: Correct
What file system technology is Cloud Filestore based on?
•	 
Network File System (NFS)
(Correct)
•	 
XFS
•	 
EXT4
•	 
ReiserFS
Explanation
Option 1 is the correct answer.
Cloud Filestore is based on Network File System (NFS), a distributed file management system. Another option is a filesystem supported by Linux, but not used by Cioud Filestore.
Question 3: 
Skipped
You are using cache because you need to store data. How does caching affect data retrieval?
•	 
Caching improves client-side JavaScript execution
•	 
Cache continues to store data even if power is lost, increasing availability
•	 
Caches can get out of sync with the real system
•	 
Retrieving data from cache is faster than retrieving it from SSD or HDD, so using cache reduces latency
(Correct)
Explanation
Option 4 is the correct answer.
Because cache uses memory, it is the fastest storage type when it comes to reading data.
A cache is a data store on the back end of a distributed system, not on the client. Caching does not affect client-side JavaScript execution.
If power is lost, the data in the cache will not be saved. Data must be reloaded. Caches can get out of sync with the systems that source the information. This is because when such a system is updated, the cache may not be updated. Cache read speed is faster than SSD and HDD.
Question 4: 
Skipped
Why can cloud providers offer flexible resource placement?
•	 
Cloud providers can take resources from low-priority customers and provide them to high-priority customers.
•	 
Because public cloud providers have extensive resources and can move capacity quickly between multiple customers, they can provide flexible resource allocation more efficiently than smaller data centers can.
(Correct)
•	 
The more resources you use, the more expensive your cloud provider will charge you
•	 
Cloud providers don't offer flexible resource placement
Explanation
Option 2 is the correct answer.
Cloud providers can offer large amounts of resources and quickly allocate such resources to different customers. Cloud providers can optimize resource allocation when customers and workloads are mixed.

Option 1 is incorrect. Except for preemptible instances, cloud providers do not take resources from one customer and allocate them to another customer.

Option 3 is incorrect. Cloud providers typically offer discounts as usage increases.
Question 5: 
Skipped
Suppose a transaction occurs in which a client randomly accesses some of the files in a drive attached to a virtual machine. What type of storage system will you use?
•	 
object storage
•	 
block storage
(Correct)
•	 
NoSQL storage
•	 
SSD storage only
Explanation
Option 2 is the correct answer.
The connected drive is a block storage device.

Option 1 is incorrect. When object storage is used in a virtual machine, Cloud Storage buckets are used, but Cloud Storage buckets cannot be directly connected to a virtual machine.

Option 3 is incorrect; NoSQL is a type of data base, not a storage system.

Option 4 is incorrect; you can use HDDs as well as SSDs.
Question 6: 
Skipped
You are deploying a new relational database to support your web application.
What type of storage system will you use to store the database data files?
•	 
object storage
•	 
data storage
•	 
block storage
(Correct)
•	 
cache
Explanation
Option 3 is the correct answer.
Databases require persistent storage on block devices. Object storage does not provide data block or file system storage.
Data storage is not a type of storage system. Caching is commonly used with databases to improve read performance. However, caches are volatile and not suitable for persistent storage of data files.
Question 7: 
Skipped
A user wants a service that requires minimal setup.
Why would you recommend Cloud Storage, App Engine, and Cloud Functions to this user here?
•	 
only charged by time
•	 
because it is serverless
(Correct)
•	 
because the user needs to configure the virtual machine
•	 
It can only run applications written in Go.
Explanation
Option 2 is the correct answer.
All three services are serverless, so users don't need to configure virtual machines. Cloud Storage charges based on the time and size of data stored. App Engine Standard and Cloud Functions are not limited to Go language only.
Question 8: 
Skipped
You are deploying an API on the public internet and are concerned that your service will be the target of a DDoS attack. What Google Cloud services should you consider when securing my APIs?
•	 
Cloud Armor
(Correct)
•	 
Cloud CDN
•	 
Cloud IAM
•	 
VPCs
Explanation
Option 1 is the correct answer.
Cloud Armor is built on top of Google Cloud's load balancing service to allow or restrict access based on IP address, deploy rules to combat cross-site scripting attacks, and protect against SQL injection attacks. or provide

Option 2 is incorrect. Cloud CDN is a content distribution service, not a security service.

Option 3 is incorrect. Identity Access Management is a security service, but it is an authentication and authorization service, not a denial of service attack mitigation.

Option 4 is incorrect. Virtual Private Clouds are used to restrict network access to organizational resources, but do not have features to mitigate denial of service attacks.
Question 9: 
Skipped
Which specialized services are likely to be used to build data warehousing platforms that require complex extraction, transformation, and loading operations on batch data as well as processing streaming data?
•	 
Apigee API platform
•	 
data analysis
(Correct)
•	 
AI and machine learning
•	 
Cloud SDK
Explanation
Option 2 is the correct answer.
Dedicated services for data analytics include products that help extract, transform, load (ETL) and work with both batch and streaming data.
The Apigee API Platform is used to manage APIs and does not address the needs described.

AI and machine learning are useful for analyzing data in data warehouses, but that set of services is not necessarily useful for ETL operations.

The Cloud SDK is used to control the service, but by itself cannot directly perform the required operations.
Question 10: 
Skipped
A software engineer came to you for advice. The software engineer implemented a machine learning algorithm to identify cancer cells in medical images, but the algorithm is computationally intensive, performing many mathematical calculations and requiring immediate access to large amounts of data. and cannot be easily distributed across multiple servers. What kind of Compute Engine configuration would you recommend in this case?
•	 
High memory, high CPU instance
•	 
High memory, high CPU, GPU instances
(Correct)
•	 
Mid-level memory, high CPU instances
•	 
High CPU, GPU instances
Explanation
Option 2 is the correct answer.
Applications that are computationally intensive require high CPU power, but the fact that they are computationally intensive indicates that GPUs should be used.
You could consider running it in a cluster, but it's not easy to distribute the work across multiple servers. For this reason, you need one server that can handle the load. High-memory machine types are recommended for immediate access to large amounts of data.
Question 11: 
Skipped
You need to map your on-premises application's authentication and authorization policies to GCP's authentication and authorization mechanisms.
GCP's documentation states that identities must be authenticated in order to be authorized.
What do you mean by ID here?
•	 
Virtual machine ID
•	 
user
(Correct)
•	 
role
•	 
set of permissions
Explanation
Option 2 is the correct answer.
An identity is an abstraction of a user. Identity also characterizes processes that run on behalf of humans or virtual machines in GCP.

ID is not related to virtual machine ID. A role is a collection of privileges that can be granted to an identity.
Question 12: 
Skipped
A client is developing an application that needs to analyze large amounts of textual information.
The client is not familiar with text mining or manipulating with language. In this case, what Google Cloud services would you recommend for this client?
•	 
Cloud Vision
•	 
Cloud ML
•	 
Cloud Natural Language Processing
(Correct)
•	 
Cloud Text Miner
Explanation
Option 3 is the correct answer.
Cloud Natural Language Processing provides functionality for analyzing text. Cloud Text Miner does not exist. Cloud ML is a general-purpose machine learning service that can be applied to text analysis, but requires knowledge of linguistic processing. Cloud Vision is for image processing.
Question 13: 
Skipped
Which of the following professional services supports both batch and stream data processing workflows?
•	 
Cloud Dataproc
(Correct)
•	 
Big Query
•	 
Cloud Datastore
•	 
AutoML
Explanation
Option 1 is the correct answer.
Dataproc is designed to run workflows in both batch and streaming modes.

Option 2 is incorrect. BigQuery is a data warehouse service.

Option 3 is incorrect. Datastore is a document database.

Option 4 is incorrect. AutoML is a machine learning service.
Question 14: 
Skipped
A product manager at your company reports that customers are complaining about the reliability of an application. The application crashes regularly, but the developer has been unable to find common patterns that trigger crashes.
Developers don't have enough insight into their application's behavior and would like to perform a detailed review of all crash data.
Which Stackdriver tools should you use to view consolidated crash information?
•	 
Cloud Dataproc
•	 
Stackdriver Monitoring
•	 
Stackdriver Logging
•	 
Stackdriver Error Reporting
(Correct)
Explanation
Option 4 is the correct answer.
Error Reporting aggregates crash information. Error Reporting is the correct answer.

Option 1 is incorrect. Dataproc is not part of Stackdriver. This is a managed Hadoop and Spark service.

Option 2 is incorrect. Monitoring collects metrics about application and server performance.

Option 3 is incorrect. Logging is a log management service.
Question 15: 
Skipped
You are designing a cloud application for a healthcare provider. A records management application manages patient medical information.
Only a small percentage of employees have access to this data.
Applications used by the billing department include insurance and payment information. Another group of employees access billing information. In addition, the claims system has two components: the private claims system and the government claims system for public insurance.
Government regulations require software used for public insurance claims to be separate from other software systems. Which of the resource tiers listed below can meet these requirements and provide flexibility as requirements change?
•	 
One organization with folders for records management and billing. Claims folders include folders for private insurance companies and public insurance by the government. Common restrictions are specified in organization-level policies, and other policies are defined in appropriate folders.
(Correct)
•	 
One folder for records management, one folder for billing, and no organizational folders. Define policies at the folder level.
•	 
One organization under the organization with a folder for records management, a folder for private insurance companies, and a folder for public insurance by the government. All restrictions are specified in an organization-level policy, and policy restrictions are set the same for every folder.
•	 
none of the above
Explanation
Option 1 is the correct answer.
You can separate the two main applications into their own folders, and separate individual insurance and government payers in separate folders. This helps meet regulatory requirements to separate government payment software from other software.

Option 2 is incorrect. Option 2 does not include organizations. This is the root of the resource hierarchy.

Option 3 is less flexible in terms of different constraints for different applications.
Question 16: 
Skipped
Constraints are used in resource hierarchy policies. Which of the following types of constraints are allowed?
•	 
Allow a specific set of values
•	 
Reject a specific set of values
•	 
Reject a value and all its child values
•	 
allow all allowed values
•	 
all of the above
(Correct)
Explanation
Option 5 is the correct answer.
All choice types are supported in policies.
Question 17: 
Skipped
An app for a financial institution requires access to a database and a Cloud Storage bucket.
You decide to create a custom role to grant just all the permissions you need.
Which of the following principles should be followed when defining custom roles?
•	 
Duty rotation principle
•	 
principle of least principle
•	 
Principle of defense in depth
•	 
principle of least privilege
(Correct)
Explanation
Option 4 is the correct answer.
Grant users only the privileges they need to perform their responsibilities. This is the principle of least privilege.

Option 1 is incorrect. Rotating responsibilities over a period of time and having different people perform tasks is another security principle.

Option 2 is not a real security principle.

Option 3 is incorrect. Defense in depth is the method of using multiple security controls to protect the same asset.
Question 18: 
Skipped
How many organizations can you create in my resource hierarchy?
•	 
1
(Correct)
•	 
2
•	 
3
•	 
infinite
Explanation
Option 1 is the correct answer.
A resource hierarchy can have only one organization. However, you can create multiple folders and projects in your resource hierarchy.
Question 19: 
Skipped
The development and operations team at your company has decided to use Stackdriver monitoring and logging. You have been asked to set up a Stackdriver workspace.
When setting up a workspace, what kind of resource is the workspace associated with?
•	 
Compute Engine instances only
•	 
Compute Engine instances or Kubernetes Engine clusters only
•	 
Compute Engine instance, Kubernetes Engine cluster, or App Engine app
•	 
project
(Correct)
Explanation
Option 4 is the correct answer.
A Stackdriver workspace is linked to a project. It's not linked to individual resources such as virtual machine instances, clusters, or App Engine apps. Options 1, 2, and 3 all state that workspaces are associated with separate computing resources, which is incorrect.
Question 20: 
Skipped
A large company plans to use GCP in many departments. Each department is independently managed and has its own budget. Most departments will spend tens of thousands of dollars a month. How would you recommend setting up a billing account in this case?
•	 
Use a single self-service account
•	 
Use multiple self-service accounts
•	 
Use a single billing account
•	 
Use multiple billing accounts
(Correct)
Explanation
Option 4 is the correct answer.
Larger companies should use billing accounts when high charges are incurred. Self-service accounts are suitable only for amounts within your credit card allowance. Subdivisions are managed independently and have their own budgets, so each should have its own billing account.
Question 21: 
Skipped
You work for a retailer with many physical stores. Every night, the store uploads daily sales data.
Your task is to create a service that verifies uploads every night. You have decided to use a service account. Your boss has raised questions about the security of your proposed solution, specifically the authentication of service accounts, and you've been asked to explain the authentication mechanism used by service accounts. What is the authentication mechanism used by the service account?
•	 
Username and password
•	 
Two-factor authentication
•	 
encryption key
(Correct)
•	 
biometric authentication
Explanation
Option 3 is the correct answer.
When a service account is created, Google will generate a key for encrypted authentication. Username and password are not options for service accounts. Two-factor authentication is an authentication method that requires two authentication factors, a username/password combination and a code from an authentication device. Biometrics cannot be used with the service and is not an option.
Question 22: 
Skipped
How many projects can you create in my account?
•	 
Ten
•	 
25 cases
•	 
no limit
•	 
Each account has limits determined by Google
(Correct)
Explanation
Option 4 is the correct answer.
The maximum number of organizations is determined by Google on an account-by-account basis. If you need to add more organizations, you can contact Google and request a limit increase.
Question 23: 
Skipped
Your manager is concerned about the fees your department is spending on cloud services. So you suggested that the team use preemptible VMs with one exception. Which of the following is "one thing" here?
•	 
database server
(Correct)
•	 
Batch processing without fixed time requirements for completion
•	 
High Performance Computing Cluster
•	 
none of the above
Explanation
Option 1 is the correct answer.
Database servers require high availability to respond to queries from users or applications. Preemptible machines are guaranteed to shut down within 24 hours.

Option 2 is incorrect. Batch processing jobs without fixed time requirements can use preemptible machines as long as the virtual machine is restarted.

Option 3 is incorrect. High performance computing clusters can use preemptible machines. This is because when a server is shut down, work on preemptible machines can be automatically scheduled to another node in the cluster.
Question 24: 
Skipped
Which file should contain commands to specify packages to install in a Docker container?
•	 
Docker.cfg
•	 
Dockerfile
(Correct)
•	 
Config.dck
•	 
install.cfg
Explanation
Option 2 is the correct answer.
The name of the file used to build and configure a Docker container is Dockerfile.
Question 25: 
Skipped
Your boss is giving a presentation to executives at the company he works for, insisting that they start using Kubernetes Engine.
You encourage your boss to highlight all the features Kubernetes offers that ease the workload of dev ops engineers. So you explained the features that Kubernetes provides. Which of the following is not a feature provided by Kubernetes?
•	 
Load balancing across Compute Engine VMs deployed within a Kuberrietes cluster
•	 
Security scan for vulnerabilities
(Correct)
•	 
Autoscaling nodes in a cluster
•	 
Automatic upgrade of cluster software as needed
Explanation
Option 2 is the correct answer.
Kubernetes provides load balancing, scaling, and automatic software upgrades. It does not provide vulnerability scanning.

GCP has a Cloud Security Scanner product, but it's designed to work with App Engine to identify common application vulnerabilities.
Question 26: 
Skipped
Your team is developing a new pipeline for analyzing data streams from sensors on your device. Older pipelines could corrupt data because parallel threads overwrote data written by other threads.
You decide to use Cloud Functions as part of your pipeline. As a Cloud Functions developer, what should you do to prevent multiple invocations of my function from interfering with each other?
•	 
Include a check in your code to make sure another call isn't running at the same time
•	 
Schedule each call to run in a separate process
•	 
Schedule each call to run in a separate thread
•	 
there is nothing to do. GCP ensures that function calls do not interfere with each other
(Correct)
Explanation
Option 4 is the correct answer.
When you call Cloud Functions, they run in a securely isolated runtime environment. No need to check if other calls are running. The Cloud Functions service provides no means for developers to control code execution at the process or thread level.
Question 27: 
Skipped
A client processes personal and health information for hospitals. All health information must be protected in accordance with government regulations.
This client wants to move their application to Google Cloud, but wants to use the encryption library they used in the past. So you suggested installing the cryptographic library on every VM running your application. What kind of image do you use in this case mosquito?
•	 
custom image
(Correct)
•	 
public image
•	 
CentOS6 or 7
Explanation
Option 1 is the correct answer.
After installing the custom code (cryptographic library in this case), create a custom image. Public images do not contain custom code, but can be used as a basis for adding custom code. Both CentOS and Ubuntu are Linux distributions. You can use it as a base image to add your custom code. Can be used alone, but without custom code.
Question 28: 
Skipped
What is the lowest level in the resource hierarchy?
•	 
folder
•	 
project
(Correct)
•	 
File
•	 
VM instance
Explanation
Option 2 is the correct answer.
Projects are the lowest level in the resource hierarchy. Organizations are at the top of the hierarchy, with folders between organizations and projects. The virtual machine instance has not entered the resource hierarchy.
Question 29: 
Skipped
What is the role that allows me to manage all of my Compute Engine instances called?
•	 
Compute Engine manager role
•	 
Compute Engine admin role
(Correct)
•	 
Compute Engine manager role
•	 
Compute Engine security administrator role
Explanation
Option 2 is the correct answer.
The Compute Engine Admin role is a role that gives users full control over their instances.

Option 1 and Option 3 are non-existent roles. Compute Engine Security Admin gives users the ability to create, modify, and delete SSL certificates and firewall rules.
Question 30: 
Skipped
What is the maximum number of vCPUs for custom VMs?
•	 
16
•	 
32
•	 
64
(Correct)
•	 
128
Explanation
Option 3 is the correct answer. Custom virtual machines can have up to 64 vCPUs.
Question 31: 
Skipped
What is the state of a Kubernetes Deployment?
•	 
progressing, stalled, completed
•	 
progressing, completed, failed
(Correct)
•	 
progressing, stalled, failed, completed
•	 
stalled, running, failed, completed
Explanation
Option 2 is the correct answer.
Kubernetes deployment states are progressing, completed, and failed only.
Question 32: 
Skipped
A client has turned to you to help reduce their development and operations overhead.
Engineers spend too much time patching servers and optimizing server utilization and want to move to serverless platforms whenever possible.
This client has heard of Cloud Functions and wants to use it whenever possible. Which of the following types of applications would you not recommend?
•	 
Long-running data warehouse data load procedure
(Correct)
•	 
IoT backend processing
•	 
Mobile application event handling
•	 
Asynchronous workflow
Explanation
Option 1 is the correct answer.
Cloud Functions are ideal for event-driven processing, such as files being uploaded to Cloud Storage or events being written to Pub/Sub queues.
Compute Engine and App Engine are better choices for long-running jobs, such as loading data into a data warehouse.
Question 33: 
Skipped
You opened the GCP Console at console.google.com and authenticated your user. What is one of the first steps you should take before running a task on a virtual machine?
•	 
Open Open Cloud Shell
•	 
Make sure you can access the virtual machine with SSH
•	 
Make sure the selected project is the one you want to work on
(Correct)
•	 
Check the list of running virtual machines
Explanation
Option 3 is the correct answer.
You need to confirm the selected project. All operations that you perform apply to resources in the selected project. Option 3 is the correct answer.
If you don't work at the command line, you don't need to open Cloud Shell. If it does open, you first need to make sure the project is correctly selected. To log in to your virtual machine using SSH, you must be working in the correct project. Therefore, you should check your project before logging in via SSH. The list of virtual machines in the VM Instance window is the list of virtual machines for the current project.
Question 34: 
Skipped
The architect has suggested a specific machine type for your workload.
When you try to create a virtual machine in the console, the machine type does not appear in the list of available machine types. What could be the reason why it is not displayed?
•	 
Incorrect subnet selected
•	 
The machine type cannot be used in the specified zone
(Correct)
•	 
You chose an incompatible operating system
•	 
Not specifying the correct memory configuration
Explanation
Option 2 is the correct answer.
Different zones may have different machine types available, so you must specify the region first and then the zone to determine the set of available machine types. If a machine type does not appear in the list, it cannot be used in that zone.

Options 1 and 3 are incorrect. Subnets and IP addresses are not related to available machine types.

Option 4 is incorrect. If you do not specify a custom machine type, do not specify a memory capacity. Defined by machine type.
Question 35: 
Skipped
If you use the Cloud Console to set the preemptible property, in which section of Create An Instance can you find that setting option?
•	 
Availability Policy
(Correct)
•	 
Identity And API Access
•	 
Sole tenancy
•	 
networking
Explanation
Option 1 is the correct answer.
In the Availability policy section of the Management tab, set whether it is preemptible.

Option 2 is incorrect. Identity and API access is used to control the virtual machine's access to Google Cloud APIs and the service account used by the virtual machine.

Option 3 is incorrect. [Sole Tenancy] is used when you need to run a virtual machine on your physical server.

Option 4 is incorrect. Networking is used to set network tags and change network interfaces.
Question 36: 
Skipped
You are the leader of a team of cloud engineers who manage cloud resources for multiple departments within your company. You noticed a configuration issue.
Some machine configurations are no longer the same as when they were created. You can't find any notes or documentation on how or why the change was made. What steps should be put in place to resolve such issues?
•	 
Force all cloud engineers to use only the command line interface in Cloud Shell
•	 
Create a script that changes the configuration using the gcloud command and store the script in your version control system
(Correct)
•	 
Create a note when changing configurations and save it to Google Drive
•	 
Restrict permissions so that only you can make changes so you always know when and why a configuration was changed
Explanation
Option 2 is the correct answer.
The best of the four is to use versioned scripts. You can document the reason for the script change. Also, you can run the script repeatedly on different machines and implement the same changes. This reduces the chance of errors when entering commands manually.

Option 1 is incorrect. Doesn't improve documentation of change reasons.

Option 3 is incorrect. Documentation could be better, but an executable script accurately reflects what was done. Details may be missing in the memo.

Option 4 is incorrect. You can become a change bottleneck yourself. Without you, you cannot make changes, and human memory does not appear to be a reliable way to track all configuration changes.
Question 37: 
Skipped
Which of the following actions can be performed on the Network tab of the virtual machine creation form?
•	 
Set the virtual machine IP address
•	 
Add a network interface to your virtual machine
(Correct)
•	 
Specify default router
•	 
Change firewall configuration rules
Explanation
Option 2 is the correct answer.
You can add another network interface on the Network tab of the VM form. So option 2 is the correct answer.
Question 38: 
Skipped
Which Google Cloud Console page do you use to create a single-instance virtual machine?
•	 
Compute Engine
(Correct)
•	 
App Engine
•	 
Kubernetes Engine
•	 
Cloud Functions
Explanation
Option 1 is the correct answer.
The Compute Engine page allows you to create a single virtual machine instance. So option 1 is the correct answer.

Option 2 is incorrect. App Engine is used for applications that can run in containers and language-specific runtime environments.

Option 3 is incorrect. Kubernetes Engine is used to create and manage Kubernetes clusters.

Option 4 is incorrect. Cloud Functions allows you to create functions that run in Google's serverless Cloud Functions environment.
Question 39: 
Skipped
When adding GPUs to your instance, you should check the following: Please select the correct one.
•	 
The instance is set to terminate during maintenance
(Correct)
•	 
the instance is preemptible
•	 
Instance has no non-boot disks
•	 
Your instance is running Ubuntu 14.02 or later
Explanation
Option 1 is the correct answer.
If you add GPUs to your VM, you should configure your instance to terminate during maintenance. This is set in the Availability Policies section of the virtual machine configuration form. Instances do not have to be preemptible and can have non-boot disks attached. Instances do not need to be running Ubuntu 14.02 or higher.
Question 40: 
Skipped
Which can be used as an image source?
•	 
disc only
•	 
Either snapshot or disk only
•	 
disk, snapshot, or another image
(Correct)
•	 
disk, snapshot, or any database export file
Explanation
Option 3 is the correct answer.
You can create an image from four sources: disk, snapshot, Cloud Storage file, and another image. So option 3 is the correct answer.

A database export file is not the source of the image.
Question 41: 
Skipped
You want to generate a list of virtual machines in my inventory and save the result in JSON format.
What commands can you use?
•	 
gcloud compute instances list
•	 
gcloud compute instances describe
•	 
gcloud compute instances list --format json
(Correct)
•	 
gcloud compute instances list --output json
Explanation
Option 3 is the correct answer.
The basic command for working with instances is gcloud compute instances. Using the --format json option puts the output in ISON format. So option 3 is the correct answer.

Option 1 is incorrect. The list command is used to show details of all instances. By default the output is in human readable format, not json.

Option 4 is incorrect. --output is not a valid option.
Question 42: 
Skipped
You are trying to delete the instance instance-1 but would like to keep the boot disk.
It does not keep other attached disks. Which gcloud command should you use?
•	 
gcloud compute instances delete instance-1--keep-disks-boot
(Correct)
•	 
gcloud compute instances delete instance-1 --save-disks-boot
•	 
gcloud compute instances delete instance-1--keep-disks=filesystem
•	 
gcloud compute delete instance-i --keep-disks=filesystem
Explanation
Option 1 is the correct answer.
gcloud compute instances is the base command, followed by delete, the instance name, and --keep disks=boot. So option 1 is the correct answer.

Option 2 is incorrect. There is no --save-disk parameter.

Option 3 is incorrect. filesystem is not a valid value for the keep-disk parameter.

Option 4 is incorrect. The command is missing the required instances option.
Question 43: 
Skipped
What should you create before creating an instance group?
•	 
Instances in Instance Group
•	 
instance template
(Correct)
•	 
boot disk image
•	 
source snapshot
Explanation
Option 2 is the correct answer.
Instance group templates are used to specify how to create instance groups. So option 2 is the correct answer.

Option 1 is incorrect. This is because the instances are automatically created when the instance group is created.

Options 3 and 4 are incorrect. You don't need to create a boot disk image and snapshot before creating an instance group.
Question 44: 
Skipped
Which of the following commands do you use to delete an instance group template from the command line?
•	 
gcloud compute instances instance-template delete
•	 
glcoud compute instance-templates delete
(Correct)
•	 
gcloud compute delete instance-template
•	 
gcloud compute delete instance-templates
Explanation
Option 2 is the correct answer.
The command to delete an instance group is gcloud compute instance-template delete. So option 2 is the correct answer.
Question 45: 
Skipped
An architect is migrating a legacy application to Google Cloud.
You want to manage the cluster as a single entity, while minimizing changes to the existing architecture. A legacy application is running in a load-balanced cluster running two differently configured nodes. A design decision made several years ago necessitates these two configurations.
Application load is more or less constant, so there is little need to scale up or down. Which GCP Compute Engine resource do you recommend using?
•	 
preemptible instance
•	 
Unmanaged instance group
(Correct)
•	 
managed instance group
•	 
GPUs
Explanation
Option 2 is the correct answer.
Unmanaged instance groups are for these limited use cases. Unmanaged instance groups are generally discouraged. Managed instance groups are the recommended way to use instance groups, but they cannot be used in two different configurations. Preemptible instances and GPUs are irrelevant in this scenario.
Question 46: 
Skipped
What kind of instances do you need in my Kubernetes cluster?
•	 
A master node and a node for running workloads
(Correct)
•	 
A master node, nodes for running workloads, and Stackdriver for monitoring node health
•	 
Kubernetes nodes. all instances are identical
•	 
Instances with 4 or more vCPUs
Explanation
Option 1 is the correct answer.
A Kubernetes cluster has one cluster master and one or more nodes to run your workloads. So option 1 is the correct answer.

Stackdriver is not part of the Kubernetes cluster. It's another Google Cloud service. Kubernetes does not require instances with more than 4 vCPUs. The default node configuration uses 1 vCPU.
Question 47: 
Skipped
You are deploying a highly available application on Kubernetes Engine.
You want to maintain availability even in the event of a major network outage in our data center.
What features of Kubernetes Engine can you use?
•	 
multiple instance groups
•	 
Multizone/regional cluster
(Correct)
•	 
Regional deployment
•	 
load distribution
Explanation
Option 2 is the correct answer.
Kubernetes Engine allows multi-zone/multi-region clusters. These clusters are used to increase application resilience. So option 2 is the correct answer.

Option 1 refers to Instance Groups, a feature of Compute Engine. It's not a feature of Kubernetes Engine.

Option 3 is incorrect. Regional deployment is the wrong term. Load balancing is part of Kubernetes' default ability to distribute loads.

Option 4 is incorrect. A data center cannot achieve resiliency if the load is not balanced across zones or regions.
Question 48: 
Skipped
Which gcloud command creates cluster cluster-1 with four nodes?
•	 
gcloud container clusters create cluster-1 --num-nodes=4
(Correct)
•	 
gcloud container beta clusters create cluster-1 --num-nodes=4
•	 
gcloud beta container clusters create cluster-14
•	 
gcloud container clusters create cluster-14
Explanation
Option 1 is the correct answer.
Option 2 has incorrect beta position.
Option 3 doesn't have --num-nodes=4 .
Option 4 does not have the -num-nodes parameter name.
Question 49: 
Skipped
When using Create Deployment from the Cloud Console, which of the following cannot be specified in the deployment?
•	 
container image
•	 
application name
•	 
Time to live (TTL)
(Correct)
•	 
initial command
Explanation
Option 3 is the correct answer.
Time to live (TTL) is not an attribute of deployment. Option 3 is the correct answer. An application name, container image, and initial command can all be specified.
Question 50: 
Skipped
What should you create with Stackdriver before you can monitor my Kubernetes cluster?
•	 
log
•	 
workspace
(Correct)
•	 
pod
•	 
ReplicaSet
Explanation
Option 2 is the correct answer.
A workspace is a logical structure for storing information about monitored project resources. So option 2 is the correct answer.

Strackdriver uses logs, but you don't need them before you start using Stackdriver. Pods & ReplicaSets are part of Kubernetes, not Stackdriver.
Question 51: 
Skipped
A new engineer is trying to set up alerts for a Kubernetes cluster.
The engineer seems to be creating a lot of alerts. You are concerned that this is not the most efficient method and that it requires more maintenance work than necessary.
What is the most efficient way to create an alert and apply that alert to what?
•	 
one instance only
•	 
instance or entity group
(Correct)
•	 
group only
•	 
pod
Explanation
Option 2 is the correct answer.
Alerts are assigned to an instance or set of instances.

Option 1 is incorrect. Contains no groups.

Option 3 is incorrect. Contains no instances.

Option 4 is incorrect. Alerts are not assigned to pods.
Question 52: 
Skipped
You are trying to run a command to start a deployment on your Kubernetes cluster.
This command has no effect at all. You suspect that a Kubernetes component is not working properly.
Which component do you think is the problem?
•	 
Kubernetes API
(Correct)
•	 
StatefulSet
•	 
Cloud SDK gcloud command
•	 
Replica Sets
Explanation
Option 1 is the correct answer.
All interaction with the cluster is done through the master using the Kubernetes API.
Commands are issued by the cluster master when an action is performed on a node.

Options 2 and 4 are incorrect. These are the controllers in the cluster and do not affect how commands are received from client devices.

Option 3 is incorrect. kubectl is used to initiate the deployment, not gcloud.
Question 53: 
Skipped
You tried using the kubect command for the first time and got an error message saying that the resource could not be found or could not connect to the cluster. What command can you use to resolve the error?
•	 
gcloud container clusters access
•	 
gdcloud container clusters get-credentials
(Correct)
•	 
gcloud auth container
•	 
gcloud auth container clusters
Explanation
Option 2 is the correct answer.
You probably don't have access to the cluster. The gcloud container clusters get credentials command is the correct command to configure kubectl to use GCP credentials on your cluster. Anything else is an invalid command.
Question 54: 
Skipped
Overnight, you saw a page informing me that multiple services running in my Kubernetes cluster had high latency when responding to API requests.
After reviewing the monitoring data, you determined that the cluster did not have enough resources to keep up with the load. And decided to add 6 VMs to the cluster.
Which parameter must be specified when issuing the cluster resize command?
•	 
cluster size
•	 
cluster name
•	 
node pool name
•	 
none of the above
(Correct)
Explanation
Option 4 is the correct answer.
When resizing, you must specify the cluster name and node pool to modify with the gcloud container clusters resize command. The size should specify the number of nodes to run. So option 4 is the correct answer.
Question 55: 
Skipped
You would like to fix the number of pods in the cluster. Which method is best?
•	 
modify the pod directly
•	 
fix the deployment
(Correct)
•	 
modify the node pool directly
•	 
Fix the node.
Explanation
Option 2 is the correct answer.
Pods are used to replicate deployments. Best practice is to fix your deployment. Deployments are configured with a number of replicas that are always running.

Option 1 is incorrect. Do not modify the pod directly.

Options 3 and 4 are incorrect. The number of pods running your application does not change.
Question 56: 
Skipped
You are using gcloud to create firewall rules. Which parameter is used to specify which subnets apply?
•	 
--subnet
•	 
--network
(Correct)
•	 
--destination
•	 
--Source-ranges
Explanation
Option 2 is the correct answer
The correct parameter is network.

Option 1 is incorrect. Subnet is not a gcloud parameter for creating firewalls.

Option 3 is incorrect. destination is not a valid parameter.

Option 4 is incorrect. source-ranges specify the sources of network traffic to which the rule applies.
Question 57: 
Skipped
You would like to create a VPN using the Cloud Console. Which section of the Cloud Console will you use?
•	 
Compute Engine
•	 
App Engine
•	 
Hybrid connectivity
(Correct)
•	 
IAM & Admin
Explanation
Option 3 is the correct answer
The VPC create option is available in the Hybrid Connectivity section. Compute, Engine, App Engine, IAM & Admin do not have VPN related features.
Question 58: 
Skipped
You are configuring a load balancer and would like to implement private load balancing. Which option do you choose?
•	 
Only Between My VMs
(Correct)
•	 
Enable Private
•	 
Disable Public
•	 
Local Only
Explanation
Option 1 is the correct answer.
In the console, you can choose between From Internet To My VMs and Only Between My VMs. This is an option to indicate private or public.

Option 1 is the correct answer. All other options are fictitious parameters.
Question 59: 
Skipped
Which two components must be configured when creating a TCP proxy load balancer?
•	 
Frontend and forwarding rules
•	 
front end and back end
(Correct)
•	 
Forwarding rules and backends only
•	 
Only backends and forwarding rules
Explanation
Option 2 is the correct answer.
A TCP proxy load balancer requires both the frontend and the backend to be configured. So option 2 is the correct answer.

Options 1 and 4 are incorrect. Some components are missing.

Option 3 is incorrect. A forwarding rule is a specified component in Network Load Balancing. Also, there is no component called Traffic Rules.
Question 60: 
Skipped
Where do you specify the port in the TCP proxy load balancer that traffic is forwarded to?
•	 
back end
•	 
front end
(Correct)
•	 
Network Services section
•	 
VPCs
Explanation
Option 2 is the correct answer.
You specify the ports to forward when you configure the frontend. Option 2 is the correct answer.

Option 1 is incorrect. Backends configure how traffic is routed to virtual machines.

Option 3 is incorrect. Network Services is a high-level area of the console.

Option 4 is also incorrect. A VPC does not specify a load balancer configuration.
Question 61: 
Skipped
A team is setting up a web service for internal use. We are considering using the same IP address for the foreseeable future. What kind of IP address will you assign?
•	 
internal
•	 
external
•	 
static
(Correct)
•	 
Ephemeral
Explanation
Option 3 is the correct answer.
A static address is assigned until it is freed. Option 3 is the correct answer.

Option 1 and option 2 are incorrect. The inside and outside addresses determine whether traffic is routed inside or outside the subnet. External addresses can reach traffic from the Internet. Internal addresses cannot.

Option 4 is incorrect. Ephemeral addresses are released when the virtual machine is shut down or deleted.
Question 62: 
Skipped
You're about to fire up a virtual machine and experiment with the new Python data science library.
You plan to SSH into the virtual machine by entering the server name, run the Python interpreter interactively for a while, and then shut down the virtual machine.
What type of IP address will you assign to this virtual machine?
•	 
Ephemeral
(Correct)
•	 
static
•	 
forever
•	 
IPv8
Explanation
Option 1 is the correct answer.
Ephemeral addresses are sufficient. This is because resources outside the subnet don't need to reach the virtual machine and can SSH into the virtual machine from the console. So option 1 is the correct answer.

Option 2 is incorrect. There is no need to allocate static addresses that need to be freed.

Option 3 is incorrect. There is no permanent type.

Option 4 is incorrect. No IPv8 address.
Question 63: 
Skipped
You created a subnet sn1 with 192,168.0.0 with 65,534 available IP addresses. However, it turns out that we don't need that many addresses, so we want to reduce the number to 254. Which command should you use?
•	 
gcloud compute networks subnets expand-ip-range sn1 --prefix-length=24
•	 
gcloud compute networks subnets expand-ip-range sn1 --prefix-length=-8
•	 
gcloud compute networks subnets expand-ip-range 511 --size=256
•	 
No command to reduce the number of available IP addresses
(Correct)
Explanation
Option 4 is the correct answer.
You cannot use commands to decrease the number of addresses.

Option 1 is incorrect. The prefix length specified in the expand-ip-range command must be a number less than the current length. If there are 65,534 addresses, the prefix length is 16.

Option 2 is also incorrect for the same reason. The prefix cannot be negative.

Also option 3 is incorrect. There is no --size parameter.
Question 64: 
Skipped
You created a subnet sn using 192.168.0.0 .
You would like to allocate 14 IP addresses. What prefix length should you use?
•	 
32
•	 
28
(Correct)
•	 
20
•	 
16
Explanation
Option 2 is the correct answer.
The prefix length specifies the length in bits of the subnet mask. The remaining bits of the IP address are used in the device address. Since IP addresses are 32 bits, reduce the mask length to get the number of bits to represent the address. 16 is 2 to the power of 4, so 4 bits are required to represent 14 addresses. 32 - 4 is 28. So option 2 is the correct answer.

Option 1 has 1 address available, Option 3 has 4,094 addresses, and Option 4 has 65,534 addresses.
Question 65: 
Skipped
Route all network traffic on the Google network and do not use the public internet.
Which tier of Network Service Tiers should you choose?
•	 
Standard
•	 
Google only
•	 
premium
(Correct)
•	 
non-internet
Explanation
Option 3 is the correct answer.
The Premium Tier Network Service Tiers route all traffic on the Google network.

Option 1 is incorrect. Standard Tier can use the public Internet when routing traffic.

Options 2 and 4 are incorrect. There is no Google-only or non-Internet tier of service.
Question 66: 
Skipped
You host a website on a Compute Engine virtual machine. Users can access your website using the domain name you provide. You stopped and restarted the virtual machine to perform maintenance work on it.
The user can no longer access the website. Nothing else has changed in the subnet. What could be the cause of this issue?
•	 
Reboot changed DNS records
•	 
Using an ephemeral IP address instead of a static IP address
(Correct)
•	 
Not enough addresses available in subnet
•	 
Subnet changed
Explanation
Option 2 is the correct answer.
Stopping and restarting the virtual machine releases the ephemeral IP address. With a static IP address, the same address will be used across reboots.

Option 1 is incorrect. Rebooting the virtual machine does not change the DNS records.

Option 3 is incorrect. If the virtual machine had enough addresses to get one when it first started, and later released the IP address, there should be one or more IP addresses, and the other devices not added to the subnet.

Option 4 is incorrect. No other changes, including subnet changes, are made.
Question 67: 
Skipped
You are deploying a distributed system. You want to use the load balancer that best fits the requirement that messages are passed between Compute Engine virtual machines using the reliable UDP protocol and all virtual machines are in the same region. What kind of load balancer should you use?
•	 
Network TCP/UDP
(Correct)
•	 
TCP proxy
•	 
SSL proxy
•	 
HTTP(S)
Explanation
Option 1 is the correct answer.
Internal TCP/UDP is the best option. This is a regional load balancer that supports UDP.

All other options are global load balancers.

Option 2 supports TCP. Does not support UDP.

Option 3 supports FITTP and HTTPS. Does not support UDP.
Question 68: 
Skipped
You would like to use the Cloud Console to check the records for my DNS entries. Which section of the Cloud Console will you use?
•	 
Compute Engine
•	 
Network Services
(Correct)
•	 
Kubernetes Engine
•	 
Hybrid connectivity
Explanation
Option 2 is the correct answer.
Network Services is the section of the Cloud Console where the Cloud DNS console is located.

Option 1 is incorrect. Compute Engine does not have a DNS management form.

Option 3 is incorrect. Kubernetes Engine also doesn't have a DNS management form.

Option 4 is also related to networking, but the Hybrid Connectivity service is an option for services such as VPN.
Question 69: 
Skipped
My manager asked me to deploy a WordPress site. You are assuming heavy traffic. A manager wants to ensure that the virtual machine hosting the WordPress site has sufficient resources.
What resources can you customize when launching my WordPress site using Cloud Marketplace?
•	 
machine type
•	 
Disc type
•	 
disk size
•	 
all of the above
(Correct)
Explanation
Option 4 is the correct answer.
Option 4 is the correct answer because it allows you to change the configuration of all items in the list. You can specify firewall rules to allow both HTTP and HTTPS traffic or change the zone in which the virtual machine runs.
Question 70: 
Skipped
What file format is used to define Deployment Manager configuration files?
•	 
XML
•	 
JSON
•	 
CSV
•	 
YAML
(Correct)
Explanation
Option 4 is the correct answer.
Option 4 is the correct answer because the configuration file is defined in YAML syntax.
Question 71: 
Skipped
Which keyword is written at the top of the Deployment Manager configuration file?
•	 
deploy
•	 
resources
(Correct)
•	 
properties
•	 
YAML
Explanation
Option 2 is the correct answer.
A configuration file defines resources and begins with the word resources. Option 2 is the correct answer.
Question 72: 
Skipped
Which of the following is used to define resources in the Deployment Manager configuration file?
•	 
type only
•	 
properties only
•	 
only name and type
•	 
type, properties, name
(Correct)
Explanation
Option 4 is the correct answer.
All three types, properties, and names are used when defining resources in the Cloud Deployment Manager configuration file. So option 4 is the correct answer.
Question 73: 
Skipped
You need to find an image available in the zone where you want to deploy your virtual machine. Which command can you use?
•	 
gcloud compute images list
(Correct)
•	 
gcloud images list
•	 
gsutil compute images list
•	 
gcloud compute list images
Explanation
Option 1 is the correct answer.

Option 2 is incorrect. Missing compute.

Option 3 is incorrect. gsutil is a command for working with Cloud Storage.

Option 4 is incorrect. The order of list and images is wrong.
Question 74: 
Skipped
Which command initiates the deployment?
•	 
gcloud deployment-manager deployments create
(Correct)
•	 
gcloud cloud-launcher deployments create
•	 
gcloud deployment-manager deployments launch
•	 
gcloud cloud-launcher deployments launch
Explanation
Option 1 is the correct answer.
The correct answer is option 1, gcloud deployment-manager deployments create .

Options 2 and 3 are incorrect. The command's service is not a cloud-launcher.

Option 4 is incorrect. launch is not a valid verb for this command.
Question 75: 
Skipped
What can be configured by expanding the More link in the Networking section when deploying a Cloud Marketplace solution?
•	 
IP address
(Correct)
•	 
Claim
•	 
access control
•	 
Custom machine type
Explanation
Option 1 is the correct answer.
Option 1 is the correct answer because it allows you to configure the IP address. Options 2 and 3 are incorrect because you cannot configure billing or access control in Deployment Manager.
Although you can configure the machine type, option 4 is also incorrect because there is no More section under Networking.
Question 76: 
Skipped
Which license type is referenced in Cloud Marketplace?
•	 
Free only
•	 
Free and paid only
•	 
Free and bring your own license (BYOL) only
•	 
Free, Paid, Bring Your Own License
(Correct)
Explanation
Option 4 is the correct answer.
Option 4 is the correct answer. Free, Paid, and Bring Your Own License are all licensing options used in Cloud Marketplace.
Question 77: 
Skipped
What license types are charged GCP fees when using Cloud Marketplace?
•	 
free of charge
•	 
Paid
(Correct)
•	 
BYOL
•	 
chargeback
Explanation
Option 2 is the correct answer.
For paid license types, option 2 is the correct answer because the license fee is included in the GCP fee. There is no charge for the free license type. With bring-your-own-license, you must obtain a license from the software vendor and pay a license fee to the vendor.
Option 4 is incorrect as there is no chargeback for such license types.
Question 78: 
Skipped
You are deploying a Cloud Marketplace application with a LAMP stack. What software are you deploying this on?
•	 
Apache server and Linux only
•	 
Linux only
•	 
MySQL and Apache only
•	 
Apache, MySQL, Linux, PHP
(Correct)
Explanation
Option 4 is the correct answer.
LAMP is an abbreviation for Linux, Apache, MySQL, PHP. Option 4, which includes everything when installing the LAMP solution, is correct.
Question 79: 
Skipped
You would like to view the list of permissions for a role using the Cloud Console. What are the steps to view this list?
•	 
Select [ROLES] in [IAM & admin]. Show all permissions
•	 
Select [ROLES] in [IAM & admin]. Check the box next to a role to view the role's access rights
(Correct)
•	 
Select [Audit Logs] under [IAM & admin]
•	 
Under IAM & admin, select Service Accounts, then Audit Logs
Explanation
Option 2 is the correct answer.
The correct procedure is to go to IAM & admin, select ROLES, then check the box next to the role.

Option 1 is incorrect. Not all roles are automatically displayed.

Option 3 is incorrect. Audit logs do not show access rights.

Option 4 is incorrect. There is no [ROLES] option for service accounts.
Question 80: 
Skipped
Which deployment stages are available when creating a custom role?
•	 
Alpha and Beta only
•	 
General availability only
•	 
Disabled only
•	 
Alpha, Beta. General Availability, Disabled
(Correct)
Explanation
Option 4 is the correct answer.
The four deployment stages are Alpha, Beta, General Availability, and Disabled.
Question 81: 
Skipped
A dev ops engineer is confused about the purpose of scopes.
What kind of resource does the scope apply access control to?
•	 
storage bucket
•	 
virtual machine instance
(Correct)
•	 
persistent disk
•	 
subnet
Explanation
Option 2 is the correct answer.
A scope is the access granted to a virtual machine instance. IAM roles are assigned to service accounts, and service accounts are assigned to virtual machine instances. Together, scopes and IAM roles determine the operations that a virtual machine instance can perform.

Options 1 and 3 are incorrect. Scope does not apply to storage resources.

Option 4 is incorrect. Scope does not apply to subnetworks.
Question 82: 
Skipped
What kind of ID is used to identify a scope?
•	 
randomly generated id
•	 
URL starting with https://www.googleserviceaccounts
•	 
URL starting with https://www.googleapis.com/auth
(Correct)
•	 
URL starting with https://www.googleapis.com/auth/[PROJECT_ID]
Explanation
Option 3 is the correct answer.
Scope IDs start with https://www.googleapis.com/auth/ followed by a scope-specific name such as devstorage.read_only or logging.write.

Option 1 is incorrect. Scope IDs are not randomly generated.

Option 2 is incorrect. The domain name is not googleserviceaccounts.

Option 4 is incorrect. Scopes are not directly associated with projects.
Question 83: 
Skipped
Permitted by an IAM role granted to the virtual machine's service account.
Reading the bucket is denied by the scope assigned to the virtual machine. In this case, what is the result when the virtual machine tries to read from the bucket?
•	 
Application doing read skips read operation
•	 
Read is performed because the least restrictive permission is granted
•	 
Reads are not performed because both scopes and IAM roles are applied to determine what operations can be performed
(Correct)
•	 
Read operations succeed, but messages are printed to Stackdriver Logging
Explanation
Option 3 is the correct answer.
The scopes and IAM roles assigned to the service account must allow the operation to succeed.

Option 1 is incorrect. Unless explicitly coded, access control has no effect on the application's control flow.

Option 2 is incorrect. The least restrictive permissions are not used.

Option 4 is incorrect. The operation will not succeed.
Question 84: 
Skipped
Which option is scoped on the virtual machine?
•	 
Allow default access and Allow full access to all Cloud APIs only
•	 
Allow default access, Allow full access to all Cloud APIs, Set access for each API
(Correct)
•	 
Allow full access to all Cloud APIs or "Set access for each API" only
•	 
[Allow default access] and (Set access for each API) only
Explanation
Option 2 is the correct answer.
The options for setting scope are Allow default access, Allow full access to all Cloud APIs, and Set access for each API.

Option 1 is incorrect. "Set access for each API" is missing.

Option 3 is incorrect. "Allow default access" is missing.

Option 4 is incorrect. "Allow full access to all Cloud APIs" is missing.
Question 85: 
Skipped
Which gcloud command should you use to set the scope?
•	 
gcloud compute instances set-scopes
•	 
gcloud compute instances set-service-account
(Correct)
•	 
gcloud compute service-accounts set-scopes
•	 
gcloud compute service-accounts define-scopes
Explanation
Option 2 is the correct answer.
The correct command is gcloud compute instances set-service-account .

Option 1 is incorrect. Missing set scopes.

Option 3 is incorrect. The command's verb is not set-scopes.

Option 4 is incorrect. Missing define-scopes.
Question 86: 
Skipped
Which GCP service do you use to view audit logs?
•	 
Compute Engine
•	 
Cloud Storage
•	 
Stackdriver Logging
(Correct)
•	 
Custom logging
Explanation
Option 3 is the correct answer.
Stackdriver Logging collects, stores, and displays log messages.

Option 1 is incorrect. Compute Engine does not manage logs.

Option 2 is incorrect. Cloud Storage is not used for viewing logs. However, you can save the log file.

Option 4 is incorrect. A custom logging solution is not a GCP service.
Question 87: 
Skipped
Which option filters log messages when viewing the audit log?
•	 
Duration and log level only
•	 
Resource, log type, log level, display period only
(Correct)
•	 
resource and duration only
•	 
log type only
Explanation
Option 2 is the correct answer.
You can filter logs only by resource, log type, log level, and display time period.

Any other option is incorrect. One or more options are missing.
Question 88: 
Skipped
Auditors should review the audit log.
You have created a custom rule for auditors and assigned read-only access. What security best practices are you following?
•	 
Defense in depth
•	 
least privilege
(Correct)
•	 
segregation of duties
•	 
Vulnerability scan
Explanation
Option 2 is the correct answer.
This example assigns the minimum privileges required to perform the task.

Option 1 is incorrect. Defense in depth is the combination of multiple security controls.

Option 3 is incorrect. It's about having more than one person perform important tasks.

Option 4 is incorrect. Vulnerability scanning is a security measure applied to an application that can detect potential vulnerabilities in the application that attackers can exploit.
Question 89: 
Skipped
How long is data stored in Stackdriver Logging?
•	 
The 7th
•	 
15th day
•	 
30 days
(Correct)
•	 
60 days
Explanation
Option 3 is the correct answer.
Stackdriver Logging stores log entries for 30 days, so option 3 is the correct answer.
Question 90: 
Skipped
Which of the following is the sink option?
•	 
Cloud Storage bucket only
•	 
BigQuery datasets and Cloud Storage buckets only
•	 
Cloud Pub/Sub topics only
•	 
Cloud Storage buckets, BigQuery datasets, Cloud Pub/Sub topics
(Correct)
Explanation
Option 4 is the correct answer.
Option 4 is the correct answer because all three Cloud Storage buckets, BigQuery datasets, and Cloud Pub/Sub topics can be used as sinks for Logging exports.
Question 91: 
Skipped
When viewing logs in Stackdriver Logging, which of the following can be used to filter log entries?
•	 
Label or text search only
•	 
Only resource types and log types
•	 
time and resource type only
•	 
label or text search, resource type, log type, time
(Correct)
Explanation
Option 4 is the correct answer.
Option 4 is the correct answer because all items in the list can be used in filtering. A log level can also be used.
Question 92: 
Skipped
Which of the following is not a standard log level that can be used to filter the log display?
•	 
Serious
•	 
Stop
(Correct)
•	 
warning
•	 
information
Explanation
Option 2 is the correct answer.
Option 2 is the correct answer. There is no such standard log level situation. Statuses are Critical, Error, Warning, Info, and Debug.
Question 93: 
Skipped
Which Stackdriver service is best for identifying bottlenecks in my application?
•	 
Monitoring
•	 
Logging
•	 
Trace
(Correct)
•	 
Debug
Explanation
Option 3 is the correct answer.
Cloud Trace is a distributed tracing application that provides detailed information about how long different pieces of code take to execute. So option 3 is the correct answer.

Option 1 is incorrect. Monitoring is used to notify development and operations engineers when resources are not functioning as expected.

Option 2 is incorrect. Logging collects, stores, and displays log data. Also, log entries can be useful in diagnosing bottlenecks, but Logging was not specifically designed for this purpose.

Option 4 is incorrect. Debug is used to generate snapshots and insert log-points.
Question 94: 
Skipped
Your organization plans to migrate its financial transaction monitoring application to Google Cloud. Auditors need to view the data and run reports in BigQuery, but they are not allowed to perform transactions in the application. You are leading the migration and want the simplest solution that will require the least amount of maintenance.
•	 
Assign roles/bigquery.dataViewer to the individual auditors.
•	 
Create a group for auditors and assign roles/viewer to them.
•	 
Create a group for auditors, and assign roles/bigquery.dataViewer to them.
(Correct)
•	 
Assign a custom role to each auditor that allows view-only access to BigQuery.
Explanation
Option3 is correct because it uses a predefined role to provide view access to BigQuery for the group of auditors. Auditors can be added or deleted from the group if job responsibilities change.

Option1 is not correct because Google recommended practice is to assign IAM roles to groups, not individuals. Groups are easier to manage than individual users and they provide high level visibility into roles and permissions.

Option2 is not correct because it uses a basic role to give auditors view access to all resources on the project.

Option4 is not correct because using a predefined role can accomplish the goal and requires less maintenance.

reference: https://cloud.google.com/iam/docs/understanding-roles
Question 95: 
Skipped
You are responsible for monitoring all changes in your Cloud Storage and Firestore instances.
For each change, you need to invoke an action that will verify the compliance of the change in near real time.
what should you use?
•	 
Use the trigger mechanism in each datastore to invoke the security script.
•	 
Use Cloud Function events, and call the security script from the Cloud Function triggers.
(Correct)
•	 
Use a Python script to get logs of the datastores, analyze them, and invoke the security script.
•	 
Redirect your data-changing queries to an App Engine application, and call the security script from the application.
Explanation
Option2 is correct because it provides fast response and requires the minimal amount of setup.

Option1 is not correct because setting triggers in each individual database requires additional setup.

Option3 is not correct because it requires custom programming.

Option4 is not correct because it requires significant custom programming.

reference:
https://cloud.google.com/functions/docs/calling
Question 96: 
Skipped
Your application needs to process a significant rate of transactions. The rate of transactions exceeds the processing capabilities of a single virtual machine (VM).
You want to spread transactions across multiple servers in real time and in the most cost-effective manner. you do?
•	 
Send transactions to BigQuery. On the VMs, poll for transactions that do not have the ‘processed’ key, and mark them ‘processed’ when done.
•	 
Set up Cloud SQL with a memory cache for speed. On your multiple servers, poll for transactions that do not have the ‘processed’ key, and mark them ‘processed’ when done.
•	 
Send transactions to Pub/Sub. Process them in VMs in a managed instance group.
(Correct)
•	 
Record transactions in Cloud Bigtable, and poll for new transactions from the VMs.
Explanation
Option3 is correct because Pub/Sub is a scalable solution that can effectively distribute a large number of tasks among multiple servers at a low cost.

Option1 is not correct because its latency is significantly higher than the real-time response required.

Option2 is not correct because it will not deliver the desired performance.

Option4 is not correct because, although fast, it will introduce an additional expense for storing the data.

reference:
https://cloud.google.com/pubsub/docs/overview
Question 97: 
Skipped
Your team needs to directly connect your on-premises resources to several virtual machines inside a virtual private cloud (VPC).
You want to provide your team with fast and secure access to the VMs with minimal maintenance and cost. What should you do?
•	 
Set up Cloud Interconnect.
•	 
Use Cloud VPN to create a bridge between the VPC and your network.
(Correct)
•	 
Assign a public IP address to each VM, and assign a strong password to each one.
•	 
Start a Compute Engine VM, install a software router, and create a direct tunnel to each VM.
Explanation
Option2 is correct because it agrees with the Google recommended practices.

Option1 is not correct because it is significantly more expensive than other existing solutions.

Option3 is not correct because it will require a sizable maintenance effort.

Option4 is not correct because setting up connections for each individual VM requires a significant amount of maintenance.

reference:
https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview
Question 98: 
Skipped
You are creating a Cloud IOT application requiring data storage of up to 10 petabytes (PB).
The application must support high-speed reads and writes of small pieces of data, but your data schema is simple. You want to use the most economical solution for data storage.
•	 
Store the data in Cloud Spanner, and add an in-memory cache for speed.
•	 
Store the data in Cloud Storage, and distribute the data through Cloud CDN for speed.
•	 
Store the data in Cloud Bigtable, and implement the business logic in the programming language of your choice.
(Correct)
•	 
Use BigQuery, and implement the business logic in SQL.
Explanation
Option3 is correct because Bigtable provides high-speed reads and writes, accommodates a simple schema, and is cost-effective.

Option1 is not correct because Cloud Spanner would not be the most economical solution.

Option2 is not correct because blob-oriented Cloud Storage is not a good fit for reading and writing small pieces of data.

Option4 is not correct because BigQuery does not provide the high-speed reads and writes required by IoT.

reference:
https://cloud.google.com/bigtable/docs/overview
Question 99: 
Skipped
You have created a Kubernetes deployment on Google Kubernetes Engine (GKE) that has a backend service. You also have pods that run the frontend service. moved or restarted. What should you do?
•	 
Create a service that groups your pods in the backend service, and tell your frontend pods to communicate through that service.
(Correct)
•	 
Create a DNS entry with a fixed IP address that the frontend service can use to reach the backend service.
•	 
Assign static internal IP addresses that the frontend service can use to reach the backend pods.
•	 
Assign static external IP addresses that the frontend service can use to reach the backend pods.
Explanation
Option1 is correct because Kubernetes service serves the purpose of providing a destination that can be used when the pods are moved or restarted.

Option2 is not correct because a DNS entry is created by service creation.

Option3 is not correct because static internal IP addresses do not automatically change when pods are restarted.

Option4 is not correct because static external IP addresses do not automatically change when pods are restarted, and they take traffic outside of Google networks.

reference:https://cloud.google.com/kubernetes-engine/docs/how-to/exposing-apps
Question 100: 
Skipped
You are responsible for the user-management service for your global company. The service will add, update, delete, and list addresses. Each of these operations is implemented by a Docker container microservice.
You want to deploy the service on Google Cloud for scalability and minimal administration.
•	 
Deploy your Docker containers into Cloud Run.
(Correct)
•	 
Start each Docker container as a managed instance group.
•	 
Deploy your Docker containers into Google Kubernetes Engine.
•	 
Combine the four microservices into one Docker image, and deploy it to the App Engine instance.
Explanation
Option1 is correct because Cloud Run is a managed service that requires minimal administration.

Option2 is not correct because managed instance groups lack management capabilities to expose their services.

Option3 is not correct because, although GKE provides scalability, it requires ongoing administration of the cluster.

Option4 is not correct because it required effort to reimplement the four microservices in one Docker container. You will also lose your microservice architecture.

reference:https://cloud.google.com/run/docs/quickstarts
Question 101: 
Skipped
What is the basic unit of computing in cloud computing?
•	 
physical server
•	 
virtual machine
(Correct)
•	 
block
•	 
subnet
Explanation
Option 2 is the correct answer.
The basic unit for purchasing computing resources is a virtual machine (VM).

Option 1 is incorrect. A physical server is the foundation of a virtual machine, but the physical server's resources are allocated to the virtual machine.

Options 3 and 4 are incorrect. Blocks and subnets are not related to basic units of computing.
Question 102: 
Skipped
You were asked to design a storage system for a web application that allows users to upload large data files and analyze them using business intelligence workflows.
Files should be stored on a highly available storage system. No file system functionality is required. Which storage system should you use on Google Cloud Platform in this case?
•	 
block storage
•	 
object storage
(Correct)
•	 
cache
•	 
Network File System
Explanation
Option 2 is the correct answer.
Object storage such as Cloud Storage has no limit on the amount of data that can be stored, and objects can be stored redundantly.

Option 1 is incorrect. You can also use block storage, but you'll need to manage replication yourself to ensure high availability.

Option 3 is incorrect. A cache is temporary in-memory storage, not a highly available persistent storage system.

Option 4 is incorrect. 4 is not a good option as file system functionality is not required.
Question 103: 
Skipped
You have been asked to set up network security on Virtual Private Cloud.
Your company has multiple subnetworks and would like to limit traffic between these subnetworks.
What network security controls should you use to control the flow of traffic between subnets?
•	 
Identity access management
•	 
router
•	 
firewall
(Correct)
•	 
IP address table
Explanation
Option 3 is the correct answer.
Google Cloud Platform (GCP) firewalls restrict the flow of traffic into and out of a network or subnetwork with software-defined network controls. So Option 3 is the correct answer.

Routers are used to route traffic to its correct destination on the network.

Identity Access Management is used to authenticate and authorize users. It is not relevant for network control between sub-networks.

The IP Address Table is not a security control.
Question 104: 
Skipped
In which of the following situations is it appropriate to invest in a server for the long term, such as committing to use the server for 3-5 years?
•	 
If you are about to start a company
•	 
If the company can accurately predict server needs over time
(Correct)
•	 
If your company has a fixed IT budget
•	 
If your company's IT budget is not fixed
Explanation
Option 2 is the correct answer.
Investing in servers pays off when you can accurately predict how many servers and other equipment you will need over time, and if you can continue to use that equipment. Startups are not established businesses with a proven track record of predicting needs in the next 3-5 years. It doesn't matter if your budget is fixed or variable. Server investments should be based on server capacity demand.
Question 105: 
Skipped
You plan to use Cloud Vision to analyze images and extract text that appears in images.
You plan to process 1,000 to 2,500 images per hour. How many virtual machines should be allocated to meet peak capacity?
•	 
1
•	 
10
•	 
25
•	 
Cloud Vision is a serverless service so you don't need it
(Correct)
Explanation
Option 4 is the correct answer.
Cloud Vision is one of Google Cloud's dedicated services. Users of the service do not need to configure their virtual machines to use the service.
Question 106: 
Skipped
Many services need to be running to support the application.
Which of the following is the preferred deployment model in this case?
•	 
Run in one large virtual machine
•	 
Using containers in managed clusters
(Correct)
•	 
Use two large virtual machines, one read-only
•	 
Use small virtual machines for all services and increase virtual machine size if CPU utilization is above 90%
Explanation
Option 2 is the correct answer.
Containers are the most flexible and efficient way to use your cluster's resources. Option 2 is the correct answer because the orchestration platform can reduce the operational load.

Running in a single cluster is not recommended as all services will stop if a server fails. Having two virtual machines doesn't help if one is read-only.

A read-only server may work with databases, but the question doesn't mention databases.

You should avoid using small virtual machines and upgrading them when the workload is overwhelmed, as this will degrade service quality.
Question 107: 
Skipped
When setting up a network on Google Cloud Platform, what are the network's resources in Google Cloud Platform?
•	 
Virtual Private Clouds
(Correct)
•	 
sub domain
•	 
cluster
•	 
none of the above
Explanation
Option 1 is the correct answer.
When creating a network, it is treated as a Virtual Private Cloud. Resources are added to your VPC. No access outside the VPC unless explicitly configured.
A subdomain is associated with a web domain and not with a Google Cloud network configuration. Clusters such as Kubernetes clusters can belong to your network, but they are not network resources.
Question 108: 
Skipped
You plan to deploy a SaaS application for users in North America, Europe, and Asia. To maintain scalability, the workload should be distributed across servers in multiple regions.
Which Google Cloud service should you use to distribute my workload?
•	 
Cloud DNS
•	 
Cloud Spanner
•	 
Cloud Load Balancing
(Correct)
•	 
Cloud CDN
Explanation
Option 3 is the correct answer.
Cloud load Balancing distributes workloads within and across regions, performs health checks, and performs autoscaling.

Option 1 is incorrect. Cloud DNS provides domain name services such as translating URLs such as www.example.com into IP addresses.

Option 2 is incorrect. Cloud Spanner is a distributed relational database, but does not perform workload distribution.

Option 4 is incorrect. Cloud CDN distributes your content across regions and reduces latency when delivering content to users around the world.
Question 109: 
Skipped
You have an application that uses Pub/Sub message queues to maintain a list of tasks to be processed by another application.
Applications that process messages from Pub/Sub queues only remove the messages after the task is complete. It takes about 10 seconds to complete the task.
It doesn't matter if two or more virtual machines perform the same task. What would be a cost-effective configuration to handle this workload?
•	 
Use preemptible virtual machines
(Correct)
•	 
Use a standard virtual machine
•	 
Use Cloud Dataproc
•	 
Using Cloud Spanner
Explanation
Option 1 is the correct answer.
This is a good use case for preemptible virtual machines. Preemptible virtual machines reduce the cost of running a second application without losing work. If a preemptible virtual machine is shut down before the task completes, the task is removed from the queue only after the task completes so that another virtual machine can run the task. Also, it doesn't matter how many times a task is run, so even if two virtual machines run the same task, it won't adversely affect the output of the application. Neither DataProc nor Spanner are suitable products for this task.
Question 110: 
Skipped
What server configuration is required to use Cloud Functions?
•	 
Virtual machine configuration
•	 
Cluster configuration
•	 
Configuring Pub/Sub
•	 
no need to do anything
(Correct)
Explanation
Option 4 is the correct answer.
Cloud Functions is a serverless product and requires no configuration.
Question 111: 
Skipped
You have been assigned the task of consolidating the log data generated by each instance of your application. Which Stackdriver management tool do you use?
•	 
Stackdriver Monitoring
•	 
Stackdriver Trace
•	 
Stackdriver Debugger
•	 
Stackdriver Logging
(Correct)
Explanation
Option 4 is the correct answer.
The Stackdriver Logging product is used to concatenate and manage logs generated by applications and servers.
Question 112: 
Skipped
You are the lead developer of a medical application that uses a patient's smartphone to capture biometric data.
If the data cannot be reliably sent to the backend application, the app must collect the data and store it on the smartphone. You want to minimize the amount of development required to keep data synchronized between your smartphone and your backend data store. Which datastore option would you recommend?
•	 
Cloud Firestore
(Correct)
•	 
Cloud Spanner
•	 
Cloud Datastore
•	 
Cloud SQL
Explanation
Option 1 is the correct answer.
Cloud Firestore is a mobile database service that allows you to synchronize data between mobile devices and centralized storage.

Option 2 is incorrect. Spanner is a global relational database, intended for large-scale applications that require transaction support in a highly scaled database.

Options 3 and 4 are incorrect. Datastore and Cloud SQL can also be used, but custom development is required to synchronize data between mobile devices and centralized data stores.
Question 113: 
Skipped
A database architect at your company is discussing the best way to migrate a database to GCP.
The database supports applications with a global user base. Users expect support for transactions and the ability to query data using frequently used query tools.
Database designers have determined that the database service they have chosen must support ANSI 2011 and global transactions. Which database service should you recommend in this case?
•	 
Cloud SQL
•	 
Cloud Spanner
(Correct)
•	 
Cloud Datastore
•	 
Cloud Bigtable
Explanation
Option 2 is the correct answer.
Spanner supports ANSE 2011 standard SQL and global transactions. Cloud SQL supports standard SQL, but without global transactions. Datastore and Bigtable are NoSQL databases.
Question 114: 
Skipped
Suppose you have a Python application that you want to run in a scalable environment with minimal administration overhead. Which GCP product should you choose?
•	 
App Engine flexible environment
•	 
Cloud Engine
•	 
App Engine standard environment
(Correct)
•	 
Kubernetes Engine
Explanation
Option 3 is the correct answer.
App Engine standard environment provides an automatically scaling serverless Python sandbox. App Engine flexible environment runs containers and requires a lot of configuration. Cloud Engine and Kubernetes Engine require considerable management and monitoring.
Question 115: 
Skipped
Where can you view the list of services when using the Cloud Console?
•	 
Deployment details page
(Correct)
•	 
Container details page
•	 
Cluster details page
•	 
none of the above
Explanation
Option 1 is the correct answer.
The Deployment details page shows your service. Containers are used to implement services. Service details are not available. The Clusters detail page does not show information about services running on the cluster.
Question 116: 
Skipped
Which service manages container images?
•	 
Kubernetes Engine
•	 
Compute Engine
•	 
Container Registry
(Correct)
•	 
Container Engine
Explanation
Option 3 is the correct answer.
Container Registry is a service for managing images that can be used by other services. This includes Kubernetes Engine and Compute Engine. So option 3 is the correct answer.

Both Compute Engine and Kubernetes Engine use images, but do not manage them. There is no service called Container Engine.
Question 117: 
Skipped
Which command should you use to print the list of container images on the command line?
•	 
gcloud container images list
(Correct)
•	 
gcloud container list images
•	 
kubectl list container images
•	 
kubectl container list images
Explanation
Option 1 is the correct answer.
Images are managed by GCP, so the correct command is gcloud .
Option 2 is incorrect. Because the verb comes before the resource.

Options 3 and 4 are incorrect. kubectl manages Kubernetes resources, not GCP resources such as container images.
Question 118: 
Skipped
You have now created a deployment. You would like to allow applications outside the cluster to access the services provided by the deployment. What should You do with the service?
•	 
Assign a public IP address
•	 
Issue the kubectl expose deployment command
(Correct)
•	 
Issue the gcloud expose deployment command
•	 
do nothing. Must be accessible at cluster level
Explanation
Option 2 is the correct answer.
The kubectl expose deployment command makes the service accessible.

Option 1 is incorrect. IP addresses are assigned to virtual machines, not services. The command gcloud does not manage Kubernetes services.

Options 3 and 4 are incorrect. Making services accessible is not a cluster-level task.
Question 119: 
Skipped
You have deployed your application to a Kubernetes cluster. This cluster processes sensor data from delivery vehicles. The amount of data received depends on the number of vehicles making the delivery.
The number of delivery vehicles depends on the number of customer orders. Customer orders increase during daylight hours, holiday seasons and major advertising campaigns.
You also want to have enough nodes to handle the load, but you also want to keep the cost low.
How should you configure my Kubernetes cluster?
•	 
Deploy as many nodes as your budget allows
•	 
Enable autoscaling
(Correct)
•	 
Monitor CPU, disk and network utilization and add nodes as needed
•	 
Create a script that runs gcloud commands to add or remove nodes as peaks begin and end
Explanation
Option 2 is the correct answer.
Autoscaling is the most cost-effective and lightest way to respond to fluctuating demand for your service.
Question 120: 
Skipped
Which of the following should a cloud engineer configure when using Kubernetes Engine?
•	 
Nodes, Pods, Services, Clusters only
•	 
Nodes, Pods, Services, Clusters, Container Images
(Correct)
•	 
Only nodes, pods, clusters and container images
•	 
Pods, services, clusters, container images
Explanation
Option 2 is the correct answer.
Cloud engineers working with Kubernetes should understand working with clusters, nodes, pods, and container images. It also requires a good knowledge of deployment.
Question 121: 
Skipped
You have a Django 1.5 Python application deployed to App Engine. This version of Django requires Python 3.
For some reason App Engine is trying to run my application using Python 2.
What files should you check and fix to ensure that this application uses Python 3?
•	 
app-config
•	 
app-yaml
(Correct)
•	 
services.yaml
•	 
deploy.yaml
Explanation
Option 2 is the correct answer.
The app.yaml file is used to configure your App Engine application. So option 2 is the correct answer. The other option is not the files used to configure App Engine.
Question 122: 
Skipped
The latest version of the microservice code has been approved for deployment by the manager, but the product owner does not want to release new features before the press release is published.
You want to deploy the code, but you don't want to expose it to the customer. What's the best way to deploy code as soon as possible without exposing it to customers?
•	 
Deploy with gcloud app deploy --no-traffic
•	 
Create a cron job to deploy after the press release is published
•	 
Deploy with gcloud app deploy --no-promote
(Correct)
•	 
Deploy normally after the press release is published
Explanation
Option 3 is the correct answer.
The correct parameter is --no-promote.
Option 1 is using no-traffic. This is not a valid parameter for the gcloud app deploy command.
Option 2 is not deploying code. Further delays in publishing the press release may precede the release of the code.
Option 4 doesn't meet the requirement of deploying code as quickly as possible.
Question 123: 
Skipped
Which of the two types of instances are available in App Engine Standard?
•	 
Resident and dynamic
(Correct)
•	 
permanent and dynamic
•	 
stable and dynamic
•	 
resident and non-resident
Explanation
Option 1 is the correct answer.
Resident instances are used in manual scaling, while dynamic instances are used in autoscaling and basic scaling. So option 1 is the correct answer. App Engine instances do not have water connection, stable, or emergency types.
Question 124: 
Skipped
What parameters do you use with gcloud app services set-traffic to specify how traffic is split?
•	 
--split-traffic
•	 
--split-by
(Correct)
•	 
--traffic-split
•	 
--split-method
Explanation
Option 2 is the correct answer.
--split-by is a parameter that specifies how to split the traffic. Valid options are cookie, ip, random. All other choices are not valid parameters for the gcloud app services set-traffic command.
Question 125: 
Skipped
What is the name of the cookie used by App Engine when using cookie-based splitting?
•	 
GOOGID
•	 
GOOGAPPUID
(Correct)
•	 
APPUID
•	 
UIDAPP
Explanation
Option 2 is the correct answer.
The cookie used for splitting in App Engine is called GOOGAPPUID.
Question 126: 
Skipped
You've been asked to deploy a Cloud Function that changes the format of an image file as soon as it's uploaded to Cloud Storage.
After a few hours, you noticed that about 10% of files were not processed correctly. After reviewing the failed files, you found that they were all significantly larger than average. What causes this error?
•	 
Syntax error in function code
•	 
Incorrect runtime selected
•	 
Timeout too short, not enough time to process large files
(Correct)
•	 
You're getting a permissions error on the Cloud Storage bucket that contains your files
Explanation
Option 3 is the correct answer.
If the timeout period is too short, fewer files will be processed in time and large files will not be processed. If only 10% of the files are failing, it's not a syntax error or a bad runtime choice like option 1 or option 2. Such errors affect all files, not just large files. Similarly, if there is a permission issue with your Cloud Storage bucket, it will affect all your files.
Question 127: 
Skipped
What types of events are available in Cloud Functions working with Cloud Storage?
•	 
upload or finalize, and delete
•	 
upload or finalize, delete, and list only
•	 
upload or finalize, delete, and metadata update only
•	 
upload or finalize, delete, metadata update, and archive
(Correct)
Explanation
Option 4 is the correct answer. Below are the four events supported by Cloud Storage.

google.storage.object.finalize
google.storage.object.delete
google.storage.object.archive
google.storage.object.metadataUpdate
Question 128: 
Skipped
How much memory can you allocate to Cloud Functions?
•	 
128MB to 256MB
•	 
128MB to 512MB
•	 
128MB to 1GB
•	 
128MB ~ 2GB
(Correct)
Explanation
Cloud Functions can be allocated between 128MB and 2GB of memory. The default is 256MB.
Question 129: 
Skipped
Which of the following commands deploys a Python Cloud Function called Pub_sub_function_test ?
•	 
gcloud functions deploy pub_sub_function_test
•	 
gcloud functions deploy pub_sub_function_test --runtime python37
•	 
gcloud functions deploy pub_sub_function_test --runtime python37 --trigger-topic gcp-ace-exam
(Correct)
•	 
gcloud functions deploy pub_sub_function test --runtime python--trigger-topic gcp-ace-exam-test-topic
Explanation
Option 3 is the correct answer.
Contains the function name, runtime environment, and Pub/Sub topic name.

Option 1 is incorrect. Both runtime and topic are missing.

Option 2 is incorrect. No topics.

Option 4 is incorrect. Invalid runtime specification. You should specify python37 as the runtime, not python.
Question 130: 
Skipped
What are the options for uploading code to Cloud Functions?
•	 
Inline editor
•	 
zip upload
•	 
Cloud source repository
•	 
all of the above
(Correct)
Explanation
Option 4 is the correct answer.
All options are available in zip from Cloud Storage.
Question 131: 
Skipped
You are creating multiple persistent disks to store data for exploratory data analysis.
This disk will be mounted to a virtual machine in the us-west2-a zone. The data is historical data retrieved from Cloud Storage. There are no data analyst performance requirements and cost is more of an issue than performance. Data is stored in a local relational database. What kind of storage do you recommend?
•	 
SSD
•	 
HDD
(Correct)
•	 
Datastore
•	 
Bigtable
Explanation
Option 2 is the correct answer.
If performance is not your primary concern and you're trying to keep costs down, then HDDs are a better choice for persistent disks for local databases. For this reason option 2 is the correct answer.

Option 1 is incorrect. SSDs are expensive and don't require the lowest latency.

Options 3 and 4 are incorrect. Both are other databases and are not used to store data in the local relational database.
Question 132: 
Skipped
Cloud SQL service provides a fully managed relational database. Which of the two types of databases are available in Cloud SQL?
•	 
DB2 and MySQL
•	 
DB2 and PostgreSQL
•	 
PostgreSQL and MySQL
(Correct)
•	 
MySQL and Oracle
Explanation
Option 3 is the correct answer.
MySQL and PostgreSQL are Cloud SQL options.

Option 1 and option 2 are incorrect. DB2 is not an option for Cloud SQL.

Option 4 is incorrect. Oracle is not an option for Cloud SQL. You can run DB2 or Oracle on your instance, but unlike Cloud SQL managed databases, you'll need to manage it yourself.
Question 133: 
Skipped
Which of the following database services does not require you to specify virtual machine configuration information?
•	 
BigQuery only
•	 
Datastore only
•	 
Firebase and Datastore
•	 
BigQuery, Datastore, Firebase
(Correct)
Explanation
Option 4 is the correct answer.
BigQuery, Datastore, and Firebase are all fully managed services that don't require you to provide virtual machine configuration information. So option 4 is the correct answer. Cloud SQL and Bigtable require you to specify virtual machine configuration information.
Question 134: 
Skipped
What kind of data model does Datastore use?
•	 
relational
•	 
document
(Correct)
•	 
column-oriented
•	 
graph
Explanation
Option 2 is the correct answer.
Datastore is a document database. Cloud SQL and Spanner are relational databases. Bigtable is a column-oriented database. Google does not provide a managed graph database.
Question 135: 
Skipped
A team of mobile developers is developing a new application.
This application needs to synchronize data between the mobile device and a backend database. Which database service do you recommend?
•	 
BigQuery
•	 
Firestore
(Correct)
•	 
Spanner
•	 
Bigtable
Explanation
Option 2 is the correct answer.
Firestore is a document database with mobile support features such as data synchronization. BigQuery is for analytics, not mobile or transactional applications. Spanner is a global relational database, but without mobile-specific features. Bigtable can be used on mobile devices, but lacks mobile-specific features such as sync.
Question 136: 
Skipped
A product manager is reviewing new features for the application.
This application requires additional storage. Which of the following considerations would you suggest to the product manager?
•	 
read/write pattern only
•	 
cost only
•	 
Integrity and cost only
•	 
Neither. all must be considered
(Correct)
Explanation
Option 4 is the correct answer.
In addition to read/write patterns, cost, and consistency, you should consider transaction support and latency.
Question 137: 
Skipped
What is the maximum size of the Memorystore cache?
•	 
100GB
•	 
300GB
(Correct)
•	 
400GB
•	 
50GB
Explanation
Option 2 is the correct answer.
Memorystore can be configured to use memory from 1 GB to 300 GB.
Question 138: 
Skipped
After a bucket's storage class is set to Coldline, what other storage classes can be changed?
•	 
Regional
•	 
Nearline
•	 
Multi-Regional
•	 
none of the above
(Correct)
Explanation
Option 4 is the correct answer.
Once a bucket is set to Coldline, it cannot be changed to another storage class. Option 4 is the correct answer. Regional and Multi-Regional can be changed to Nearline and Coldiine. Nearline buckets can be changed to Coldline.
Question 139: 
Skipped
What do you need to create before you start storing data in BigQuery?
•	 
data set
(Correct)
•	 
bucket
•	 
persistent disk
•	 
entity
Explanation
Option 1 is the correct answer.
To store data using BigQuery, you need a dataset to store. So option 1 is the correct answer. Buckets are used in Cloud Storage, not BigQuery. When using BigQuery, you don't manage persistent disks. Entities are Datastore data structures, not BigQuery.
Question 140: 
Skipped
What features can you configure when running Second Generation MySQL databases on Cloud SQL?
•	 
machine type
•	 
maintenance window
•	 
failover replica
•	 
all of the above
(Correct)
Explanation
Option 4 is the correct answer.
You can configure MySQL version, connections, machine type, automatic backups, failover replicas, database flags, maintenance windows, and labels on Second Generation instances.
Question 141: 
Skipped
A colleague is wondering why some storage charges are so high. A colleague explained that they changed all storage to Nearline and Coldline Storage. They access most objects every day. What is one possible reason why storage costs are higher than expected?
•	 
Nearline and Coldline incur access charges
(Correct)
•	 
Transfer fee
•	 
Multi-Regional Coldline is expensive
•	 
Regional Coldline is expensive
Explanation
Option 1 is the correct answer.
Access charges are used by Nearline Storage and Coldline Storage. There are no associated transfer charges. Options 3 and 4 do not refer to actual storage classes.
Question 142: 
Skipped
Created a Cloud Spanner instance and database. How often should you use apt-get to update your virtual machine packages according to Google's best practices?
•	 
every 24 hours
•	 
every 7 days
•	 
every 30 days
•	 
None of the above. Cloud Spanner is a managed service
(Correct)
Explanation
Option 4 is the correct answer.
You don't need to patch the underlying computing resources when using Cloud Spanner. Google manages the resources used by Cloud Spanner. Updating packages is the preferred method when using virtual machines, such as with Connpute Engine. However, this is not required for managed services.
Question 143: 
Skipped
A software team is developing a distributed application.
And you want to send messages between applications.
After the receiving application reads the message, it deletes it. You want your system to be designed to be robust against failures.
For this reason, messages should be kept for at least three days before being deleted. Which GCP service is best designed to support this use case?
•	 
Bigrable
•	 
Dataproc
•	 
Cloud Pub/Sub
(Correct)
•	 
Cloud Spanner
Explanation
Option 3 is the correct answer.
This use case is well suited for Pub/Sub. Messages are sent to topics and are best suited for a subscription model. Pub/Sub has a retention period and supports a retention period of less than 3 days.

Option 1 is incorrect. Bigtable is designed for storing large amounts of data.

Option 2 is incorrect. Dataproc processes and analyzes data and does not pass data between systems.

Option 4 is incorrect. Cloud Spanner is a global relational database. Applications can be designed for this use case, but they require extensive development and are expensive to implement.
Question 144: 
Skipped
Your company is implementing an IoT service and will receive a large amount of streaming data.
This data should be stored in Bigtable. You want to explore your Bigtabie environment from the command line. What command do you run to verify that the command line tools are installed?
•	 
apt-get install bigtable-tools
•	 
apt-get install cbt
•	 
gcloud components install cbt
(Correct)
•	 
gcloud components install bigtable-tools
Explanation
Option 3 is the correct answer.
The correct command to install the Bigtable command-line tools is gcloud components instail cbt.

Option 1 and option 2 are incorrect. apt-get is used to install packages on some Linux systems, but it's not specific to GCP.

Option 4 is incorrect. There is no command like bigtabte-tools.
Question 145: 
Skipped
Cloud Dataproc is a managed service for which two big data platforms?
•	 
Spark and Cassandra
•	 
Spark and Hadoop
(Correct)
•	 
Hadoop and Cassandra
•	 
Spark and TensorFlow
Explanation
Option 2 is the correct answer.
Cloud Dataproc is a managed service for Spark and Hadoop. Cassandra is a big data distributed database, but it is not a database provided by Google as a managed service.

Options 1 and 3 are incorrect.

Option 4 is incorrect. TensorFlow is a deep learning platform not included in Dataproc.
Question 146: 
Skipped
Your department has been asked to analyze large batches of data each night.
This job will take approximately 3 to 4 hours to run. Shut down the resource as soon as the analysis is complete. So you decided to write a script to create a Dataproc cluster every day at midnight. Which command will be used to create the cluster spark-nightly-analysis in us-west2-a zone?
•	 
bq dataproc clusters create spark-nightly-analysis --zone us-west2-a
•	 
gcloud dataproc clusters create spark-nightly-analysis --zone us-west2-a
(Correct)
•	 
gcloud dataproc clusters spark-nightly-analysis --zone us-west2-a
•	 
none of the above
Explanation
Option 2 is the correct answer.
The correct command is gcloud dataproc clusters create, followed by the cluster name and --zone parameter.

Option 1 is incorrect. bq is a command line tool for BigQuery, not Dataproc.

Option 3 is the gcloud command, which is incorrect because it has no verb or command.

Option 4 is incorrect.
Question 147: 
Skipped
You have many buckets that contain old data that has rarely been used. You don't want to delete it, but you want to minimize storage costs. So you decided to change the storage class of each bucket to Coldline. What command structure should you use?
•	 
gcloud rewrite -s [STORAGE_CLASS] gs://[PATH_TO_OBJECT]
•	 
gsutil rewrite -s [STORAGE_CLASS] gs://[PATH_TO_OBJECT]
(Correct)
•	 
cbt rewrite -s [STORAGE_CLASS] gs://[PATH_TO_OBJECT]
•	 
bq rewrite -s [STORAGE_CLASS] gs://[PATH_TO_OBJECT]
Explanation
Option 2 is the correct answer.
gsutil is the correct command.

Option 1 is incorrect. gcloud commands are not used to manage Cloud Storage.

Similarly, options 3 and 4 are incorrect. Use Bigtable and BigQuery commands, respectively.
Question 148: 
Skipped
When exporting from BigQuery, the compression options Deflate and Snappy are available on which file types?
•	 
Avro
(Correct)
•	 
CSV
•	 
XML
•	 
Thrift
Explanation
Option 1 is the correct answer.
Avro supports Deffate and Snappy compression. CSV supports Gzip and has no compression. XML and Thrift are not export file type options.
Question 149: 
Skipped
Which of the following file formats is not an option for export files when exporting BigQuery?
•	 
CSV
•	 
XML
(Correct)
•	 
Avro
•	 
JSON
Explanation
Option 2 is the correct answer.
XML is not an option for BigQuery's export process. All other options are available.
Question 150: 
Skipped
Which of the following file formats is not supported when importing data into BigQuery?
•	 
CSV
•	 
Parquet
•	 
Avro
•	 
YAML
(Correct)
Explanation
Option 4 is the correct answer
YAML is not a file storage format. Used to specify configuration data. All other choices are all supported import file formats.
Question 151: 
Skipped
You have set up a Cloud Spanner process to export data to Cloud Storage.
You noticed that each time a process ran, You was billed for another GCP service. You believe this charge is related to the export process. Which other GCP services may incur charges during Cloud Spanner exports?
•	 
Dataproc
•	 
Dataflow
(Correct)
•	 
Datastore
•	 
bq
Explanation
Option 2 is the correct answer
Dataflow is a pipeline service for processing streaming and batch data that implements the workflows used by Cloud Spanter.

Option 1 is incorrect. Dataproc is a managed Hadoop and Spark service. This is used in data analysis.

Option 3 is incorrect. Datastore is a NoSQL database.

Also option 4 is incorrect. bg is only used in BigQuery.
Question 152: 
Skipped
You want the router on the tunnel you are creating to learn routes from all GCP regions on my network. What features of GCP routing do you want to enable?
•	 
global dynamic routing
(Correct)
•	 
Regional routing
•	 
VPCs
•	 
firewall rules
Explanation
Option 1 is the correct answer
Global dynamic routing is used to learn all routers in your network.

Option 2 is incorrect. Regional routing only learns regional routes.

Options 3 and 4 are incorrect. It is not used to configure routing options.
Question 153: 
Skipped
What kind of unique ID should be assigned to the BGP protocol when creating a cloud router?
•	 
IP address
•	 
ASN
(Correct)
•	 
Dynamic loading routing ID
•	 
none of the above
Explanation
Option 2 is the correct answer
An Autonomous System Number (ASN) is a number used to identify a cloud router on your network. Option 2 is the correct answer. An IP address is not a unique ID for the BGP protocol.

Option 3 is incorrect. No dynamic loading routing ID.

Option 4 is incorrect.
Question 154: 
Skipped
You are using gcloud to create a VPN. Which command should you use?
•	 
gcloud compute target-von-gateways only
•	 
gcloud compute forwarding-rule and gcloud compute target-vpn-gateways only
•	 
gcloud compute vpn-tunnels only
•	 
gcloud compute forwarding-rule, gcloud compute target-vpn-gateways, and gcloud compute vpn-tunnels
(Correct)
Explanation
Option 4 is the correct answer
When creating a VPN using gcloud, you must create forwarding rules, tunnels, gateways, and use all of the gcloud commands listed.
Question 155: 
Skipped
Which GCP objects are treated as both resources and identities?
•	 
billing account
•	 
service account
(Correct)
•	 
project
•	 
role
Explanation
Option 2 is the correct answer.
A service account is a resource managed by an administrator, but it also acts as an identity to which roles can be assigned. Billing accounts are not associated with identities. A project is not an ID. A role is a resource, but not an identity. Roles can be assigned permissions, but permissions are only used when the role is associated with an identity
Question 156: 
Skipped
You are planning to develop a web application with roles already established to manage permissions such as the ability to delete. Which of the following roles provides these capabilities?
•	 
basic role
•	 
predefined roles
(Correct)
•	 
custom role
•	 
Application role
Explanation
Option 2 is the correct answer.
Predefined roles are defined for specific products such as App Services and Compute Engine.
In general, bundled permissions are required when managing or using services. Primitive roles are the basis for other roles. Custom roles are created by users to meet their specific needs. Application roles are fictitious roles.
Question 157: 
Skipped
You are reviewing a new GCP account created for use by the finance department.
The auditor has asked who can create projects by default, so you explain who has permission to create projects by default. Who is this person?
•	 
Project admin only
•	 
all users
(Correct)
•	 
Only users without the resourcemanager.projects.create role
•	 
Billing account users only
Explanation
Option 2 is the correct answer. By default, all users in your organization can create projects. resourcermanager.projects.create is a role that allows users to create projects. Billing accounts are not associated with project creation.
Question 158: 
Skipped
You are planning how to grant permissions to users of your company's GCP account, and you need to document what each user can do.
Auditors are most concerned about what are called organizational IAM roles, so you explained that users with organizational IAM roles can do many tasks, except for one thing. What is the role of “something” here?
•	 
Define the structure of your resource hierarchy
•	 
Decide which permissions should be assigned to users
(Correct)
•	 
Defining an IAM Policy in a Resource Hierarchy
•	 
Delegate other administrative roles to other users
Explanation
Option 2 is the correct answer.
Users in organizational IAM roles are not necessarily responsible for determining the permissions assigned to them. This is determined based on the individual's role in the organization and security policies established within the organization.
Question 159: 
Skipped
You are deploying a Python web application to GCP. This application uses only custom code and basic Python libraries. In the near future, we hope to minimize both the cost of running the application and the development and operations overhead of managing the application by using it sporadically. Which compute service is best suited to run this application?
•	 
Compute Engine
•	 
App Engine standard environment
(Correct)
•	 
App Engine flexible environment
•	 
Kubernetes Engine
Explanation
Option 2 is the correct answer.
App Engine standard environment can run Python applications. When there is no load, autoscaling can shrink your instances down to zero, thus minimizing your costs.

Options 1 and 3 are incorrect. Compute Engine and App Engine flexible environment require more configuration management than App Engine standard environment.

Option 4 is incorrect. Kubernetes Engine is used when you need to support large applications or multiple applications using the same computing resources.
Question 160: 
Skipped
The company you work for is releasing a new online service based on a new user interface experience driven by a set of services running on a server.
There is a separate set of services that manage authentication and authorization.
The service's datastore set tracks account information. All three sets of services must be highly reliable and scale to meet demand. Which Google Cloud service would be best for deploying this?
•	 
App Engine standard environment
•	 
Compute Engine
•	 
Cloud Functions
•	 
Kubernetes Engine
(Correct)
Explanation
Option 4 is the correct answer.
The descriptive scenario is perfect for Kubernetes. Each group of services is structured in pods and deployed using Kubernetes deployment.
Kubernetes Engine manages node health, load balancing, and scaling. App Engine standard edition has language-specific sandboxes, but they are not suitable for this use case. Cloud Functions are designed for short-running event processing. It is not the continuous processing required in this scenario. Compute Engine can meet the requirements of this use case, but requires additional effort on the part of application administrators and dev ops to configure load balancers, monitor health, and manage software deployments.
Question 161: 
Skipped
A mobile application uploads images for analysis, such as identifying objects in images and extracting text that may be embedded in images.
A third party created a mobile application and you developed an image analysis service. This third party and you have both agreed to use Cloud Storage for image storage.
You want to keep the two services completely separate, but you need a way to trigger image analysis as soon as an image is uploaded. What should you do in this case?
•	 
Run the image analysis service in a VM, modify the mobile app to start the VM running the image analysis service, and configure that VM to copy files from storage to the VM's local storage.
•	 
Write a function in Python that Cloud Functions will call when a new image file is written to the Cloud Storage bucket that receives the new image. This function should send the URL of the uploaded file to the image analysis service. The image analysis service then loads the images from Cloud Storage, performs the analysis to produce results, and saves the results to Cloud Storage
(Correct)
•	 
Run a Kubernetes cluster continuously, dedicating one pod to listing the contents of the upload bucket and discovering new files in Cloud Storage, and another pod dedicated to running image analysis software.
•	 
Run a Compute Engine VM to continuously list the contents of your Cloud Storage upload bucket to discover new files. Continuously run the image analysis service in another VM.
Explanation
Option 2 is the correct answer.
This is a perfect use case for Cloud Functions. Cloud Functions are triggered on file upload events. Cloud Functions call the image processing service. In this setup the two services are independent. No additional server required. A does not meet the requirement to maintain service independence.

Options 3 and 4 have a higher administrative burden and probably higher operating costs than option 2.
Question 162: 
Skipped
The company you work for has noticed a significant increase in customers in Europe.
My company's applications are running on us central1, so latency is becoming an issue.
So you decide to propose deploying the service in the European regions. You have several options. Which of the following factors should not be considered?
•	 
cost
•	 
latency
•	 
regulation
•	 
reliability
(Correct)
Explanation
Option 4 is the correct answer.
All Google regions have the same service level agreement and the same reliability.

Option 1 is incorrect. Costs may vary by region.

Option 2 is incorrect. Latency is a consideration when you need a region closer to your end users or when the data you need is already stored in a specific region.

Option 3 is incorrect. Regulations may require data to be stored within a geographic region such as the European Union.
Question 163: 
Skipped
Which of the following language runtimes are not supported when using App Engine standard environment?
•	 
Java
•	 
Python
•	 
C.
(Correct)
•	 
Go
Explanation
Option 3 is the correct answer.
App Engine standard environment does not support the C language. If you need to run a C language application, you can compile and run it in a container running in the App Engine flexible environment.
Question 164: 
Skipped
Kubernetes reserves CPU resources as a percentage of available cores. What % to what percentage range does the percentage correspond to?
•	 
1%~10%
•	 
0.25%~6%
(Correct)
•	 
0.25% ~ 2%
•	 
10%-12%
Explanation
Option 2 is the correct answer.
Kubernetes reserves CPU power according to the following schedule:
1. First Core: 6%
2. Next core: 1% (up to 2 cores)
3. Next 2 cores: 0.5% (up to 4 cores)
4. Any core (more than 5 cores): 0.25%
Question 165: 
Skipped
What are the one-time tasks that must be completed before using the console?
•	 
Set up billing
(Correct)
•	 
create a project
•	 
Create a storage bucket
•	 
Specify default zone
Explanation
Option 1 is the correct answer.
If not enabled when you start using the console, you will need to set up billing. Option 1 is the correct answer.

Option 2 is incorrect. A project can only be created if billing is enabled.

Option 3 is incorrect. You don't need to create storage buckets to work with the console.

Option 4 is incorrect. Specifying a default zone is not a one-time task. Zones can be changed during a project.
Question 166: 
Skipped
Your manager has asked you to help him understand cloud computing costs.
Your team runs dozens of virtual machines for three different applications. Two of the applications are used by the marketing department and one by the finance department. Managers are trying to figure out how to charge each unit the cost of the virtual machines used by their respective applications.
What would you suggest to solve this problem?
•	 
access control
•	 
persistent disk
•	 
label and description
(Correct)
•	 
description only
Explanation
Option 3 is the correct answer.
Labels and descriptions are useful for tracking resource attributes. It is not required to perform the task. As the number of servers grows, it can become difficult to keep track of which virtual machines serve which applications.

Options 1 and 2 are incorrect. They are used for security and storage purposes, but are not useful for managing multiple virtual machines.

Option 4 is incorrect. A label is required.
Question 167: 
Skipped
When adding another disk in the Google Cloud Console, you can set all but one of the following parameters: Which parameters cannot be set?
•	 
Disc type
•	 
Encryption key management
•	 
block size
(Correct)
•	 
Source image of disk
Explanation
Option 3 is the correct answer.
Block size is not an option in the Additional disks dialog.

Options to specify encryption key management, disk type, and source image are available.
Question 168: 
Skipped
When using the Cloud SDK command-line interface, which of the following contains commands for managing resources on Compute Engine?
•	 
gcloud compute instances
(Correct)
•	 
gcloud instances
•	 
gcloud instances compute
•	 
none of the above
Explanation
Option 1 is the correct answer.
gcloud compute instances is the first command to manage your Compute Engine resources.
Question 169: 
Skipped
You have received a 10 GB dataset from a third-party research firm.
A group of data scientists would like to access this data from a statistical program written in R. R works fine on Linux and Windows file systems. Also, data scientists are familiar with working with files in R.
Data scientists want to have their own dedicated virtual machine to make that data available on the file system. How to make this data immediately available to the virtual machine, minimizing the steps required by the data scientist?
•	 
Store data in Cloud Storage
•	 
Create a virtual machine using a source image created from a disk containing data
(Correct)
•	 
Save data to Google Drive
•	 
Load data into BigQuery
Explanation
Option 2 is the correct answer.
10 GB of data is small enough to fit on one disk. Using the data stored on a disk to create an image of the disk allows you to specify the source image when creating a virtual machine.

Option 1 is incorrect. A data scientist must copy the data from Cloud Storage to the virtual machine's disk.

Option 3 is incorrect. You have to copy the data as well.

Option 4 is incorrect. Read data into the database instead of the file system specified in the requirement.
Question 170: 
Skipped
Which of the following commands creates a virtual machine named web-server-1 with 4 CPUs?
•	 
gcloud compute instances create -- machine-type=ni-standard-4 web-server-1
(Correct)
•	 
gcloud compute instances create --cpus=4 web-server-1
•	 
gcloud compute instances create --machine-type=71-standard-4 --instance-name web-server-1
•	 
gcloud compute instances create --machine-type=n1-4-cpu web-server-1
Explanation
Option 1 is the correct answer.
It contains the correct machine type and names the instance correctly.
Question 171: 
Skipped
Management is considering three different cloud providers.
You have been tasked with compiling billing and cost information so that management can compare cost structures across clouds. Which of the following describes the cost of virtual machines on GCP?
•	 
Virtual machines are billed in 1-second increments. The cost depends on the number of CPUs and amount of memory in the machine type. You can create custom machine types. Preemptible virtual machines cost up to 80% less than standard virtual machines. For sustained use, Google will apply a discount.
(Correct)
•	 
Virtual machines are billed by the second. Also, a virtual machine can run for up to 24 hours before being shut down.
•	 
In some regions only, Google applies sustained use discounts. Virtual machine costs vary depending on the number of CPUs and amount of memory in the machine type. You can create custom machine types. Preemptible virtual machines cost up to 80% less than standard virtual machines.
•	 
The minimum charge for a virtual machine is an hourly usage charge. Also, the cost depends on the number of CPUs and amount of memory in the machine type.
Explanation
All statements in option 1 are correct and related to billing and costs.

Option 2 is correct in that virtual machines are billed per second, but it is incorrect because preemptible virtual machines are shut down within 24 hours of booting.

Option 3 is incorrect. Discounts are not restricted to some regions.

Option 4 is incorrect. The minimum unit in which a virtual machine is billed is not an hour.
Question 172: 
Skipped
The console is displaying a list of Linux virtual machine instances.
All instances are assigned an external IP address. One of the instances has the SSH option disabled. What is the reason for this?
•	 
This instance is preemptible and does not support SSH
•	 
this instance is stopped
(Correct)
•	 
This instance is configured with the No SSH option
•	 
SSH option is not disabled
Explanation
Option 2 is the correct answer.
Instances can be stopped. You cannot connect to the instance via SSH while it is in a stopped state. So option 2 is the correct answer. Launching an instance enables SSH access.

Option 1 is incorrect. Because you can log in to preemptible machines.

Option 3 is incorrect. No SSH option.

Option 4 is incorrect. SSH option can be disabled.
Question 173: 
Skipped
You have noticed a very slow response time when issuing commands to a Linux server. So you decided to reboot the machine. What command do you use in the console to reboot?
•	 
Reboot
•	 
Reset
(Correct)
•	 
Restart
•	 
Shutdown Startup order
Explanation
Option 2 is the correct answer.
You can use the Reset command to restart the virtual machine.

VM properties remain the same, but data in memory is lost. The console does not have Reboot, Restart, Shutdown, Startup options.
Question 174: 
Skipped
You build a number of machine learning models on your instance and want to add a GPU to your instance.
Your machine learning model is taking a very long time to run. It seems that the GPU is not being used. What causes this?
•	 
GPU library not installed
(Correct)
•	 
Your operating system is not Ubuntu based
•	 
Your instance does not have more than 8 cores
•	 
Not enough free space on persistent disk
Explanation
Option 1 is the correct answer.
For proper functioning, the operating system must have the GPU library installed. So option 1 is the correct answer. The operating system does not have to be Ubuntu. Also, your instance doesn't need to have more than 8 CPUs before you can attach and use GPUs. Free disk space does not determine whether the GPU is used.
Question 175: 
Skipped
You decide to delegate the task of creating backup snapshots to a member of your team.
What role should a team member have to be given to create a snapshot?
•	 
Compute Image Admin
•	 
Storage Admin
•	 
Compute Snapshot Admin
•	 
Compute Storage Admin
(Correct)
Explanation
Option 4 is the correct answer.
To work with snapshots, you must be assigned the Compute Storage Admin role.

The other option is a role that doesn't exist.
Question 176: 
Skipped
You are building an image with Ubuntu 14.04 and now want my users to use Ubuntu 16.04.
You don't want to just delete the Ubuntu 14.04 image, you want to let users know to start using Ubuntu 16.04. What features of the image can be used for this?
•	 
redirect
•	 
Stop using images
(Correct)
•	 
not supported
•	 
Migration
Explanation
Option 2 is the correct answer.
Deprecated marks the image as deprecated and allows you to specify a replacement image for future use.
So option 2 is the correct answer. Deprecated images are available, but may not have security vulnerability patches or other updates applied.

Other options are features of the image that do not exist.
Question 177: 
Skipped
Which command deletes instance ch06-instance-3?
•	 
gcloud compute instances delete instancesch06-instance-3
•	 
gcloud compute instance stop ch06-instance-3
•	 
gcloud compute instances delete ch06-instance-3
(Correct)
•	 
gcloud compute delete ch06-instance-3
Explanation
Option 3 is the correct answer.
The command to delete an instance is gcloud compute instances delete, followed by the instance name. Option 3 is the correct answer.

Option 1 is incorrect. There is no instance parameter.

Option 2 is incorrect. The command will stop, but the instance will not be deleted.

Option 4 is incorrect. Required instances to indicate the type of entity to be deleted is missing.
Question 178: 
Skipped
You would like to display a list of fields that can be used to sort the list of instances.
Which command can be used to display field names?
•	 
gcloud compute instances list
•	 
gcloud compute instances describe
(Correct)
•	 
gcloud compute instances list --detailed
•	 
gcloud compute instances describe --detailed
Explanation
Option 2 is the correct answer.
Use the describe command.

Option 1 is incorrect. Show some fields, but not all.

Options 3 and 4 are incorrect. There is no detailed parameter.
Question 179: 
Skipped
What are the conditions under which an instance group scales autoscaling?
•	 
CPU usage and operating system updates
•	 
Disk usage and CPU usage only
•	 
Network latency, load balancing ability, CPU utilization
(Correct)
•	 
Disk usage and operating system updates only
Explanation
Option 3 is the correct answer.
You can set autoscaling policies to add or remove instances based on CPU utilization, monitoring metrics, load balancing capacity, and queue-based workloads. Scaling can be triggered when monitoring metrics are configured for disk, network latency, and memory resources.
Question 180: 
Skipped
A new engineer looks at the best cases to use Kubernetes and the best cases to use instance groups.
You pointed out that Kubernetes uses instance groups.
What role do instance groups play in a Kubernetes cluster?
•	 
Monitor instance health
•	 
Create a pod and deployment
•	 
Create a collection of virtual machines that can be managed as a unit
(Correct)
•	 
Create a notification channel for alerts
Explanation
Option 3 is the correct answer.
In Kubernetes, the instance group is created as part of the process of creating the cluster, so option 3 is correct. Stackdriver, not instance groups, is used to monitor node health and create alerts and notifications.
Kubernetes creates pods and deployments, which are not served by instance groups.
Question 181: 
Skipped
You have developed an application that calls a service running in a Kubernetes cluster.
Services run in pods and can be terminated and replaced by other pods if the pod is unhealthy. At that time, different IP addresses may be assigned.
How should you code my application to ensure that it works in this situation?
•	 
Query Kubernetes for a list of IP addresses of pods running services you want to use
•	 
Communicate with Kubernetes services so that applications do not need to work with specific pods.
(Correct)
•	 
Query Kubernetes for a list of pods running the service you want to use
•	 
Get the desired IP address using the gcloud command
Explanation
Option 2 is the correct answer.
A service is an application that provides an API endpoint that allows applications to discover the pods running a particular application.

For options 1 and 3, coding against the API to manage the cluster is more code than working with the service, and many API functions need to be changed. Option 4 is incorrect.
Question 182: 
Skipped
You want to create a script to deploy a Kubernetes cluster containing GPUs.
You have deployed a cluster before and am not familiar with some of the required parameters. You should deploy this script as soon as possible. Which of the following could be used as one way to rapidly develop this script?
•	 
Use the GPU template in Kubernetes Engine's Cloud Console to generate gcloud commands to create a cluster
(Correct)
•	 
Search the web for scripts
•	 
Check the documentation for gcloud parameters for adding GPUs
•	 
Use an existing script to add parameters for connecting GPUs
Explanation
Option 1 is the correct answer.
The most reliable method is to start with an existing template, fill in the parameters, and generate the gcloud command.

Option 4 is also possible, but multiple parameters required by the configuration may not be present in the starting script. This method may require some trial and error.

Option 2 and Option 3 are also possible, but may take longer to complete.
Question 183: 
Skipped
What file format do deployment configuration files created in the Cloud Console use?
•	 
CSV
•	 
YAML
(Correct)
•	 
TSVs
•	 
JSON
Explanation
Option 2 is the correct answer.
Deployment configuration files created in the Cloud Console are saved in YAML format. CSV, TSV, JSON are not used.
Question 184: 
Skipped
Which operations in a Kubernetes cluster use Stackdriver?
•	 
notification only
•	 
Monitoring and notifications only
•	 
logging only
•	 
Notification, Monitoring, Logging
(Correct)
Explanation
Option 4 is the correct answer.
Stackdriver is a comprehensive monitoring, logging, alerting, and notification service that you can use to monitor your Kubernetes clusters.
Question 185: 
Skipped
What kind of information does the Details page show about my Stackdriver instance?
•	 
CPU utilization only
•	 
network traffic only
•	 
Disk I/O, CPU utilization, network traffic
(Correct)
•	 
CPU utilization and disk I/O
Explanation
Option 3 is the correct answer.
The Stackdriver Instance Detail page shows time series graphs for CPU utilization, network traffic, and disk I/O.
Question 186: 
Skipped
A deployment team needs to be notified when an application running on some Kubernetes cluster is experiencing issues.
Another team member wants another way to be notified besides Stackdriver alerts.
What's the most efficient way to send notifications and fulfill team requests?
•	 
Configure SMS text message, Slack, and email notifications for alerts
(Correct)
•	 
Create separate alerts for each notification channel
•	 
Create email notification alerts and have these notification emails forwarded to other notification systems
•	 
Use a single third-party notification mechanism
Explanation
Option 1 is the correct answer.
Alerts can have multiple channels. Option 1 is the correct answer.

Channels include email, webhooks, SMS text messages, and third-party tools such as PagerDuty, Campfire, and Siack. No need for multiple alerts for individual notifications.

Option 3 is ad-hoc and incurs additional maintenance burden.

Option 4 does not meet your requirements.
Question 187: 
Skipped
You have deployed your application to a Kubernetes cluster. You notice that multiple pods have been starving for resources for a period of time, causing pods to die.
When resources are available, these newly instantiated pods are created.
Even though the new pod's IP address is different from the terminated pod's IP address, the client is still able to connect to the pod. Which Kubernetes component makes this possible?
•	 
Service
(Correct)
•	 
ReplicaSet
•	 
Alert
•	 
StatefulSet
Explanation
Option 1 is the correct answer.
Services provide some level of indirect access to Pods. Pods are ephemeral. The client connects to a service that can discover pods. ReplicaSets and StatefulSets provide managed pods. Alerts are for reporting the state of resources.
Question 188: 
Skipped
Checking cluster details in the Cloud Console.
You want to check how many vCPUs are available in my cluster. Where is this information displayed?
•	 
Node Pools section of the Cluster Details page
•	 
Labels section of the Cluster Details page
•	 
[Cluster Listing| page summary row
•	 
Options 1 and 3
(Correct)
Explanation
Option 4 is the correct answer.
The cluster listing in the Total Cores column or the Size parameter in the Node Pool section of the Details page shows the number of vCPUs.
So option 4 is the correct answer. The Labels section does not display vCPU information.
Question 189: 
Skipped
An engineer recently joined the team and is unaware of the team's standard rules for creating clusters and other Kubernetes objects. In particular, the engineer did not set the correct labels for multiple clusters.
You would like to fix the cluster label from the Cloud Console. How do you do that?
•	 
Click the [Connect] button.
•	 
Click the Deploy menu option.
•	 
Click the Edit menu option.
(Correct)
•	 
Enter a new label in the Labels section.
Explanation
Option 3 is the correct answer.
Click the [EDIT] button to change, add or remove labels.

The [CONNECT] button appears on the cluster view page. The DEPLOY button creates a new deployment. When viewing details, there is no way to enter labels in the [Labels] section.
Question 190: 
Skipped
You would like to see a list of deployments. Which option would you choose in the Kubernetes Engine navigation menu?
•	 
Clusters
•	 
Storage
•	 
Workloads
(Correct)
•	 
Deployments
Explanation
Option 3 is the correct answer.
Deployments are listed under Workloads. The Cluster option shows cluster details, but not deployment details. [Storage] shows persistent volume and storage class information. You cannot select the [Deployments] option.
Question 191: 
Skipped
Which deployment parameters can be set on the [Create Deployment] page of the Cloud Console?
•	 
container image
•	 
cluster name
•	 
application name
•	 
none of the above
(Correct)
Explanation
Option 4 is the correct answer.
You can specify a container image, cluster name, application name, label, initial command, and namespace.
Question 192: 
Skipped
You're supporting a machine learning engineer testing a set of classifiers. There are five classifications such as mi-classifier-1 and ml-classifier2.
An engineer notices that ml-classifier-3 is not working as expected and wants to remove it from the cluster. How can you remove the service ml-ciassifier-3 ?
•	 
Run the command kubectl delete service mi-classifier-3
(Correct)
•	 
Run the command kubect delete mi-classifier-3
•	 
Run the command gcloud service delete ml-classifier-3
•	 
Run the command gcloud container service delete ml-classifier-3
Explanation
Option 1 is the correct answer. kubect delete service ml-classifier-3 is the correct answer. 

Option 2 has no service.
Options 3 and 4 are incorrect. This is because services are managed by Kubernetes, not GCP.
Question 193: 
Skipped
A data warehouse architect wants to deploy an extract, transform, and load process on Kubernetes. The designer submitted a list of libraries to install, including GPU drivers. There are a number of container images that may meet your requirements. How can you view detailed information for each of these containers?
•	 
Run the command gcloud container images list details
•	 
Run the command gcloud container images describe
(Correct)
•	 
Run the command gcloud image describe
•	 
Run the command gcloud container describe
Explanation
Option 2 is the correct answer.
The correct command is gcloud container images describe. describe is a gcloud verb or operation that describes an object. All other choices are invalid commands.
Question 194: 
Skipped
Developed a microservice ready for production. Before deploying, you need to know how to manage the service lifecycle.
Architects are specifically looking for ways to deploy service updates with minimal service disruption.
What elements of App Engine components can be used during service updates to minimize service disruption?
•	 
service
•	 
version
(Correct)
•	 
instance group
•	 
instance
Explanation
Option 2 is the correct answer.
Version supports migration. An app can contain multiple versions. You can migrate traffic to the new version by deploying with the --migrate parameter. So option 2 is the correct answer.

Options 1 and 4 are incorrect. A service is a higher level abstraction that represents the functionality of a microservice. An application has multiple services, but they serve different purposes. You can add and remove instances as needed, but they will only run one version of a service.

Option 3 is incorrect. Instance groups are part of Compute Engine, not an App Engine component.
Question 195: 
Skipped
You have released an application that runs on App Engine Standard. Demand has peaks and you need up to 12 instances at peak times.
However, 5 instances is sufficient for most other situations.
What's the best way to ensure you have enough instances to meet demand without overpaying?
•	 
Configure autoscaling for your app, specifying a maximum of 12 instances and a minimum of 5 instances
(Correct)
•	 
Configure basic scaling for your app, specifying a maximum of 12 instances and a minimum of 5 instances
•	 
Just before the peak period, create a cron job to add the instances. Terminate instances after the peak period ends
•	 
Configure app instance detection and do not specify maximum or minimum number of instances
Explanation
Option 1 is the correct answer.
Autoscaling allows you to set maximum and minimum number of instances. So option 1 is the correct answer.

Option 2 is incorrect. Basic scaling does not support maximum and minimum instance counts.

Option 3 is not recommended because peak loads are difficult to predict and even if schedules are predictable today, they may change over time.

Option 4 is incorrect. There is no instance detection option.
Question 196: 
Skipped
Which component is the lowest in the hierarchy of App Engine components?
•	 
application
•	 
instance
(Correct)
•	 
version
•	 
service
Explanation
Option 2 is the correct answer.
Instances are the lowest level components.

So option 2 is the correct answer. An application has one or more services. A service has one or more versions. A version runs on one or more instances when the application is running.
Question 197: 
Skipped
You plan to deploy multiple App Engine applications from your project. What did this plan fail to consider?
•	 
App Engine only supports one application per project
(Correct)
•	 
App Engine only supports two applications per project
•	 
App Engine application exists outside the project
•	 
none. this is a common pattern
Explanation
Option 1 is the correct answer.
A project can only support one App Engine app. So option 1 is the correct answer. If you want to run other applications, you have to put them in each project.
Question 198: 
Skipped
Which parameters can be configured for basic scaling?
•	 
max_instances and min_instances
•	 
idle_timeout and min_instances
•	 
idle_timeout and max_instances
(Correct)
•	 
idle_timeout and target_throughput_utilization
Explanation
Option 3 is the correct answer.
Basic scaling only allows you to specify idle time and maximum number of instances. So option 3 is the correct answer.

min_instances is not supported. target_throughput_utilization is an autoscaling parameter, not a base scaling parameter.
Question 199: 
Skipped
What is the runtime parameter in app.yaml used to specify?
•	 
script to run
•	 
URL to access the application
•	 
language runtime environment
(Correct)
•	 
Maximum time the application can run
Explanation
Option 3 is the correct answer.
The runtime parameter specifies the language environment in which to run. So option 3 is the correct answer.
The script to run is specified in the script parameter. The URL to access the application is based on the project name and domain appspot.com. There is no parameter that specifies the maximum time the application can run.
Question 200: 
Skipped
A team of developers has created an optimized version of the service. It runs 30% faster than before in most cases. The developer team wants to roll it out to all users right away, but you think you'll have to release large-scale changes over time in case there are major bugs.
How can you assign only some users to the new version without exposing it to all users?
•	 
Issue the command gcloud app services set-traffic
(Correct)
•	 
Issue the command gcloud instances services set-traffic
•	 
Issue the command gcloud app set-traffic
•	 
Change the destination IP address of the service for some clients
Explanation
Option 1 is the correct answer.
The correct response is gcloud app services set-traffic.

Option 2 is incorrect. You don't need instances.

Option 3 is incorrect. services not specified.

Option 4 is incorrect. Requires changes on the client side.
Question 201: 
Skipped
What parameter do you use with gcloud app services set-traffic to specify the traffic percentage for each instance?
•	 
--split-by
•	 
--splits
(Correct)
•	 
--split-percent
•	 
--percent-split
Explanation
Option 2 is the correct answer.
--splits is a parameter that specifies the list of instances and the percentage of traffic they receive.
Option 2 is correct. None of the other choices are valid parameters for the gcloud app services set-traffic command.
Question 202: 
Skipped
What component status can you see in the App Engine console?
•	 
service only
•	 
version only
•	 
instance and version
•	 
service, version, instance
(Correct)
Explanation
Option 4 is the correct answer.
The App Engine console shows a list of services and versions, as well as the usage of each instance.
Question 203: 
Skipped
Which of the following is a valid choice for traffic splitting criteria?
•	 
IP address only
•	 
HTTP cookies only
•	 
Random and IP Address Only
•	 
IP address, HTTP cookie, random
(Correct)
Explanation
Option 4 is the correct answer.
IP address, HTTP cookies, and random splitting are all three methods of splitting traffic.
Question 204: 
Skipped
A product manager is proposing a new application. This application requires multiple backend services, three business logic services, and access to a relational database.
Each service provides a single function, and multiple of these services are required to complete a business task. Service execution time depends on the size of the input and is expected to take up to 30 minutes in some cases. Which Google Cloud product is the best serverless option for running this intertwined service?
•	 
Cloud Functions
•	 
Compute Engine
•	 
App Engine
(Correct)
•	 
Cloud Storage
Explanation
Option 3 is the correct answer.
App Engine is designed to support the tightly coupled services that make up your application. This is different from Cloud Functions.
Cloud Functions support single-purpose functions that operate independently in response to isolated events in the Google Cloud and are designed to complete within a specified time period.
Compute Engine is not a serverless option.
Cloud Storage is not a computing product.
Question 205: 
Skipped
All of the following items generate events that can be triggered using Cloud Functions.
However, there is one exception. which one.
•	 
Cloud Storage
•	 
Cloud Pub/Sub
•	 
SSL
(Correct)
•	 
Firebase
Explanation
Option 3 is the correct answer.
SSL is a secure protocol for accessing servers remotely. For example, it is used to access Compute Engine instances. It is not an event that can be triggered using Cloud Functions. Three other GCP products to choose from generate events with associated triggers.
Question 206: 
Skipped
You are designing a function to run on Cloud Functions.
The amount of memory required by this function exceeds the default amount of memory. Also, the function should only be applied when the finalize event occurs after the file has been uploaded to Cloud Storage.
Additionally, the function only applies logic to files of the standard image file type.
Which of the following required features cannot be specified in parameters and must be implemented in the function code?
•	 
Cloud Functions name
•	 
memory allocated to the function
•	 
File types to apply to the function
(Correct)
•	 
event type
Explanation
Option 3 is the correct answer.
There is no option to specify which file types the function applies to. However, you can specify the packets to which the function applies.
You can only store files or types that you work with in that bucket. Alternatively, the function can check the file type and then, depending on the type, decide whether or not to execute the rest of the function. All other choices are parameters of the Cloud Storage function.
Question 207: 
Skipped
What is the default timeout period for Cloud Functions to run?
•	 
30 seconds
•	 
1 minute
(Correct)
•	 
9 minutes
•	 
20 minutes
Explanation
Option 2 is the correct answer.
By default, Cloud Functions run for up to 1 minute before timing out. However, the Cloud Functions timeout parameter can be set for up to 9 minutes before timing out.
Question 208: 
Skipped
You want to create a Cloud Function that converts audio files to another format.
Audio files are uploaded to Cloud Storage. As soon as the file has finished uploading, it will start converting.
What trigger should you specify in Cloud Functions to perform the transformation after the file is uploaded?
•	 
google.storage.object.finalize
(Correct)
•	 
google.storage.object.upload
•	 
google.storage.object.archive
•	 
google.storage.object.metadataUpdate
Explanation
Option 1 is the correct answer.
The correct trigger is google.storage.object.finatize which is triggered when a file is uploaded
will be executed later.

Option 2 is incorrect. Not a valid trigger name.

Option 3 is incorrect. Triggers run when files are archived, not when they are uploaded.

Option 4 is incorrect. Triggers run when some metadata attribute changes, not just after a file upload.
Question 209: 
Skipped
You've been asked to deploy Cloud Functions to work with Cloud Pub/Sub.
While reviewing Python code, you noticed a reference to the Python function base64.b64decode.
Why do Pub/Sub Cloud Functions need the decode function?
•	 
Shouldn't be in a function, because it's not needed
•	 
Pub/Sub topic messages are encoded so that binary data can be used where text data is expected. A message must be decoded to be able to access the data in that message
(Correct)
•	 
A padding character should be added to the end of the message to make all messages the same length
•	 
The decode function maps data from the dictionary data structure to the list data structure
Explanation
Option 2 is the correct answer.
Messages are stored in text format base64 so that binary data can be stored in text format in messages.

Option 1 is incorrect. You need to map from binary encoding to standard text encoding.

Option 3 is incorrect. The function does not pad the blanks with additional characters or justify the length.

Option 4 is incorrect. Dictionary data types are not converted to list data types.
Question 210: 
Skipped
Your company has a web application that allows job seekers to upload resume files. Some of these files are Microsoft Word files, PDF files, text files, etc.
You want to save all your resumes as PDFs. How can you get the fastest upload-to-conversion time and the least amount of coding effort?
•	 
Create an App Engine application with multiple services and convert all documents to PDF
•	 
Implement Cloud Functions in Cloud Storage that run on the finalize event. The function checks the file type and if it's not PDF, calls the PDF conversion function. Write the PDF version to the bucket with the original file
(Correct)
•	 
Add the names of all files to a Cloud Pub/Sub topic and periodically run a batch job that converts the original files to PDF
•	 
Implement Cloud Functions in Cloud Pub/Sub that run on the finalize event. The function checks the file type and if it's not PDF, calls the PDF conversion function. Write PDF version to bucket with original file
Explanation
Option 2 is the correct answer.
Optionally, use the Cloud Storage finalize event to trigger the conversion. The delay from the time the file is uploaded to the time the conversion is completed is minimal. Option 1 is also possible, but requires more coding than option 2.

Option 3 is not the best option as the files will not be converted until the batch job is run.

Option 4 is incorrect. You cannot use the finalize event to create Cloud Pub/Sub Cloud Functions. This event is for Cloud Storage, not Cloud Pub/Sub.
Question 211: 
Skipped
What types of triggers allow developers to invoke Cloud Functions using HTTP POST, GET, and PUT calls?
•	 
HTTP
(Correct)
•	 
Webhooks
•	 
Cloud HTTP
•	 
none of the above
Explanation
Option 1 is the correct answer.
HTTP triggers can use POST, GET, and PUT calls. Webhooks and Cloud HTTP do not have valid trigger types.
Question 212: 
Skipped
You have been tasked with defining a lifecycle configuration on a Cloud Storage bucket.
All possible options should be considered for changing the storage class. All of the following changes are possible.
However, there is one exception. Which are the exceptions?
•	 
Nearline to Coldline
•	 
Regional to Nearline
•	 
Multi-Regional to Coldline
•	 
Regional to Multi-Regional
(Correct)
Explanation
Option 4 is the correct answer.
Once a bucket is created as Regional or Multi-Regional, it cannot be changed to Multi-Regional or Regional.
You can change from Nearline to Coldline and Regional to Nearline. You can also change from Multi-Regional to Coldline.
Question 213: 
Skipped
Your manager has asked me for help in reducing my Cloud Storage charges. Some files stored in Cloud Storage are rarely accessed. What kind of storage would you recommend for such files?
•	 
Nearline
•	 
Regional
•	 
Coldline
(Correct)
•	 
Multi-Regional
Explanation
Option 3 is the correct answer.
Since your goal is to reduce costs, you should use the lowest cost storage option. Coldline has the lowest cost at $0.07 USD/GB per gigabyte per month, making option 3 the correct answer.

Nearline has the next lowest cost, followed by Regional. Multi-Regional has the highest price per gigabyte. Both Nearline and Coldline incur access charges, which are not taken into account in this issue.
Question 214: 
Skipped
You work for a start-up company that develops analytics software for IoT data.
You constantly need to ingest large amounts of data and store that data for months. Multiple applications need to see this data at this start-up company.
Data volumes are expected to grow to petabyte scale. Which database should you use?
•	 
Cloud Spanner
•	 
Bigtable
(Correct)
•	 
BigQuery
•	 
Datastore
Explanation
Option 2 is the correct answer.
Bigtable is a columnar database that can ingest large amounts of data in a consistent manner. So option 2 is the correct answer. It also delivers sub-millisecond latency and is a good option for supporting queries.
Cloud Spanner is a global relational database and is not suitable for fast ingestion of large amounts of data. Datastore is an object data model, not suitable for IoT or other time series data. BigQuery is an analytics database and is not intended for ingesting large amounts of data in a short period of time.
Question 215: 
Skipped
Cloud SQL is a fully managed relational database service, but some tasks must be performed by a database administrator. Which of the following tasks should a Cloud SQL user perform?
•	 
Apply security patches
•	 
Running regularly scheduled backups
•	 
Create database
(Correct)
•	 
Operating system tuning for optimal Cloud SQL performance
Explanation
Option 3 is the correct answer.
Database creation is the responsibility of a Cloud SQL database administrator or other user. Google applies security patches and performs other maintenance.

Option 1 is incorrect. GCP performs regularly scheduled backups.

Option 2 is incorrect. Database administrators must schedule backups, but GCP ensures that backups run on schedule.

Option 4 is incorrect. Cloud SQL users cannot SSH into Cloud SQL servers, so operating system tuning is not possible. However, this is not a problem. Appropriately managed by Google.
Question 216: 
Skipped
Which of the following commands should you use to create a backup of your Cloud SQL database?
•	 
gcloud sql backups create
(Correct)
•	 
gsutil sql backups create
•	 
gcloud sql create backups
•	 
gcloud sql backups export
Explanation
Option 1 is the correct answer.
Cloud SQL is controlled with the gcloud command. The order of words in gcloud commands is gcloud, services (in this case SQL), resources (in this case backups), commands or verbs (in this case create). Option 1 is the correct answer.

Option 2 is incorrect. gsutil is used to work with Cloud Storage, not Cloud SQL.

Option 3 is incorrect. Word order is incorrect. Backups should be before create.

Option 4 is incorrect. The correct command or verb is create.
Question 217: 
Skipped
You ran an export from a Dataproc cluster. what was exported?
•	 
Data in Spark DataFrames
•	 
All tables in the Spark database
•	 
Configuration data about the cluster
(Correct)
•	 
All tables in Hadoop database
Explanation
Option 3 is the correct answer
Exporting from Dataproc exports data about your cluster configuration. So option 3 is the correct answer.

Option 1 is incorrect. DataFrames data is not exported.

Option 2 is incorrect. Spark does not have tables that store data persistently like relational databases do.

Also option 4 is incorrect. No data is exported from Hadoop.
Question 218: 
Skipped
A data scientist team has asked for access to data stored in Bigtable so that they can train machine learning models. The data scientist team explains that Bigtable lacks the features needed to build machine learning models.
Which of the following GCP services might your data scientist team use to build machine learning models?
•	 
Datastore
•	 
Dataflow
•	 
Dataproc
(Correct)
•	 
Data Analyze
Explanation
Option 3 is the correct answer
Services Dataproc supports Apache Spark with libraries for machine learning.

Option 1 and option 2 are incorrect. None are analytics or machine learning services.

Also option 4 is incorrect. DataAnalyze is not a real service.
Question 219: 
Skipped
Which of the following is the correct command to create a Pub/Sub topic?
•	 
gcloud pubsub topics create
(Correct)
•	 
gcloud pubsub create topics
•	 
bq pubsub create topics
•	 
cbt pubsub topics create
Explanation
Option 1 is the correct answer
The correct command for option 1 uses gcloud, followed by the service (in this case pubsub), then the resource (in this case topics), then the verb (in this case create).

Option 2 is incorrect. The last two words are out of order.

Options 3 and 4 are incorrect. You are not using gcloud.
bq is BigQuery's command-line tool. cbt is Bigtable's command line tool.
Question 220: 
Skipped
Which of the following commands creates a subscription on topic ace-exam-topic1 ?
•	 
gcloud pubsub create --topic=ace-exam-topic1 ace-exam-sub1
•	 
gcloud pubsub subscriptions create --topic=ace-exam-topic1
•	 
gcloud pubsub subscriptions create --topic-ace-exam-topic1 ace-exam-sub1
(Correct)
•	 
gsutit pubsub subscriptions create --topic=ace-exam-topic1 ace-exam-sub1
Explanation
Option 3 is the correct answer
Use gcloud pubsub subscriptions create followed by the topic and subscription name.

Option 1 is incorrect. The word subscriptions is missing.

Option 2 is incorrect. Missing subscription name.

Also option 4 is incorrect. That's because you're using gsutil, not gcloud.
Question 221: 
Skipped
Which is one of the direct advantages of using message queues in distributed systems?
•	 
Enhanced security
•	 
Separate services. So if one service is delayed, the others are not delayed.
(Correct)
•	 
More programming languages supported
•	 
Store messages until read by default
Explanation
Option 2 is the correct answer
Using message queues between services provides service isolation. Therefore, a delay in one service does not cause delays in other services. So option 2 is the correct answer.

Option 1 is incorrect. Adding Message Queuing does not directly mitigate security risks that can exist in distributed systems, such as overly lax permissions.

Option 3 is incorrect. Adding queues is not directly related to programming language.

Also option 4 is incorrect. By default, message queues have no retention period.
Question 222: 
Skipped
What command should you run to verify that the beta gcloud command is installed?
•	 
gcloud components beta install
•	 
gcloud components install beta
(Correct)
•	 
gcloud commands install beta
•	 
gcloud commands beta install
Explanation
Option 2 is the correct answer
Followed by gcloud components, followed by install, beta.

Option 1 is incorrect. The order of beta and install is wrong.

Also option 3 and option 4 are incorrect. It uses commands instead of components.
Question 223: 
Skipped
Which parameter does BigQuery use to automatically detect a file's schema on import?
•	 
--autodetect
(Correct)
•	 
--autoschema
•	 
--detectschema
•	 
--dry_run
Explanation
Option 1 is the correct answer
The correct parameter name is autodetect and option 1 is correct.

Options 2 and 3 are not valid bq parameters.

Also, option 4 is a valid parameter, but returns the estimated size of the data scanned during query execution.
Question 224: 
Skipped
Which command creates a VPC from the command line?
•	 
gcloud compute networks create
(Correct)
•	 
gcloud networks vpc create
•	 
gsutil networks vpc create
•	 
gcloud compute create networks
Explanation
Option 1 is the correct answer.
Option 2 is incorrect. networks vpc is not a valid command element.
Option 3 is incorrect. gsutil is a command used to work with Cloud Storage.
Option 4 is incorrect. Wrong order of options.
Question 225: 
Skipped
Which firewall rule item determines whether the rule applies to inbound or outbound traffic?
•	 
action
•	 
subject
•	 
priority
•	 
direction
(Correct)
Explanation
Option 4 is the correct answer
Direction specifies whether the rule applies to incoming or outgoing traffic. So option 4 is the correct answer.

Option 1 is incorrect. Action is Allow or Block.

Option 2 is incorrect. Targets specify the set of instances to which the rule applies.

Option 3 is incorrect. Priority determines which of all matching rules is applied.
Question 226: 
Skipped
You would like to define a CIDR range that applies to all destination addresses. Which IP address do you specify?
•	 
0.0.0.0/0
(Correct)
•	 
10.0.0.0/8
•	 
172.16.0.0/12
•	 
192.168.0.0/16
Explanation
Option 1 is the correct answer
0.0.0.0/0 matches all IP addresses.

Option 2 is a block of 16,777,214 addresses.

Option 3 is a block of 1,048,574 addresses.

Option 4 is a block of 65,534 addresses. You can experiment with CIDR blocking options using a CIDR calculator tool, such as the one provided at www.subnet-calculator.com/cidr.php.
Question 227: 
Skipped
Which of the following commands will perform an automatic backup on the instance ace-exam-mysql at 3:00 AM?
•	 
gcloud sql instances patch ace-exam-mysql--backup-start-time 03:00
(Correct)
•	 
gcloud sql databases patch ace-exam-mysql --backup-start-time 03:00
•	 
cbt sql instances patch ace-exam-mysql--backup-start-time 03:00
•	 
bq gcloud sql instances patch ace-exam-mysql --- backup-start-time 03:00
Explanation
Option 1 is the correct answer.
The basic command is gcloud sql instances patch. followed by the instance name and the start time passed in the --backup-start-time parameter.
Question 228: 
Skipped
Which query language does Datastore use?
•	 
SQL
•	 
MDX
•	 
GQL
(Correct)
•	 
DataFrames
Explanation
Option 3 is the correct answer.
Datastore uses the SQL-style query language GQL.

Option 1 is incorrect. SQL is not used with this database.

Option 2 is incorrect. MDX is a query language for online analytical processing (OLAP) systems.

Option 4 is incorrect. DataFrames are data structures used in Spark.
Question 229: 
Skipped
Which of the following commands is used to create a Cloud Storage bucket?
•	 
gcloud storage buckets create
•	 
gsutil storage buckets create
•	 
gsutil mb
(Correct)
•	 
gcloud mb
Explanation
Option 3 is the correct answer.
gsutil is a command-line utility for working with Cloud Storage. Cloud Storage is one of the few GCP services that doesn't use gcloud (BigQuery and Bigtable are other GCP services that don't use gcloud). The mb (short for make bucket) following gsutil is the verb to create a bucket.
Question 230: 
Skipped
You need to copy the files on your local device to a Cloud Storage bucket.
What commands can you use? You assume that you have the Cloud SDK installed on your local computer.
•	 
gsutil copy
•	 
gsutil cp
(Correct)
•	 
gcloud cp
•	 
gcloud storage objects copy
Explanation
Option 2 is the correct answer.
gsutil is a command to copy files to Cloud Storage.

Option 1 is incorrect. The verb is cp, not copy.

Also option 3 and option 4 are incorrect. The command-line utility for working with Cloud Storage is gsutil, not gcloud.
Question 231: 
Skipped
You are migrating a large number of files from local storage to Cloud Storage.
You prefer to use the Cloud Console rather than scripting. Which of the following Cloud Storage operations can be performed in the console?
•	 
File upload only
•	 
folder upload only
•	 
File and folder upload
(Correct)
•	 
Use the diff command to compare local files with files in a bucket
Explanation
Option 3 is the correct answer.
The console allows you to upload both files and folders.

Options 1 and 2 are incorrect. There are no operations that can be performed on the console.

Also option 4 is incorrect. Cloud Console does not have a diff operation.
Question 232: 
Skipped
Is a Virtual Private Cloud a resource that falls under which of the following?
•	 
zone
•	 
region
•	 
super region
•	 
global
(Correct)
Explanation
Option 4 is the correct answer.
A Virtual Private Cloud is global.
By default, every region has a subnet. Resources in each region can be accessed from a VPC.
Question 233: 
Skipped
You have been tasked with defining the CIDR ranges used in your project.
The project has two VPCs with multiple subnets in each VPC. How many CIDR ranges do you need to define?
•	 
1 for each VPC
•	 
one for each subnet
(Correct)
•	 
one for each region
•	 
one for each zone
Explanation
Option 2 is the correct answer.
IP ranges are assigned to subnets. Option 2 is the correct answer. Each subnet is assigned IP addresses exclusively. IP ranges are assigned network constructs, not zones or regions. A VPC has multiple subnets, but each subnet has its own address range.
Question 234: 
Skipped
The legal department should isolate resources in their own VPC. Networks must provide routing to other services available in the global network. The VPC network has not learned global routes. Which parameter might be missing when creating a VPC subnet?
•	 
DNS server policy
•	 
dynamic routing
(Correct)
•	 
static routing policy
•	 
system routing policy
Explanation
Option 2 is the correct answer.
Dynamic routing is a parameter that specifies whether routes are learned regionally or globally.

Option 1 is incorrect. DNS is a name resolution service and has nothing to do with routing.

Option 3 is incorrect. There are no static routing policy parameters.

Option 4 is incorrect. System routing is not really an option.
Question 235: 
Skipped
Which record type do you use to specify IPv4 addresses for your domain?
•	 
AAAA
•	 
A.
(Correct)
•	 
NS
•	 
SOA
Explanation
Option 2 is the correct answer.
A records are used to map domain names to IPv4 addresses.

Option 1 is incorrect. AAAA records are used with IPv6 addresses.

Option 3 is incorrect. NS is a nameserver record.

Also option 4 is incorrect. SOA is the start of authority record.
Question 236: 
Skipped
The CEO of the startup you work for has just read the latest report on the company under attack called Cash Poisoning. The CEO is considering implementing additional security measures to reduce the risk of DNS spoofing and cache poisoning. what do you suggest?
•	 
Use DNSSEC
(Correct)
•	 
Add SOA record
•	 
Add a CNAME record
•	 
Delete CNAME records
Explanation
Option 1 is the correct answer.
DNSSEC is a secure protocol designed to prevent spoofing and cache poisoning. So option 1 is the correct answer.

Options 2 and 3 are incorrect. SOA and CNAME records contain data about DNS records. These are not additional security measures.

Also option 4 is incorrect. Deleting the CNAME record does not improve security.
Question 237: 
Skipped
What does the TTL parameter specify for DNS records?
•	 
How long the record should be in the cache before querying it again
(Correct)
•	 
Time a client has to respond to a request for DNS information
•	 
Time allowed to create CNAME records
•	 
Time a human must manually verify information in DNS records
Explanation
Option 1 is the correct answer.
The TTL parameter specifies how long records are kept in the cache. After this time the data is queried again. So option 1 is the correct answer.

Option 2 is incorrect. This period is not related to timeouts.

Option 3 is incorrect. TTL is not related to time limits for data modification operations.

Also option 4 is incorrect. No manual review required.
Question 238: 
Skipped
What are Cloud Marketplace Categories?
•	 
dataset only
•	 
Operating system only
•	 
Developer tools and operating system only
•	 
Datasets, Operating Systems, Developer Tools
(Correct)
Explanation
Option 4 is the correct answer.
Solution categories include all categories mentioned above. So option 4 is the correct answer. Others include Kubernetes Apps, APIs & Services, and Databases.
Question 239: 
Skipped
Where can you launch Cloud Marketplace solutions?
•	 
Solution overview page
(Correct)
•	 
Cloud Marketplace main page
•	 
Network Services
•	 
none of the above
Explanation
Option 1 is the correct answer.
To launch the solution, click Launch under the Compute Engine link on the overview page. Option 1 is the correct answer.

Option 2 is incorrect. The main page displays overview information about the product.

Option 3 is incorrect. Network services are not related to this topic.

Option 4 is incorrect.
Question 240: 
Skipped
You need a quick look at the set of operating systems available on Cloud Marketplace. Which procedure would be useful in such a case?
•	 
Find listings on the web using Google Search
•	 
Using filters in Cloud Marketplace
(Correct)
•	 
Scroll through the list of solutions displayed on the Cloud Marketplace start page
•	 
Operating system filtering is not possible
Explanation
Option 2 is the correct answer.
Cloud Marketplace is a set of predefined filters, such as filtering by operating system. Option 2 is the correct answer.

Option 1 might give you the correct information in the end, but it's not efficient.

Option 4 is incorrect. Not practical for such a simple task.
Question 241: 
Skipped
What does IAM stand for?
•	 
Identity and Authorization Management
•	 
Identity and Access Management
(Correct)
•	 
Identity and Auditing Management
•	 
Individual Access Management
Explanation
Option 2 is the correct answer.
IAM stands for Identity and Access Management.
Question 242: 
Skipped
There is a bug in the microservice. You have checked the application output and cannot identify the problem. You decide that you need to review the code one at a time.
Which Stackdriver service should you use to gain insight into the state of the service at a particular point in its execution?
•	 
Monitoring
•	 
Logging
•	 
Trace
•	 
Debug
(Correct)
Explanation
Option 4 is the correct answer.
Debug is used to generate a snapshot. The snapshot shows the status of the application at a particular point in its execution. Therefore, option 4 is correct.

Option 1 is incorrect. Monitoring is used to notify the development operations engineer when a resource is not functioning as expected.

Option 2 is incorrect. Logging collects, stores, and displays log data.

Option 3 is incorrect. Cloud Trace is a distributed tracing application that provides detailed information about the time it takes to execute different pieces of code.
Question 243: 
Skipped
There may be a problem with BigQuery at us-central. Where can you check the status of the BigQuery service to see the details in the simplest way?
•	 
Email Google Cloud Support
•	 
Check https://status.cloud.google.com/
(Correct)
•	 
Check https://bigquery.status.cloud.google.com/
•	 
Call Google Technical Support
Explanation
Option 2 is the correct answer.
Option 2 is correct because the Google Cloud Status Dashboard (https://status.cloud.google.com/) displays information about the status of GCP services.
Question 244: 
Skipped
Which Stackdriver service is used to generate an alert when a virtual machine's CPU usage exceeds 80%?
•	 
Logging
•	 
Monitoring
(Correct)
•	 
Cloud Trace
•	 
Cloud Debug
Explanation
Option 2 is the correct answer.
The Monitoring service is used to set thresholds on metrics and generate alerts when metrics exceed thresholds for a specific period of time. So option 2 is the correct answer.

Option 1 is incorrect. Logging is to collect logged events.

Option 3 is incorrect. Cloud Trace is an application trace.

Option 4 is incorrect. Debug is used to debug your application.
Question 245: 
Skipped
You have created a virtual machine. You would like Stackdriver Monitoring to send an email alert when the average CPU usage exceeds 75% over a 5 minute period. What should you do with the virtual machine for this?
•	 
Install the Stackdriver workspace
•	 
Install the Stackdriver Monitoring agent on your virtual machine
(Correct)
•	 
Edit the virtual machine configuration in the Cloud Console and check the "Monitor With Stackdriver" checkbox
•	 
Configure notification channels
Explanation
Option 2 is the correct answer.
You need to install the Monitoring agent on your virtual machine. Option 2 is the correct answer because the agent collects the data and sends it to Stackdriver.

Option 1 is incorrect. Workspace is not installed on virtual machines. Created with Stackdriver.

Option 3 is incorrect. There is no Monitor with Stackdriver checkbox on the virtual machine configuration form.

Option 4 is incorrect. Set up notification channels in Stackdriver, not in your virtual machine.
Question 246: 
Skipped
Where can you monitor resources with Stackdriver?
•	 
Google Cloud Platform only
•	 
Google Cloud Platform & Amazon Web Services only
•	 
Google Cloud Platform and on-premises data centers
•	 
Google Cloud Platform, Amazon Web Services, on-premises data centers
(Correct)
Explanation
Option 4 is the correct answer.
Option 4 is the correct answer, as Stackdriver can monitor resources in GCP, AWS, and on-premises data centers.

Any other option is incorrect. Two other options are not included.
Question 247: 
Skipped
Which of the following can have a hierarchical structure?
•	 
Organization resource only
•	 
Folder resource only
•	 
Folder and project resources
(Correct)
•	 
Project resource only
Explanation
Option 3 is the correct answer.
In the resource hierarchy, there is one organization at the root and folders under which other folders or projects can be placed. A folder can contain multiple folders and multiple projects.
Question 248: 
Skipped
The company you work for has many policies that must apply to all projects.
You decide to apply a policy to your resource hierarchy. Shortly after applying the policy, an engineer noticed that an application that worked before the policy was implemented no longer works.
The engineer wants the application to make an exception. How can you override a policy inherited from another entity in my resource hierarchy?
•	 
Inherited policies can be overridden by defining policies at the folder or project level
•	 
Inherited policies cannot be overridden
(Correct)
•	 
Policy can be overridden by linking to a service account
•	 
Policy can be overridden by linking to billing account
Explanation
Option 2 is the correct answer.
Define a policy at the folder or project level to override the inherited policy. Service accounts and billing accounts are not part of the resource hierarchy and are not involved in policy overrides.
Question 249: 
Skipped
You are deploying a new custom application and would like to delegate some of the administrative tasks to your development and operations engineers.
Engineers do not need all the rights of a full application administrator, but they do need a subset of those rights. What kind of role should be used to grant these permissions?
•	 
basic role
•	 
predefined roles
•	 
Advanced role
•	 
custom role
(Correct)
Explanation
Option 4 is the correct answer.
Basic roles include Owner, Editor, and Viewer. Predefined roles are for GCP products and services such as App Engine and BigQuery. In custom applications, you can create permissions to give users the minimum required role.
Question 250: 
Skipped
An application administrator is responsible for managing all resources within a project.
This application administrator wants to delegate some of the responsibilities of the service account to another administrator. If additional service accounts are created, other administrators will also need to manage them. What is the best way to delegate the permissions required to manage service accounts?
•	 
Grant iam.serviceAccountUser to administrators at the project level
(Correct)
•	 
Grant iam.serviceAccountUser to administrators at the service account level
•	 
Grant iam.serviceProjectAccountUser to administrators at the project level
•	 
Grant iam.serviceProjectAccountUser to administrators at the service account level
Explanation
Option 1 is the correct answer.
Granting iam.serviceAccountUser to a user at the project level allows that user to manage all service accounts in the project. When a new service account is created, it is automatically granted permission to manage service accounts. You can grant iam.serviceAccountUser to administrators at the service account level, but you must set the role for all service accounts. When a new service account is created, the account administrator must grant iam.serviceAccountUser to other administrators of the new service account.
iam.serviceProjectAccountuser is an incorrect role.
Continue
Retake test
Question 1: Incorrect
Your company is developing an image distribution application. This application stores and shares images in a Cloud Storage bucket called "test1".
Which command should you use to allow the image to be read by all users?
•	 
gsutil iam ch -d objectViewer: allUsers gs: // test1
•	 
gsutil iam ch -d allUsers: Viewer gs: // test1
•	 
gsutil iam ch allUsers: objectViewer gs: // test1
(Correct)
•	 
gcloud iam ch objectViewer: allUsers gs: // test1
(Incorrect)
Explanation
Option 3 is the correct answer. By running the'gsutil iam ch allUsers: objectViewer gs: // test1'command, you can set permissions for all users to read the images stored in the Cloud Storage bucket called test1. Use gsutil instead of gcloud for commands for Cloud Storage. ObjectViewer is also the correct way to grant read access to objects in your bucket. The gsutil iam ch command grants all users read permission for the images stored in the bucket. By writing gsutil iam ch allUsers: objectViewer after it, specify all users and set the read permission of the object.
Options 1 and 2 are incorrect. Granting -d removes the permission.
Option 4 is incorrect. gcloud iam ch objectViewer: allUsers gs: // test1 is not a valid command because "objectViewer: allUsers" is reversed.
【reference】  https://cloud.google.com/storage/docs/gsutil/commands/iam
Question 2: Correct
As an infrastructure administrator, you are backing up your Cloud Datastore. The backup file should be stored in a bucket named test1-backup.
Which command should you use to perform the backup?
•	 
gcloud datastore backup gs: // test1-backup
•	 
gcloud datastore indexes gs: // test1-backup
•	 
gcloud datastore operations gs: // test1-backup
•	 
gcloud datastore export gs: // test1-backup --async
(Correct)
Explanation
Option 4 is the correct answer. You can export Cloud Datastore entities to Cloud Storage by running the command `gcloud datastore export [destination url]`. The gcloud datastore command is a command for managing Cloud Datastore. gcloud datastore export is a command to execute data extraction, and it is executed by specifying the output destination as gcloud datastore export [output destination url] `.
The main actions are:
· Export: exports data to Google Cloud Storage
· Import: Imports data from Google Cloud Storage
· Indexes: Manages CloudFirestore indexes.
· Operations: Manages long-running operations of Cloud Firestore.
Option 1 is incorrect. The command gcloud datastore backup is not a valid command.
Option 2 is incorrect. indexes manages Cloud Firestore indexes.
Option 3 is incorrect. operations manage the long-running operations of Cloud Firestore.
【reference】  https://cloud.google.com/datastore/docs/export-import-entities
Question 3: 
Skipped
As a developer, you are developing an application that performs reads and writes on two Cloud Storage buckets. What settings do applications need to have to get permission to read and write to this Cloud Storage bucket?
•	 
Create a service account and grant a bucket policy that provides read and write permissions.
•	 
Create a federation ID and grant a bucket policy that provides read and write permissions.
•	 
Create a service account and grant it a role that provides read and write permissions.
(Correct)
•	 
Create a federation ID and grant a bucket policy that provides read and write permissions.
Explanation
Option 3 is the correct answer. Service accounts are non-human users and are used by applications and the like to access resources on the cloud without the involvement of end users or to perform actions. You can control permissions on resources by assigning an IAM role to a service account. Therefore, you need to grant a service account for your application to allow reads and writes to this Cloud Storage bucket.
Option 1 is incorrect. Control permissions with an IAM role, not a bucket policy.
Options 2 and 4 are incorrect. Federation ID is an authorization method used when delegating authentication to an external identity provider. You can use this to authenticate to your application or Google Cloud. Not appropriate for this requirement.
【reference】 
https://cloud.google.com/iam/docs/best-practices-for-using-and-managing-service-accounts
https://cloud.google.com/storage/docs/access-control/iam
Question 4: 
Skipped
You are migrating your database system in your on-premises environment to Google Cloud. You have set a firewall rule to control ingress and egress traffic to the network for the VM, but it seems that the required traffic is blocked.
What is the method for diagnosing firewall traffic conditions?
•	 
Enable logging for subnets where firewalls are installed.
•	 
Enable firewall rule logging for firewall rules
(Correct)
•	 
Enable logging for VPCs with firewalls installed.
•	 
Monitor firewall rules using CloudDebugger.
Explanation
Option 2 is the correct answer. You can use the firewall rule log to audit, validate, and analyze the effectiveness of firewall rules. For example, you can determine if a firewall rule designed to deny traffic is working as intended. Therefore, by analyzing the firewall rule log, it is possible to verify the communication status to see if the required traffic is blocked.
Option 1 is incorrect. There is no log called the log of the subnet where the firewall is installed. Use the VPC flow log to check access to the subnet.
Option 3 is incorrect. The VPC flow log records the network flow sent and received by the VM instance, and it is not possible to check the traffic status based on the firewall.
Option 4 is incorrect. Cloud Debugger is a service that allows you to investigate the status of a running application in real time without stopping or slowing down the execution speed. It is not possible to check the traffic status based on the firewall.
【reference】  https://cloud.google.com/vpc/docs/firewall-rules-logging
Question 5: 
Skipped
Your company has decided to set up a Docker container-based CI / CD environment on Google Cloud. You need to create a set of Docker images to better manage groups of containers.
Which command would you run to group the container images?
•	 
Add labels using the "gcloud container images add-label" command.
•	 
Add the configuration using the gcloud container images add-configuration command.
•	 
Add settings using the gcloud beta container images list-tags command.
•	 
Add tags using the gcloud container images add-tag command.
(Correct)
Explanation
Option 4 is the correct answer. You can add tags to images by running the command `gcloud container images add-tag [image name before tagging] [image name after tagging]`. You can use this tag to manage your images. gcloud container is a command that deploys and manages a cluster of machines to run the container. gcloud beta container images add-tag allows you to tag images. Add the actual image name, such as `gcloud container images add-tag [image name before tagging] [image name after tagging]`.
Option 1 is incorrect. gcloud container images add-label is not a valid command.
Option 2 is incorrect. gcloud container images add-configuration is not a valid command.
Option 3 is incorrect. gcloud beta container images list-tags lists the tags.
【reference】 https://cloud.google.com/container-registry/docs/managing#tagging_images
Question 6: 
Skipped
As an engineer, you are building an application on Google Cloud that uses a distributed architecture. This application uses Cloud Pub / Sub to send messages to components for data analysis. Each message only needs to be sent once, but it seems that it can be sent multiple times by mistake.
Which parameters should be investigated to determine the cause?
•	 
--dead-letter-topic
•	 
--ack-deadline
(Correct)
•	 
--min-retry-delay
•	 
--max-retry-delay
Explanation
Option 2 is the correct answer. --ack-deadline specifies the number of seconds Cloud Pub / Sub publishers will wait on a retry. If it cannot confirm that the subscriber has received the message within the specified number of seconds, it will retry.
If this value is too small, the subscriber may receive the message correctly, but the message may be sent multiple times. It is necessary to identify the cause of multiple messages being sent by mistake. Therefore, check --ack-deadline to see if the optimal send interval has been set.
Option 1 is incorrect. --dead-letter-topic is a topic that forwards undeliverable messages (dead letter topic). This time it's not undeliverable, so it's irrelevant.
Option 3 is incorrect. --min-retry-delay is the minimum send interval (time) for sending messages in succession. It's possible that the optimal number of seconds to send, not the send interval, is the issue, so check --ack-dead-line.
Option 4 is incorrect. --max-retry-delay is the maximum send interval (time) for sending messages in succession. It's possible that the optimal number of seconds to send, not the send interval, is the issue, so check --ack-dead-line. 
【reference】 https://cloud.google.com/sdk/gcloud/reference/beta/pubsub/subscriptions/update
Question 7: 
Skipped
You have been asked to check the default region and zone settings.
Which command line commands do you use to display the default regions and zones for a particular project?
•	 
gcloud compute project-info describe --project [PROJECT-ID]
(Correct)
•	 
gcloud compute project-info add-metadata --project = [PROJECT-ID]
•	 
gcloud compute project-info describe --project = [PROJECT-ID]
•	 
gcloud compute project-info add-metadata --project [PROJECT-ID]
Explanation
Option 1 is the correct answer. You can get the compute service settings for the specified project by running the command `gcloud compute project-info describe --project [project ID]`. Use the `gcloud compute` command because the default region and zone settings are for compute services. By adding project-info describe, it becomes a command to get a specific project setting.
Option 3 is incorrect. gcloud compute project-info describe --project = [PROJECT-ID] Instead, enter the project name with gcloud compute project-info describe --project [project ID] in between.
Options 2 and 4 are incorrect. gcloud compute project-info add-metadata is a command used to set metadata.
【reference】 https://cloud.google.com/sdk/gcloud/reference/compute/project-info
Question 8: 
Skipped
As a developer, you are trying to create an image named testimage1 from a snapshot named test1.
Which command should you use?
•	 
gcloud compute disks create testimage1 --source-snapshot = test1
•	 
gcloud compute images create testimage1 --source-snapshot = test1
(Correct)
•	 
gcloud images create testimage1 --source-snapshot = test1
•	 
gcloud create images create testimage1 --source-snapshot = test1
Explanation
Option 2 is the correct answer. You can create a new virtual image from a snapshot by running the command `gcloud compute images create [new image name] --source-snapshot = [snapshot name]`.
Option 1 is incorrect. The gcloud compute disks create testimage1 is a command that creates disks to attach to a virtual machine. 
Option 3 is incorrect. gcloud images create testimage1 is out of order and is not a valid command.
Option 4 is incorrect. gcloud create images create testimage1 is out of order and is not a valid command.
【reference】 https://cloud.google.com/sdk/gcloud/reference/compute/disks/create#--source-snapshot
Question 9: 
Skipped
You have set up a persistent disk to connect to an instance running as a service account. Then you want to set the metadata for the instance that runs as a service account.
Select the role you need to do this.
•	 
roles / iam.instanceAdmin.v1
•	 
roles / compute.serviceAccountCreator
•	 
roles / iam.serviceAccountCreator
•	 
roles / compute.instanceAdmin.v1
(Correct)
Explanation
Option 4 is the correct answer. Roles / compute.instanceAdmin.v1 is required to populate the instance with metadata. Members with both roles / compute.instanceAdmin.v1 and roles / iam.serviceAccountUser can create and manage instances that use the service account. Specifically, the following permissions are set:
-Create an instance to run as a service account
-Connecting a persistent disk to an instance running as a service account
-Setting instance metadata for an instance running as a service account
-Connect using SSH to an instance running as a service account
-Reconfiguring an instance running as a service account 

Option 1 is incorrect. There is no role called roles / iam.instanceAdmin.v1. Option 2 is incorrect. There is no role called roles / compute.serviceAccountCreator.
Option 3 is incorrect. roles / iam.serviceAccountCreator has the right to create a service account and not the right to operate resources as a service account.
【reference】 https://cloud.google.com/compute/docs/access/iam
Question 10: 
Skipped
Your company stores internal documents in Cloud Storage buckets and uses them as a shared document management system. As a security officer, you need to make sure that all data written to this storage is encrypted.
Which of the following is the procedure for encrypting data when storing it in a CloudStorage bucket?
•	 
Encryption is performed by default.
(Correct)
•	 
Enable encryption when creating a Cloud Storage bucket.
•	 
Set encryption in the life cycle policy settings.
•	 
Set encryption by bucket policy.
Explanation
Option 1 is the correct answer. All data stored in Cloud Storage is automatically encrypted by default. The encryption methods of Cloud Storage are as follows: 

■ Server-side encryption Encryption that takes place after Cloud Storage receives the data, where the data is encrypted and then written to disk and stored. Uses the following two encryption keys:
-Can be managed using your own encryption key. These keys act as additional encryption that enhances standard Cloud Storage encryption. 
-KMS allows you to create and manage cryptographic keys. These keys act as additional encryption that enhances standard Cloud Storage encryption. 

■ Client-side encryption Encryption takes place before the data is sent to Cloud Storage. Such data is sent encrypted to Cloud Storage, but it is also encrypted on the server side. 

Options 2, 3 and 4 are incorrect. Encryption when writing to storage is performed automatically and does not need to be set separately by the user.
【reference】 https://cloud.google.com/storage/docs/encryption
Question 11: 
Skipped
Your company is developing an application that performs data processing every 30 minutes. This application is developed on App Engine and runs batch jobs. There is a problem with the execution interval of this batch job and it needs to be improved.
Which file would you edit to fix this issue?
•	 
cron.yaml
(Correct)
•	 
config.yaml
•	 
setting.yaml
•	 
app.yaml
Explanation
Option 1 is the correct answer. cron.yaml is a file for setting a scheduled task (cron job) that operates at a specified time or at regular intervals. It is necessary to modify cron.yaml in order to modify the periodic processing such as batch jobs.

Option 2 is incorrect. A file called config.yaml does not exist as a configuration file for App Engine.
Option 3 is incorrect. The file setting.yaml does not exist as a configuration file for App Engine.
Option 4 is incorrect. app.yaml is a file that describes settings related to the application, the entire network, and the programming language runtime. This time it doesn't matter.

【reference】 https://cloud.google.com/appengine/docs/flexible/java/configuring-datastore-indexes-with-index-yaml#index_definitions
Question 12: 
Skipped
The company uses multiple Compute Engines to develop applications. This application needs redundancy to withstand zone failures by scaling up and down depending on the load.
Which feature of Compute Engine do you want to use?
•	 
Unmanaged instance group
•	 
Instance template
•	 
Managed instance group
(Correct)
•	 
Resource group
Explanation
Option 3 is the correct answer. Managed instance groups allow multiple application operations on the same VM. This is best used when creating a redundant configuration that can withstand zone failures. Managed instance groups provide features such as auto-scaling, auto-repair, multi-zone deployment, and auto-update for scalable and highly available workload processing.
Option 1 is incorrect. Unmanaged instance groups are a good place to load balance a group of heterogeneous instances, or if you need to manage your own instances. Therefore, it is easier to set up this requirement by using a managed instance group.
Option 2 is incorrect. Instance templates are templates that you can use to create virtual machine instances and managed instance groups, and are not relevant here.
Option 4 is incorrect. Resource groups are a feature in Cloud Monitoring to define a set of resources as a group and are not relevant here.
【reference】 https://cloud.google.com/compute/docs/instance-groups/
Question 13: 
Skipped
The development team used the service account to develop the application, but the error occurred due to poor access control between resources. You decide to identify the cause by viewing the audit log.
Which log should you check?
•	 
Policy deny audit log
(Correct)
•	 
Data access audit log
•	 
Management activity log
•	 
ID access audit log
Explanation
Option 1 is the correct answer. If an error occurs because access control between resources is not possible, access is denied by the policy. Therefore, it is necessary to check the status of policy denials and to check the policy denial audit log. The policy denial audit log is recorded when the Google Cloud service denies access to a user or service account due to a policy violation.
Option 2 is incorrect. The data access audit log is irrelevant this time because it records API calls to read resource configurations and metadata, as well as user-driven API calls to create, modify, and read user-supplied resource data.
Option 3 is incorrect. This time it is irrelevant because the management activity log records events related to API calls and other management actions that change the resource's configuration or metadata.
Option 4 is incorrect. There is no audit log named ID Access Audit Log.

【reference】 https://cloud.google.com/logging/docs/audit
Question 14: 
Skipped
As an infrastructure administrator, you manage Cloud Storage on Google Cloud. You want to know the creation time and content type of a stored object.
Which command do you use to get the metadata for the object?
•	 
gsutil version gs: // BUCKET_NAME / OBJECT_NAME
•	 
gsutil list gs: // BUCKET_NAME / OBJECT_NAME
•	 
gsutil describe gs: // BUCKET_NAME / OBJECT_NAME
•	 
gsutil stat gs: // BUCKET_NAME / OBJECT_NAME
(Correct)
Explanation
Option 4 is the correct answer. You can get the metadata of an object by running the command `gsutil stat gs: // [bucket name] / [object name]`.
Option 1 is incorrect. Running gsutil version will print the version of the gsutil tool.
Option 2 is incorrect. gsutil list is not a valid command.
Option 3 is incorrect. gsutil describe is not a valid command.

【reference】 https://cloud.google.com/storage/docs/gsutil/commands/stat
Question 15: 
Skipped
Your company must authorize the auditor to review the audit logs in order to perform an IT audit. Which of the following roles can be used to grant the minimum required permissions without granting more permissions than required to read the log?
•	 
roles / logging.privateLogReader
•	 
roles / logging.admin.privateLogReader
•	 
roles / logging.privateLogsViewer
(Correct)
•	 
roles / logging.admin.privateLogViewer
Explanation
Option 3 is the correct answer. roles / logging.privateLogViewer grants read permission to all logs, including access transparency logs and data access audit logs. Only the roles / logging.privateLogsViewer should be set because the auditor only needs to be granted read permission.

Option 1 is incorrect. There is no role called roles / logging.privateLogReader.
Option 2 is incorrect. Since roles / logging.admin grants full permissions to Logging, it grants more permissions than required. This is an inappropriate setting that violates the principle of least privilege.
Option 4 is incorrect. roles / logging.admin.privateLogViewer does not grant read permission for data access audit logs, which is an inappropriate setting.

【reference】 https://cloud.google.com/logging/docs/access-control
Question 16: 
Skipped
The company has decided to move a Windows server in an on-premises environment to Google Cloud. You would like to see a list of Windows Server images available as a server administrator.
Which command should you use for this?
•	 
gcloud compute images list --project windows-cloud --no-standard-images
(Correct)
•	 
gcloud compute images describe --project windows-cloud --no-standard-images
•	 
gcloud compute images list --project = windows-cloud --no-standard-images
•	 
gcloud compute images describe --windows-cloud
Explanation
Option 1 is the correct answer. You can get the Windows Server image list by executing the command `gcloud compute images list --project [image project name] --no-standard-images` with windows-cloud specified in [image project name]. increase.
Options 2 and 4 are incorrect. `gcloud compute images describe` Used to get detailed information about a single image.
Option 3 is incorrect. gcloud compute images list --project = = windows-cloud --project, not --no-standard-images  Enter the project name by opening half-width characters like windows-cloud.

【reference】 https://cloud.google.com/sdk/gcloud/reference/compute/images/list
Question 17: 
Skipped
You manage to grant roles to the member test-user@gmail.com for project name test1. Now it is necessary to grant the authority of the editor role to the project name test1.

Which command should you use?
•	 
gcloud projects add-iam-policy-binding test1 for'user: test-user@gmail.com' add'roles / editor'
•	 
gcloud projects add-iam-policy-binding test1 --member ='user: test-user@gmail.com'  --role ='roles / editor'
(Correct)
•	 
gcloud projects set-iam-policy-binding with --member'user: test-user@gmail.com'  --role'roles / editor'
•	 
gcloud projects add-iam-policy-binding --project = test1 --member ='user: test-user@gmail.com'  --role ='roles / editor'
Explanation
Option 2 is the correct answer. By executing the command gcloud projects add-iam-policy-binding test1 --member = [member name] --role = [role name] `, you can specify a specific project and set a role for the member. You can grant specific permissions.
Option 1 is incorrect. The command for 'user: test-user@gmail.com'add'roles / editor' is incorrect. By executing --member = [member name] --role = [role name] `, you can specify a specific project, set a role for a member, and grant specific privileges.
Option 3 is incorrect. There is no subcommand called set-iam-policy-binding.
Option 4 is incorrect. The subcommand --project = test1 does not exist.
【reference】 https://cloud.google.com/sdk/gcloud/reference/projects/add-iam-policy-binding
Question 18: 
Skipped
Your e-commerce (EC) company utilizes a MySQL database with a Cloud SQL server. You need to connect to your MySQL instance from the command line as a database administrator.
Which command do you use to connect to the database as the root user using the embedded client?
•	 
gcloud sql connect [INSTANCE-ID] --root = user
•	 
gsutil sql connect [INSTANCE-ID] --user = root
•	 
gsutil sql connect [INSTANCE-ID] --root = user
•	 
gcloud sql connect [INSTANCE-ID] --user = root
(Correct)
Explanation
Option 4 is the correct answer. You can specify and connect to a specific database by running the command `gcloud sql connect [instance name] --user = [database user name]`.
Option 1 is incorrect. The --root parameter does not exist.
Options 2 and 3 are incorrect. The `gsutil` command is used to operate Cloud Storage and is not relevant here.
【reference】 https://cloud.google.com/sdk/gcloud/reference/sql/connect
Question 19: 
Skipped
Your company stores EC site data in BigQuery and performs daily data analysis. You plan to run some big queries against BigQuery. You would like to know the cost of querying in advance.

Which bg command do you need to run?
•	 
Use bg command with the --cost flag.
•	 
Use bg command with the --dry_run flag.
(Correct)
•	 
Use bg commandwith the dryRun parameter.
•	 
Use bg command with the cost parameter.
Explanation
Option 2 is the correct answer. Queries are billed based on the number of bytes read, so specify the --dry_run flag of the bg command to get the estimated number of bytes to be read and estimate the price based on this number of bytes.

Option 1 is incorrect. The --cost flag does not exist in bg command

Option 3 is incorrect. The dryRun parameter does not exist in bg command

Option 4 is incorrect. The cost parameter does not exist in bg command

【reference】  https://cloud.google.com/bigquery/docs/best-practices-costs#bq
Question 20: 
Skipped
The application being developed by your company needs to work with multiple clients on Google Cloud. Therefore, it is necessary to switch between multiple working clients on the command line.
How should you switch clients?
•	 
Create a configuration for each client and activate it with the gcloudauth command.
•	 
Create a configuration for each client and activate it with the gcloud config configuration activate command.
(Correct)
•	 
Apply the configuration role for each client and activate it for each client with the gcloud config configuration activate command.
•	 
Apply the settings role for each client and it for each client with the gcloudauth command.
Explanation
Option 2 is the correct answer. You can quickly change the Cloud SDK (gcloud command) settings by running the command `gcloud config configurations activate [configuration name]`. This makes it easy to switch between working clients.
Option 1 is incorrect. The `gcloud auth` command is for authenticating your Google Cloud account and is not relevant here.
Options 3 and 4 are incorrect. Roles are permissions to Google Cloud resources that you can't use to change Cloud SDK (gcloud command) settings.

【reference】  https://cloud.google.com/sdk/gcloud/reference/config
Question 21: 
Skipped
You are using the Google Kubernetes Engine to set up your service. In doing so, you need to ensure that all internal clients send requests to stable internal IP addresses.
What type of service should you create?
•	 
ClusterIP
(Correct)
•	 
NodePort
•	 
LoadBalancer
•	 
ExternalName
Explanation
Option 1 is the correct answer. Specify the Cluster IP type for all internal clients to send requests to a stable internal IP address. When you create a Service of type ClusterIP, the Service is assigned an internal (virtual) IP address that can only be communicated within the cluster.
Option 2 is incorrect. If you specify the NodePort type, you can access the Service from the outside with the IP address of the Node and the port number specified in advance. The NodePort type is irrelevant to internal network communication and is therefore incorrect.
Option 3 is incorrect. If you specify the LoadBalancer type, you can use the load balancer provided by Google Cloud to expose the Service to the outside of the cluster. The LoadBalancer type is irrelevant to internal network communication and is incorrect.
Option 4 is incorrect. If you specify the ExternalName type, when traffic is received, it can be forwarded to the external site according to the CNAME addressed to the external domain specified in advance. It is irrelevant to allow all internal clients to send requests to a stable internal IP address, which is incorrect.
【reference】  https://cloud.google.com/kubernetes-engine/docs/concepts/service
Question 22: 
Skipped
You need to assign Google Cloud project management privileges to users who are assigned as new Google Cloud administrators.
What set of permissions do you need to manage a Google Cloud project?
•	 
resourcemanager.projects.get
•	 
resourcemanager.projects.get, resourcemanager.projects.getIamPolicy resourcemanager.projects.setIamPolicy
(Correct)
•	 
resourcemanager.projects.getIamPolicyresourcemanager.projects.setIamPolicy
•	 
resourcemanager.projects.getresourcemanager.projects.setIamPolicy
Explanation
Option 2 is the correct answer. resourcemanager.projects.get is needed to get the project information and resourcemanager.projects.getIamPolicy is needed to get the IAM policy set in the project. In addition, resourcemanager.projects.setIamPolicy is required to set the project's IAM policy.
Options 1, 3 and 4 are incorrect. To manage your Google Cloud project, you need all the permissions included in Option 2.

【reference】  https://cloud.google.com/resource-manager/docs/access-control-proj
Question 23: 
Skipped
You have enabled audit logging for Google Cloud management. Then, the storage usage fee has increased significantly.
What type of audit log do you think is the cause?
•	 
Data access audit log
(Correct)
•	 
Data traffic audit log
•	 
System event audit log
•	 
Traffic audit log
Explanation
Option 1 is the correct answer. If you enable the data access audit log, the log is generated every time data access occurs, which makes the data size very large and causes an increase in cost. Therefore, it is disabled by default except for BigQuery. Must be explicitly enabled to write data access audit logs for Google Cloud services other than BigQuery.
Option 2 is incorrect. There is no type of audit log called data traffic audit log.
Option 3 is incorrect. The system event audit log is a log when the configuration of Google Cloud resources is changed, and the data size does not become very large. System event audit logs are generated by Google's system and not by direct user action.
Option 4 is incorrect. There is no type of audit log called traffic audit log.
【reference】  https://cloud.google.com/logging/docs/audit
Question 24: 
Skipped
The team is using Google Cloud to develop their applications and uses a large number of custom images. You need to be able to release a new version of each image while maintaining the ability to roll back to a previous version if needed.
Which features of Compute Engine do you use for that?
•	 
Image management group
•	 
Image family
(Correct)
•	 
Resource group
•	 
Instance group
Explanation
Option 2 is the correct answer. Use the image family to release a new version of each image while retaining the ability to roll back to a previous version as needed. Image families allow you to group related images and perform rollforward and rollback between specific image versions.
Option 1 is incorrect. A feature called image management groups does not exist in Compute Engine.
Option 3 is incorrect. Resource group is a function to manage a series of resources as a group in Cloud Monitoring.
Option 4 is incorrect. An instance group is a collection of virtual machine (VM) instances that can be managed as a single entity. Compute Engine has two types of instance groups: managed and unmanaged.

【reference】
https://cloud.google.com/compute/docs/instance-groups/
https://cloud.google.com/compute/docs/images#image_families
Question 25: 
Skipped
You would like to understand in detail how GCP launches virtual instances.
What optional parameter can you use when launching an instance to see such details?
•	 
--verbose
•	 
--async
(Correct)
•	 
--describe
•	 
--details
Explanation
Option 2 is the correct answer.
--async displays information about the launch process. So option 2 is the correct answer.

Option 1 is incorrect. --verbose is a parameter similar to many Linux commands.

Option 3 is incorrect. --describe prints details about the instance, not necessarily the boot process.

Option 4 is incorrect. --details is not a valid parameter.
Question 26: 
Skipped
What are pods in Kubernetes?
•	 
collection of containers
•	 
Application code deployed to a Kubernetes cluster
•	 
A single instance that is a running process within a cluster
(Correct)
•	 
A controller that manages communication between clients and Kubernetes services
Explanation
Option 3 is the correct answer.
A Pod is a single instance of a running process in a cluster. Option 3 is the correct answer.

Option 1 is incorrect. A pod runs containers, but it is not a set of containers.

Option 2 is incorrect. Application code runs in containers deployed in pods.

Option 4 is incorrect. Pods are not controllers and cannot manage communication between clients and Kubernetes services.
Question 27: 
Skipped
What command is used to run the Docker image on the cluster?
•	 
gcloud container run
•	 
gcloud beta container run
•	 
kubectl run
(Correct)
•	 
kubectl beta run
Explanation
Option 3 is the correct answer.
The kubectl command is used to control the workload once the Kuberhetes cluster is created. Option 3 is the correct answer.

Option 1 and option 2 are incorrect. gcloud is not used to manipulate Kubernetes processes.

Option 4 is incorrect. kubectl commands do not require beta.
Question 28: 
Skipped
What can you specify when creating an alert policy?
•	 
Terms, Notices, Term
•	 
Terms, Notices and Documents
(Correct)
•	 
conditions only
•	 
Conditions, Documents, Term of Validity
Explanation
Option 2 is the correct answer.
When creating an alert policy, you can specify conditions, notifications, and documents. Option 2 is the correct answer.

Options 1 and 4 are incorrect. Policies do not have a Time to Live attribute.

Option 3 is incorrect. Notifications and documentation are not included.
Question 29: 
Skipped
You have multiple microservices running in a Kubernetes cluster. And you notice a performance degradation. After looking at some logs, you started thinking that the cluster might be misconfigured. So, to investigate, you open the Cloud Console. How can you view details for a specific cluster?
•	 
Enter the cluster name in the search bar
•	 
Click on the cluster name
(Correct)
•	 
Using the gcloud cluster details command
•	 
none of the above
Explanation
Option 2 is the correct answer.
On the Cloud Console page, click the cluster name to display the Detail page.

Typing the cluster name in the search bar does not display detailed information about the cluster. This method displays detailed information for an instance group. There is no command like gcloud cluster details.
Question 30: 
Skipped
What command do you use to print a list of deployments from the command line?
•	 
gcloud container clusters list-deployments
•	 
gcloud container clusters list
•	 
kubectl get deployments
(Correct)
•	 
kubectl deployments list
Explanation
Option 3 is the correct answer.
Deployments are managed by Kubernetes, not GCP, so you should use the kubect command instead of gcloud.
Question 31: 
Skipped
The concern is performance degradation when users connect to the application.
You would like to see additional instances added to my App Engine application when there are 21 or more concurrent requests. Which parameters should be specified in app.yaml?
•	 
max_concurrent_requests
(Correct)
•	 
target_throughput_utilization
•	 
max_instances
•	 
max_pending_latency
Explanation
Option 1 is the correct answer.
max_concurrent_requests allows you to specify the maximum number of concurrent requests before another instance is started. So option 1 is the correct answer. target_throughput_utilization works similarly, but specifies the maximum throughput utilization using a scale of 0.05 to 0.95. max_instances specifies the maximum number of instances, not the criteria for adding instances. max_pending latency is based on the amount of time a request waits, not the number of requests.
Question 32: 
Skipped
You have released a new version of the service. You were waiting for approval from the product manager to start sending traffic to the new version, but we got approval to route traffic to the new version.
What parameter do you use with gcloud app services set-traffic to specify that traffic should be moved to the new version of the application?
•	 
--move-to-new
•	 
--migrate-to-new
•	 
--migrate
(Correct)
•	 
--move
Explanation
Option 3 is the correct answer.
--migrate is a parameter that specifies that traffic should be moved or migrated to the new instance. So option 3 is the correct answer.
None of the other choices are valid parameters for the gcloud app services set-traffic command.
Question 33: 
Skipped
To call an HTTP trigger in Cloud Functions, which of the following do you use to construct the request?
•	 
GET only
•	 
POST and GET only
•	 
DELETE, POST, GET
(Correct)
•	 
DELETE, POST, REVISE, GET
Explanation
Option 3 is the correct answer. HTTP requests using GET, POST, DELETE, PUT, and OPTIONS can fire HTTP triggers in Cloud Functions. So option 3 is the correct answer.
Question 34: 
Skipped
You are defining a Cloud Function that writes a record to a database when a file in Cloud Storage is archived. What parameters should you set when creating a function?
•	 
runtime only
•	 
trigger-resource only
•	 
runtime, trigger-resource, trigger-event only
(Correct)
•	 
runtime, trigger-resource, trigger-event, file-type
Explanation
Option 3 is the correct answer.

Option 1 and option 2 are incorrect because you have to set everything.

Option 4 is incorrect because file-type is not a parameter for creating Cloud Functions on Cloud Storage.
Question 35: 
Skipped
What is the latest version of an object called when the bucket uses versioning?
•	 
live version
(Correct)
•	 
top version
•	 
active version
•	 
safe version
Explanation
Option 1 is the correct answer.
The latest version of an object is called the live version.
Options 2 and 3 are incorrect. The terms "top" and "active" do not refer to versions.
Question 36: 
Skipped
Which of the following Cloud Spanner configurations has the highest hourly cost?
•	 
us-central1
•	 
nam3
•	 
us-west1-a
•	 
nam-eur-asia1
(Correct)
Explanation
Option 4 is the correct answer.
A multi-region configuration with multiple continents in nam-eur-aisa1 is the most expensive. So option 4 is the correct answer.

Option 1 is regional and has a lower cost than multi-region nam-eur-asia1. Option 3 is incorrect. This is a zone. Spanner is configured for regional or multi-region.

Option 2 is incorrect. This is a multi-region with a single continent, which is cheaper than a multi-region deployment with multiple continents.
Question 37: 
Skipped
You were tasked with building an internal data warehouse. It must support tens of petabytes of data and use SQL as its query language. Which managed database service will you choose?
•	 
BigQuery
(Correct)
•	 
Bigtable
•	 
Cloud SQL
•	 
SQL Server
Explanation
Option 1 is the correct answer.
BigQuery is a managed service, but it's designed for data warehousing and analytics. Queries use standard SQL. Bigtable can support the amount of data you describe, but it doesn't use SQL as its query language. Cloud SQL is not a good option for tens of petabytes scale. SQL Server is Microsoft's relational database. It's not GCP's managed database service.
Question 38: 
Skipped
You created a Cloud Spanner instance. You were tasked with building a method for storing product catalog data. After creating a Cloud Spanner instance, what steps should you take to load data?
•	 
Run gcloud spanner update-security-patches
•	 
Create a database on your instance
(Correct)
•	 
Create a table to store data
•	 
Use the Cloud Spanner console to import data into tables created with your instance
Explanation
Option 2 is the correct answer.
The next step is to create a database within the instance. After the database is created, you can create tables and load data into them.

Option 1 is incorrect. Because Cloud Spanner is a managed database, it doesn't require security patches.

Option 3 is incorrect. You cannot create a table without first creating a database.

Option 4 is incorrect. When an instance is created, no tables are created into which data can be imported.
Question 39: 
Skipped
Your manager asked me to set up basic Pub/Sub. Use this as a sandbox for new developers to learn the messaging system. Which two resources should be created within Pub/Sub?
•	 
Topics and resources
•	 
Topics and databases
•	 
Topics and subscriptions
(Correct)
•	 
table and subscription
Explanation
Option 3 is the correct answer.
Pub/Sub works with topics and subscriptions. Topics receive and store messages. Subscriptions provide messages to receiving applications.

Option 1 is incorrect. A table is a relational database data structure, not a message queue.

Similarly, option 2 is incorrect. A database resides in an instance of a database management system, not a messaging system.

Option 4 is incorrect. Tables are not messaging system resources.
Question 40: 
Skipped
You received a large dataset from an Internet of Things (IoT) system.
You want to analyze that data using BigQuery. What command should you use to make my data available for analysis in BigQuery?
•	 
bq load -- autodetect ---source_format=[FORMAT] [DATASET].[TABLE] [PATH_TO_SOURCE]
(Correct)
•	 
bq import-autodetect ---source_format=[FORMAT] [DATASET].[TABLE]
•	 
cloud BigQuery load --autodetect ---source_format=[FORMAT] [DATASET].[TABLE] [PATH_TO_SOURCE]
•	 
gcloud BigQuery load --autodetect --source_format=[FORMAT] [DATASET].[TABLE] [PATH_TO_ SOURCE]
Explanation
Option 1 is the correct answer
The correct command is option 1, bg load. The autodetect and source format parameters and the path to the source are correctly specified for all options. Option 2 is incorrect. Import is used instead of load.

Also option 3 and option 4 are incorrect.
Question 41: 
Skipped
You are a developer on a project using Bigtable for your IoT application. You need to export data from Bigtable so that you can analyze some data with another tool. What should you use to export the data to minimize the effort on my part?
•	 
A Java program designed to import and export data from Bigtable
(Correct)
•	 
gcloud bigtable table export
•	 
bq bigtable table export
•	 
Import tool provided by Analysis Tool
Explanation
Option 1 is the correct answer
Bigtable data is exported using a compiled Java program. Option 1 is the correct answer.

Option 2 is incorrect. There is no gcloud Bigtable command.

Option 3 is incorrect. bq is not used in Bigtable.

Also option 4 is incorrect. It does not export data from Bigtable.
Question 42: 
Skipped
You would like to restrict traffic to a set of instances. You decided to set a specific network tag on each instance. Which items in a firewall rule can reference network tags to determine the set of instances affected by the rule?
•	 
action
•	 
subject
(Correct)
•	 
priority
•	 
direction
Explanation
Option 2 is the correct answer.
This can be all instances in a network, instances with a specific network tag, or instances using a specific service account. So option 2 is the correct answer.

Option 1 is incorrect. Action is Allow or Block.

Option 3 is incorrect. Priority determines which of all matching rules is applied.

Option 4 is incorrect. This specifies whether the rule applies to inbound or outbound traffic.
Question 43: 
Skipped
You are using gcloud to create firewall rules. Which command should you use?
•	 
gcloud network firewall-rules create
•	 
gcloud compute firewall-rules create
(Correct)
•	 
gcloud network rules create
•	 
gcloud compute routes create
Explanation
Option 2 is the correct answer
The product you are working with is compute and the resource you are creating is firewall rules.

Options 1 and 3 use network instead of compute.

Option 4 uses rules instead of firewall-rules rules.
Question 44: 
Skipped
You have a rule that allows inbound traffic to a virtual machine.
You would like it to apply only if there is no other rule blocking that traffic. What priority should you set for this rule?
•	 
0
•	 
1
•	 
1000
•	 
65535
(Correct)
Explanation
Option 4 is the correct answer
The maximum value allowed in the range of priority values. The higher the number, the lower the priority. Specifying the lowest priority ensures that other matching rules are applied.
Question 45: 
Skipped
You are creating a VPN using the Cloud Console. You want to configure the GCP side of the VPN. Which section of the Create VPN form will you use?
•	 
Tunnels
•	 
Routing Options
•	 
Google Compute Engine VPN
(Correct)
•	 
IKE Version
Explanation
Option 3 is the correct answer
Google Compute Engine VPN specifies information for the Google Cloud side of the VPN connection. Provide a name, description, network, region, and IP address.

Option 1 is incorrect. Tunnels are related to connections between clouds and remote networks.

Option 2 is incorrect. Routing options relate to how you configure your router.

Option 4 is incorrect. The IKE version is related to private key exchange.
Question 46: 
Skipped
Which command should you use to create a DNS zone on the command line?
•	 
gsutil dns managed-zones create
•	 
gcloud beta dns managed-zones create
(Correct)
•	 
gcloud beta managed-zones create
•	 
gcloud beta dns create managed zones
Explanation
Option 2 is the correct answer.
Option 1 is incorrect. It uses gsutil commands for working with Cloud Storage.

Option 3 is incorrect. no dns.
Also option 4 is incorrect. Wrong order of words.
Question 47: 
Skipped
What resources are health checks used to check?
•	 
Load balancer
•	 
virtual machine
(Correct)
•	 
storage bucket
•	 
persistent disk
Explanation
Option 2 is the correct answer.
A health check monitors the health of the virtual machines used by the load balancer. Option 2 is correct.
Question 48: 
Skipped
Which command should you use to create a network load balancer on the command line?
•	 
gcloud compute forwarding-rules create
(Correct)
•	 
gcloud network forwarding-rules create
•	 
gcloud compute create forwarding-rules
•	 
gcloud network create forwarding-rules
Explanation
Option 1 is the correct answer.

Option 2 is incorrect. The service is compute, not network.

Option 3 is incorrect. create is after forwarding-rules.

Also option 4 is incorrect. Incorrect service. Also, the verb position is wrong.
Question 49: 
Skipped
What properties can you set when defining a disk in a virtual machine?
•	 
device name only
•	 
A boolean specifying the boot disk and a boolean specifying auto-delete
•	 
A boolean value specifying automatic deletion only
•	 
device name, boolean value specifying boot disk, boolean value specifying auto-delete
(Correct)
Explanation
Option 4 is the correct answer.
You can set all three. Specifically, the keys are deviceName, boot, and autodelete.
Question 50: 
Skipped
You need to use template files with Deployment Manager.
The files are expected to be complex. What language should you use?
•	 
Jinja
•	 
Ruby
•	 
Go
•	 
Python
(Correct)
Explanation
Option 4 is the correct answer.
Google recommends using Python for complex templates, so option 4 is the correct answer.

Option 1 is incorrect. Jinja2 is only recommended for simple templates.

Options 2 and 3 are incorrect. None of the languages are supported by templates.
Question 51: 
Skipped
A development and operations engineer is noticing a spike in server CPU utilization. You explained that you just started deploying.
And you want to show the DevOps Engineer details about the deployment you just started.
What commands can you use?
•	 
gcloud cloud-launcher deployments describe
•	 
gcloud deployment-manage deployments list
•	 
gcloud deployment-manager deployments describe
(Correct)
•	 
gcloud cloud-launcher deployments list
Explanation
Option 3 is the correct answer.
The correct answer is option 3 gcloud deployment-manager deployments describe .

Options 1 and 4 are incorrect.
cloud-launcher is not the name of the service.

Option 2 is incorrect. list gives a brief overview of each deployment. describe gives a detailed description.
Question 52: 
Skipped
A developer intern is confused about what roles are used for.
What would you describe as a collection of IAM roles?
•	 
Identities
•	 
Access right
(Correct)
•	 
access control list
•	 
Audit log
Explanation
Option 2 is the correct answer.
Roles are used to group permissions. Grouped permissions can be assigned to identities.

Option 1 is incorrect. Roles do not have identities, but identities can be granted roles.

Option 3 is incorrect. Roles do not use access control lists.

Option 4 is incorrect. Roles do not include audit logs. Logs are collected and managed by Stackdriver Logging.
Question 53: 
Skipped
You are discussing cloud security measures with an auditor. Auditors have asked how we implement multiple best practices. How would you describe IAM's predefined roles leading to the adoption of which security best practices?
•	 
Least Privilege Grant
•	 
Separation of duties
•	 
Defense in depth
•	 
Option 1 and Option 2
(Correct)
Explanation
Option 4 is the correct answer.
Predefined roles can implement both least privilege and segregation of duties. Predefined roles by themselves do not implement defense in depth, but can be used in conjunction with other security controls to implement defense in depth.
Question 54: 
Skipped
You manage your account as an administrator of Google Cloud. You need to assign a new service account to an existing VM instance on Compute Engine.
Which command should you use?
•	 
gcloud compute assign-instances service-account
•	 
gcloud compute instances assign-service-account
•	 
gcloud compute set instances-service-account
•	 
gcloud compute instances set-service-account
(Correct)
Explanation
Option 4 is the correct answer. You can grant a service account to a Compute Engine instance by running the command `gcloud compute instances set-service-account [instance name] --service-account = [service account name]`. Perform an operation on the instances with the gcloud compute instances command. After that, by adding set-service-account, it becomes a command to assign a service account.
Option 1 is incorrect. assign-instances service-account is not a valid command.
Option 2 is incorrect. assign-service-account is not a valid command.
Option 3 is incorrect. set instances-service-account is not a valid command.

【reference】  https://cloud.google.com/sdk/gcloud/reference/compute/instances/set-service-account
Question 55: 
Skipped
You are the administrator of the Google Cloud environment. Reports should be integrated into Cloud Monitoring Dashboards in the same operations suite to monitor resources distributed across multiple projects.
What kind of settings do you need for this?
•	 
In the Monitoring navigation panel, select Settings and run Add Google Cloud Project.
(Correct)
•	 
Configure one Operations Suite account, create a group in the Operations Suite and add the project name.
•	 
For each project, create an operations suite account and add the project name.
•	 
Configure a single Operations Suite account and link all projects to the same account.
Explanation
Option 1 is the correct answer. In the Monitoring navigation panel, select Settings and then Add Google Cloud Project to monitor resources distributed across multiple projects in the same suite of Cloud Monitoring Dashboards. You can integrate the report into.
You can change your project's Cloud Monitoring configuration to monitor resources associated with multiple Google Cloud projects and accounts. Projects or accounts that you add to the metric scope in this way are called monitored projects. The setting procedure at that time is as follows.
• In the Monitoring navigation panel, select Settings.
• Select Add Google Cloud Project to add the project. Dashboards in the Operations Suite Cloud Monitoring make it easy to track how important metrics change over time.
The dashboard allows you to visualize data to debug the high latency of your application, track key metrics for your application, and more.
【reference】 https://cloud.google.com/monitoring/settings
Question 56: 
Skipped
The company uses App Engine to build web applications. The developer is developing a component that sends CloudPub / Sub messages. The Cloud Pub / Sub API is currently disabled. It is necessary to use the service account to authenticate the application that uses the API.
What is the required response for an application to use Cloud Pub / Sub?
•	 
Create and configure a subscription on the CloudPub / SubAPI console screen.
•	 
Execution processing by CloudPub / SubAPI is automatically enabled by linking with the service.
•	 
Enable CloudPub / SubAPI in the Pub / Sub section of the Google Cloud Console.
(Correct)
•	 
Enable Cloud Pub / Sub API on the account screen of the Google Cloud console.
Explanation
Option 3 is the correct answer. To set up a Pub / Sub topic subscription in your application, follow these steps:
1. Select an existing project or create a new one. The first time you use Google Cloud, a default project will be created.
2. In the Cloud Console Home section, make a note of your project ID. Use this value when setting up your current Cloud Storage project during the Cloud SDK initialization process. You also pass this ID to your Python script when you launch your publisher and subscriber applications.
3. Go to the Pub / Sub section of the Google Cloud Console. Follow the instructions to enable the API.
4. Click Create Topic. The publisher application sends a message to the topic. Use the name hello_topic.
5. On the Topic Details page, click Create Registration.

Therefore, it is necessary to enable CloudPub / SubAPI in the Pub / Sub section of the Google Cloud Console

【reference】  https://cloud.google.com/pubsub/docs/building-pubsub-messaging-system
Question 57: 
Skipped
One company decided to use Google Cloud to set up a Docker container-based CI / CD environment.
Which service is used to store and manage Docker container images?
•	 
Cloud Storage
•	 
Container Registry
(Correct)
•	 
Docker repository
•	 
Kubernetes Engine repository
Explanation
Option 2 is the correct answer. Use the Container Registry when setting up a Docker container-based CI / CD environment using Google Cloud. Container Registry is a private container image registry that supports the Docker Image Manifest V2 and OCI image formats. This provides a subset of the features of the Artifact Registry.
Option 1 is incorrect. Cloud Storage is object storage for enterprises of all sizes. There is no limit to the amount of data that can be saved. You can get the data as many times as you want. Not available when setting up a Docker container-based CI / CD environment.
Options 3 and 4 are incorrect. These services are not Google Cloud services.
【reference】  https://cloud.google.com/container-registry
Question 58: 
Skipped
When setting up the Compute Engine VM, it was necessary to check detailed information such as CPU type. Which command is used to get a list of all CPU types available in a particular zone?
•	 
gcloud dns managed-zones describe
•	 
gcloud compute zones describe
(Correct)
•	 
gcloud zones describe
•	 
gcloud compute zones list
Explanation
Option 2 is the correct answer. You can get a list of all CPU types available in a particular zone by running gcloud compute zones describe. gcloud is a command line tool for manipulating computing resources. VM operation is done via the gcloud compute.
Option 1 is incorrect. gcloud dns managed-zones describe is a command that displays the details of cloud DNS managed zones.
Option 3 is incorrect. gcloud zones describe is not a valid command.
Option 4 is incorrect. The gcloud compute zones list lists the Google Compute Engine zones.
【reference】  https://cloud.google.com/sdk/gcloud/reference/compute/zones/describe
Question 59: 
Skipped
You are an engineer in charge of web application development using Google Cloud. We are currently using the DeploymentManager template to prepare to deploy the preferences. You've modified a complex DeploymentManager template and want to make sure that all defined resource dependencies are properly met before committing to a project.
Which of the following is the proper confirmation method?
•	 
Run the DeploymentManager template for the same project to see the status of interdependent resources.
•	 
Use the preview option in the same project to run the Deployment Manager template to see the status of interdependent resources.
(Correct)
•	 
On the Stackdriver Logs page of the Google Cloud console, run the Deployment Manager template to see the status of interdependent resources.
•	 
On the Stackdriver Logs page of the Google Cloud console, use the preview option to run the Deployment Manager template to see the status of interdependent resources.
Explanation
Option 2 is the correct answer. You can use the preview option in the same project and then run the Deployment Manager template to ensure that all resource dependencies defined are properly met. The Deployment Manager service previews the configuration by extending the complete configuration and creating "shell" resources. Run the Deployment Manager template to see the status of the interdependent resources.

【reference】 https://cloud.google.com/deployment-manager/docs/configuration/preview-configuration-file
Question 60: 
Skipped
You have created an instance for SQL Server with Compute Engine to build a relational database. Which of the following is the easiest way to connect to this SQL Server?
•	 
Use the sqlcmd utility to connect to your SQL Server instance.
(Correct)
•	 
Set your Windows password in the Google Cloud console. Make sure the firewall rule for port 22 exists and log in to the instance with your credentials.
•	 
Install the RDP client on your desktop and use your credentials to log in to your instance.
•	 
Install the RDP client on your desktop, set your Windows username and password in Google Cloud Console, and log in to your instance using your credentials.
Explanation
Option 1 is the correct answer. GCP allows you to configure SQL Server by installing a SQL Server license on Compute Engine. To connect to a SQL Server-configured instance from the Internet, you can use the sqlcmd utility to connect to a SQL Server instance.
[Overview of connection procedure]
1. Make sure you have installed the client and configured access to the instance.
2. Follow the steps similar to the examples on each page of Using SQL Server Quick Starts and Utilities to connect using the sqlcmd command.
3. To connect to your instance using SSL, use the steps in the Client-Initiated Encryption section and related sections of the Encrypting Connection to SQL Server on Linux page.
【reference】 
https://cloud.google.com/sql/docs/sqlserver/connect-admin-ip
Question 61: 
Skipped
Your company hosts a web application on Google Cloud's Compute Engine. This application tends to have a temporary spike in load. Therefore, it is necessary to set an autoscaling policy.
What is the incorrect element available when setting the autoscaling policy?
•	 
Concurrency quantity
(Correct)
•	 
CPU usage
•	 
Number of requests
•	 
External HTTP (S) load balancer processing power
Explanation
Option 1 is the correct answer. ComputeEngine can scale managed instance groups based on request count, CPU utilization, and external HTTP (S) load balancer processing power, but concurrency quantities do not trigger cluster resizing.
Options 2, 3 and 4 are incorrect because they are correct elements. ComputeEngine scales based on the number of requests, CPU utilization, and the processing power of the external HTTP (S) load balancer.
Question 62: 
Skipped
As a result of IT audits, your company needs to set bucket-level access to Cloud Storage buckets. However, after applying bucket-level access control, some users who could access the objects in the bucket have now lost access.
Which of the following is the most likely cause?
•	 
They can't be accessed because you don't have the IAM permission to allow the user to access the objects in your bucket.
(Correct)
•	 
They cannot access it because there is no ACL that allows the user to access the objects in the bucket.
•	 
Uniform bucket policies do not allow you to control user permissions and must be changed to ACLs.
•	 
Cannot access because there is no firewall that allows the user to access the objects in the bucket.
Explanation
Option 1 is the correct answer. When you enable uniform bucket-level access for a bucket, access control lists (ACLs) are disabled, and only bucket-level IAM permissions grant access to that bucket and the objects in it. Therefore, if a previously accessible user was accessing it through an ACL, it is likely that they are no longer able to access it because they do not have IAM privileges after configuring the bucket-level access.
Options 2 and 3 are incorrect. Since access is not possible because there is no IAM permission setting, access is not possible even if ACL is attached.
Option 4 is incorrect. Firewalls set up traffic communication for virtual networks. This time, access is disabled after changing the bucket level access settings, so it is considered that the traffic communication control settings are irrelevant.
【reference】
https://cloud.google.com/storage/docs/uniform-bucket-level-access
Question 63: 
Skipped
Your company has decided to use multiple clouds, including Google Cloud, as a hybrid cloud. You will only be using Google Cloud resources in Asia. How can you prevent Google Cloud resources from being created outside of Asia?
•	 
Create constrained data lifecycle management policies at the organizational level of the resource hierarchy to limit where resources are available.
•	 
Create constrained data lifecycle management policies at the folder level of the resource hierarchy to limit where resources are available.
•	 
Create constrained policies at the folder level in the resource hierarchy to limit where resources are available.
•	 
Create constrained policies at the organizational level of the resource hierarchy to limit where resources are available.
(Correct)
Explanation
Option 4 is the correct answer. Constraints are a standard way to limit where resources can be created, and you can apply constrained policies to limit resource usage in a particular region. If the policy is applied at the folder level, it is inefficient to set overall resource limits as in this case, as the policy must be applied to every folder individually. Therefore, you must create constrained policies at the organization level to limit resource usage at the organization level.
Option 1 is incorrect. The data lifecycle management policy is used for management such as deleting data after a specific period. It is incorrect because it has nothing to do with this scenario.
Options 2 and 3 are incorrect. If the policy is applied at the folder level, it would be inefficient to set overall resource limits as in this case, as the policy would have to be applied to every folder individually. Therefore, create constrained policies at the organization level to limit resource usage at the organization level.
【reference】 
https://cloud.google.com/resource-manager/docs/organization-policy/defining-locations
Question 64: 
Skipped
As a solution architect, you are building an image analysis system that uses an image identification API. Processing by this image identification API takes about 5 seconds for each image. The frequency of image processing is highly volatile, as there are periods when images are not uploaded and there are periods when many images are uploaded in a short period of time.
Which service should you use to build this image processing application?
•	 
Compute Engine
•	 
Cloud Dataflow
•	 
Cloud Functions
(Correct)
•	 
Container Registry
Explanation
Option 3 is the correct answer. If the frequency of image processing is highly volatile, such as when images are not uploaded or when many images are uploaded in a short period of time, it is inefficient to use an application that always starts a VM. It will be. Therefore, it is best to use a serverless application that is charged only when it is used. This time, it is best to use Cloud Functions to execute the work process using the image identification API. Cloud Functions can develop Google Cloud event response processing such as uploading files to Cloud Storage serverlessly.
By using Cloud Functions, you can complete the analysis process within the time limit. You don't have to run the application continuously to see new image uploads, as there are periods when images aren't uploaded. Therefore, there is no need for a full-scale system configuration using App Engine Flexible, Cloud Engine, Kubernetes Engine, etc.
Option 1 is incorrect. Compute Engine is a service that allows you to rent virtual machines for an hourly fee. Costs will be incurred due to pay-as-you-go billing according to the startup time. This time, in addition to calling the process using the image identification API to execute the work process, it is more cost-effective to use Cloud Functions for the process using the virtual server because there is volatility in the execution frequency.
Option 2 is incorrect. Cloud Dataflow is a stream and batch processing service. Cloud Dataflow can perform ETL processing in a fully managed environment. It is not suitable for work processes like this one.
Option 4 is incorrect. Container Registry provides centralized management and vulnerability analysis of Docker images. Fine-grained access control also allows you to decide which users are allowed access to what. Therefore, it has nothing to do with this case.
【reference】
https://cloud.google.com/functions/docs/how-to
Question 65: 
Skipped
An instance of Compute Engine utilizes a persistent disk, and you were asked to clone this persistent disk. In that case, the specific elements must be the same on the source disk and the clone disk. Which elements should be the same?
•	 
zone
(Correct)
•	 
size
•	 
snap shot
•	 
VPC
Explanation
Option 1 is the correct answer. When cloning a persistent disk, the source and clone disks must be in the same zone and region and of the same type.
Option 2 is incorrect. The size of the clone must be greater than or equal to the size of the source disk. It doesn't have to be the same.
Option 3 is incorrect. This time it doesn't matter because snapshots are used when data protection is needed to improve restoring, such as backup or disaster recovery.
Option 4 is incorrect. The VPCs do not have to be the same. The source and clone discs must be in the same zone and region and of the same type.
【reference】
https://cloud.google.com/compute/docs/disks/create-disk-from-source
Question 66: 
Skipped
At your company, an in-house auditor is conducting an IT audit and wants to collect and analyze application log files generated from these servers for reporting purposes. Which of the following audit logs are available for projects, folders, and organizations?
•	 
Resource access audit log
•	 
User access audit log
•	 
Data access audit log
(Correct)
•	 
Policy access audit log
Explanation
Option 3 is the correct answer. Cloud Audit Logs collects the following logs and can be used by users for analysis.
--Administrative activity audit log
--Data access audit log
--System event audit log
--Policy denial audit log 

Option 1 is incorrect. Cloud Audit Logs does not collect resource access audit logs.
Option 2 is incorrect. Cloud Audit Logs does not collect user access audit logs.
Option 4 is incorrect. Cloud Audit Logs does not collect policy access audit logs.
【reference】 https://cloud.google.com/logging/docs/audit
Question 67: 
Skipped
Preemptible virtual machines are shut down after a certain amount of time has passed since they were run.
Which of the following times is it?
•	 
12 hours
•	 
24 hours
(Correct)
•	 
36 hours
•	 
48 hours
Explanation
Option 2 is the correct answer. If the preemptive machine is not shut down within 24 hours, Google will stop the instance. This stop is called a preempt.
Preemptible VM instances are much cheaper than standard VMs (60-91% discount). However, the Compute Engine may stop this instance if it needs to reclaim compute capacity for allocation to other VMs. Preemptable Instances are a feature that takes advantage of Compute Engine's surplus capacity, and availability depends on usage.
Preemptible instances behave like regular instances, but with the following limitations:
· The Compute Engine can stop a preemptible instance at any time due to a system event. It is unlikely that the Compute Engine will shut down a preemptible instance due to a system event, but it can vary from zone to zone on a daily basis, depending on the circumstances at the time.
· The Compute Engine always shuts down the preemptible instance after running for 24 hours. 
· Preemptible instances are finite Compute Engine resources and are not always available. ·
Preemptible instances cannot become regular VM instances through Live migration. Also, it cannot be set to restart automatically in the event of a maintenance event.
· Because of these restrictions, preemptible instances are not covered by service level agreements. They are excluded from Compute Engine SLAs
・ Compute Engine Free tier for Google Cloud does not apply to preemptible instances.

Question 68: 
Skipped
You need to create a custom role on a resource for application development. Which of the following settings can give a user permission to create a custom role?
•	 
iam.roles.create
(Correct)
•	 
Compute.roles.create
•	 
iam.create.roles
•	 
Compute.create.roles
Explanation
Option 1 is the correct answer. Users with iam.roles.create permissions are granted the right to create custom roles. Set an IAM role with the permissions as follows: 
resource "google_organization_iam_custom_role"
"my-custom-role" {
Role_id = "myCustomRole"
Org_id = "123456789"
Title = "My Custom Role"
Description = "A description"
Permissions = [
"iam.roles.list", "iam.roles.create", "iam.roles.delete"
]
【reference】
https://cloud.google.com/sdk/gcloud/reference/iam/roles/create

Options 2, 3 and 4 are incorrect. These are the wrong role permissions.
Question 69: 
Skipped
A certain application uses Compute Engine to execute a daily batch job at 1am in the middle of the night to back up data. These jobs take about an hour to complete. The data is not very important, so backups do not always need to be successful every day. Choose a Compute Engine configuration that minimizes costs from the following.
•	 
Use a micro-instance type 3-node cluster in Compute Engine.
•	 
Use N2 machine type preemptive VM instances in Compute Engine.
•	 
Use a single node cluster of N2 machine type in Compute Engine.
•	 
Use E2 standard machine type preemptive VM instances in Compute Engine.
(Correct)
Explanation
Option 4 is the correct answer. Since the purpose of this scenario is to execute the process for only one hour a day, it is necessary to select the best instance type to execute at a low price in the short term. In this case, you can minimize the cost by using a preemptible VM.
Preemptible VMs are instances that can be created and run at a much lower cost than regular instances. However, the Compute Engine can stop this instance if other tasks require access to the resource. Preemptable Instances are a feature that takes advantage of Compute Engine's surplus capacity, and availability depends on usage. The E2 machine type is a cost-optimized VM that provides up to 32 vCPUs with up to 8 GB per vCPU and up to 128 GB of memory. It is the most suitable type for finding the most cost-optimized processing like this time.
Option 1 is incorrect. Using a micro-instance type 3-node cluster is too high performance and is costly.
Options 2 and 3 are incorrect. For N2 machine types, up to 80 vCPUs and 8 GB of memory per vCPU are available on the Intel Cascade Lake CPU platform. This is more powerful and more expensive than the E2 machine type.
【reference】 
https://cloud.google.com/compute/docs/machine-types/
https://cloud.google.com/compute/docs/instances/preemptible/
Question 70: 
Skipped
The company is building a mobile app where users can share photos. This mobile app stores photos in Cloud Storage and created two buckets for redundancy. Which command should you use to synchronize the contents of the two buckets?
•	 
gcloud rsync
•	 
gsutil rsync
(Correct)
•	 
gsutil cp sync
•	 
gcloud cp sync
Explanation
Option 2 is the correct answer. gsutil is a command line tool for working with Cloud Storage. The gsutil rsync command can synchronize a local directory to a bucket. You can use this to synchronize the contents of the two buckets.  For example, you can use the following commands to match the contents of :
gs: // example-bucket and the local directory of local-dir:
gsutil -m rsync -r local-dir gs: // example-bucket  If you use the rsync -d flag, it will tell gsutil to delete files in the sync destination (gs: // example-bucket) that do not exist in the sync source (local-dir). In this way you can also sync between the two buckets. 
Options 1 and 4 are incorrect. A command line tool for gsutil to work with Cloud Storage, not gcloud.
Option 3 is incorrect. There is no command called gsutil cp sync.
【reference】 
https://cloud.google.com/storage/docs/working-with-big-data
Question 71: 
Skipped
You are starting application develop using a managed VM instance group. The application needs to run only one instance of VM per project. How should you configure this instance group?
•	 
Disable autoscaling and set the number of instances to launch to 1.
•	 
Enable autoscaling and set the number of instances to launch to 1.
•	 
Disable autoscaling and set the minimum number of instances to 1 and the maximum number to 1.
•	 
Enable autoscaling and set the minimum number of instances to 1 and the maximum number to 1.
(Correct)
Explanation
Option 4 is the correct answer. You can maintain 1 VM instance at all times by enabling autoscaling for your instance group and setting the minimum number of instances to 1 and the maximum number to 1. Autoscaling adds VMs to managed instance groups under heavy load and removes VMs when the need for VMs decreases. In other words, if the minimum number is 1, the number of instances is adjusted so that the minimum value is 1 at the time of scale-in, and if the maximum number is 1, the number of instances is adjusted so that the maximum value is 1 at the time of scale-out. As a result, the number of instances can always be 1.

【reference】
https://cloud.google.com/compute/docs/autoscaler
Question 72: 
Skipped
As an infrastructure manager, you have a monitoring system in place. Which notification method is incorrect when making notifications based on CPU usage conditions?
•	 
Email
•	 
Slack
•	 
PagerDuty
•	 
Teams
(Correct)
Explanation
Option 4 is the correct answer. Notification settings using Teams are not possible. All three other options are valid notification channels that can be configured in Operations Suite Monitoring. Operations Suite Monitoring allows you to configure alert policies to notify you when an event occurs or when a particular system or custom metric violates a predefined rule. You can also use multiple conditions to define complex alert rules. Notifications can be received via email, SMS, Slack, PagerDuty and more.
Options 1, 2 and 3 are correct. You can receive Operation Suite notifications via email, SMS, Slack, PagerDuty and more.

【reference】  https://cloud.google.com/monitoring#section-5
Question 73: 
Skipped
As a system administrator, you are using the Operations Suite Logging to configure Google Cloud logging. Which service is incorrect destination for LogSync's log forwarding?
•	 
CloudSQL
(Correct)
•	 
Cloud Storage bucket
•	 
BigQuery dataset
•	 
Cloud Pub / Sub topic
Explanation
Option 1 is the correct answer. CloudSQL is not available as a destination for logging exports. You can use a log router to forward specific logs to supported destinations in any Cloud project. The log destinations supported by Logging are:
- Saving them as JSON files in your Cloud Storage bucket. This provides low-cost long-term storage.
- Creating a table in your BigQuery dataset. This provides big data analysis function.
- Deliver JSON-formatted messages to Pub / Sub topics. 
- Logging integration with third parties such as Splunk is supported.
- Retaining log entries in the Cloud Logging log bucket. This provides Cloud Logging storage with a customizable retention period.x 
Options 2, 3 and 4 are incorrect because they are correct. Cloud Storage buckets, BigQuery datasets, and Cloud Pub / Sub topics are all three Log Destinations supported by Logging.
【reference】
https://cloud.google.com/stackdriver/docs/solutions/gke/using-logs
Question 74: 
Skipped
As a system administrator, you are performing log analysis for Operations Suite Logging. Which of the following has the ability to filter and display the logs?
•	 
Log Explorer
(Correct)
•	 
Log viewer
•	 
Log router
•	 
Log sync
Explanation
Option 1 is the correct answer. Use Log Explorer in the Google Cloud Console to provide recommended queries that make it easier to find important logs. Log Explorer uses Boolean expressions to specify specific parts of a project's entire log entry. You can use such queries to select log entries from a particular log or log service, or log entries that meet the criteria for metadata or user-defined fields.
Option 2 is incorrect. Log Viewer is an analysis tool previously used by Log Explorer.
Option 3 is incorrect. Cloud Logging receives log entries through the Cloud Logging API, and in the process the log entries go through the log router. The log router sink matches each log entry against existing inclusion and exclusion filters to determine whether to send the log entry to a destination, such as a Cloud Logging bucket, or to exclude Cloud Logging ingestion altogether.
Option 4 is incorrect. Sync is used to forward logs to multiple destinations.
【reference】 
https://cloud.google.com/logging/docs/view/query-library-preview#kubernetes-filters
Question 75: 
Skipped
There is a problem with the application code and there has been an application error. As an application engineer, you need to use the Operations Suite to identify application code bottlenecks.
Which feature will generate a detailed report of the latencies that cause performance degradation?
•	 
Cloud Debugger
•	 
Cloud Trace
(Correct)
•	 
Cloud Logging
•	 
Cloud Monitoring
Explanation
Option 2 is the correct answer. Cloud Trace is a tool used for bug analysis in the operation suite. It automatically analyzes all traces of your application and produces detailed latency reports that cause poor performance. This feature allows you to use the Operations Suite to identify bottlenecks in your application code.
Option 1 is incorrect. Cloud Debugger is a Google Cloud feature that allows you to investigate the state of a running application in real time without stopping or slowing down the application.
Option 3 is incorrect. Cloud Logging is a tool for real-time log management and scalable analysis
Option 4 is incorrect. Cloud Monitoring provides visibility into application and infrastructure performance, availability, and health.
[Reference] https://cloud.google.com/trace/
Question 76: 
Skipped
You need to check the status of BigQuery to see what's wrong with BigQuery.
Where can you check the status of BigQuery service in Google Cloud?
•	 
Google Cloud Status Dashboard
(Correct)
•	 
BigQuery console
•	 
Cloud Customer Care Portfolio
•	 
TAM
Explanation
Option 1 is the correct answer. The Google Cloud Status Dashboard (https://status.cloud.google.com/) provides information about the status of Google Cloud services. It is possible to check the status of the BigQuery service.
Option 2 is incorrect. Check in the Google Cloud Status Dashboard instead of the Bigquery console.
Option 3 is incorrect. The Cloud Customer Care Portfolio is a scalable and flexible set of services centered around user needs. This has nothing to do with checking the status of the service.
Option 4 is incorrect. By subscribing to Premium Support, you will receive support for critical user awareness workloads from a dedicated Technical Account Manager (TAM). This has nothing to do with checking the status of the service.
【reference】
https: //status.cloud.google.com/
Question 77: 
Skipped
Which of the following services has a pricing model based on the settings of the virtual machine?
•	 
BigQuery and CloudPub / Sub
•	 
Compute Engine and BigQuery
•	 
Kubernetes Engine and BigQuery
•	 
Compute Engine and Kubernetes Engine
(Correct)
Explanation
Option 4 is the correct answer. Both Compute Engine and Kubernetes Engine base their costs on virtual machines. Compute Engine calculates disk size, machine type memory, and network usage in gigabytes (GB). With KubernetesEngine, the clusters you create will incur a flat rate of $ 0.10 per cluster for one hour after the free tier expires, plus CPU, memory, and ephemeral storage computing resources provisioned in the pod.
【reference】 
https://cloud.google.com/compute/all-pricing
https://cloud.google.com/kubernetes-engine/pricing
Question 78: 
Skipped
You are in charge of the task of starting and managing VMs using Compute Engine. Which feature do you use for dynamic provisioning based on a dedicated configuration file?
•	 
Managed instance group
(Correct)
•	 
Unmanaged instance group
•	 
Service account
•	 
Instance cluster
Explanation
Option 1 is the correct answer. You can utilize a managed instance group (MIG) for dynamic provisioning based on a dedicated configuration file. MIG is a redundantly configurable group that allows applications to be deployed on multiple identical VMs. You can perform auto-scaling, auto-repair, region (multi-zone) deployment, auto-update, and more for the instances that make up a managed instance group for scalable and highly available workload processing. 
Option 2 is incorrect. non-managed instance groups allow load balancing across a set of VMs that you manage yourself. Managed instance groups are best suited for configurations where automatic provisioning is performed.
Option 3 is incorrect. Service accounts are special accounts used by applications and virtual machine (VM) instances, not users. The application uses the service account to make authorized API calls. It is approved as the service account itself, or as a Google Workspace or Cloud Identity user by domain-wide delegation.
Option 4 is incorrect. There is no group called an instance cluster, but if you are using Docker you can configure a cluster with VMs.
【reference】 
https://cloud.google.com/compute/docs/instance-groups
Question 79: 
Skipped
You need to create a table iot-ingest-data in Bigtable. What commands can you use?
•	 
cbt createtabie iot-ingest-data
(Correct)
•	 
gcloud bigtable tables create ace-exam-bt-table
•	 
gcloud bigtable create tables ace-exam-bt-table
•	 
gcloud create ace-exam-bt-table
Explanation
Option 1 is the correct answer.
You should use the cbt command. This is the command line for working with Bigtable. All other choices are gcloud which is incorrect.
Question 80: 
Skipped
You received an e-mail from one of our executives asking for advice on building a proposal system to enhance product sales.
It sounds like this executive has heard about a GCP solution that seems to be the perfect fit for solving such a problem. Which GCP services would you recommend investigating?
•	 
Cloud Dataproc. Especially Spark and machine learning libraries
(Correct)
•	 
Cloud Dataproc. In particular, Hadoop
•	 
Cloud Spanner. It is a global relational database that can store large amounts of data
•	 
Because Cloud SQLSQL is a powerful query language
Explanation
Option 1 is the correct answer.
Using Spark and its learning library with Dataproc is perfect for this use case.

Option 2 suggests Hadoop, but it's not a good choice for machine learning applications.

Option 3 is incorrect. Spanner is designed as a global relational database to support transaction processing systems, not analytics and machine learning systems.

Option 4 is incorrect. SQL is a powerful query language, but it doesn't support the kind of machine learning algorithms needed to solve the problem presented.
Question 81: 
Skipped
Your database administrator has asked me to export a MySQL database in Cloud SQL. A database administrator loads the data into another relational database, but does so with minimal effort. In particular, this loading method does not require the database administrator to define a schema. Which file format do you recommend for this task?
•	 
SQL
(Correct)
•	 
CSV
•	 
XML
•	 
JSON
Explanation
Your database administrator has asked me to export a MySQL database in Cloud SQL. A database administrator loads the data into another relational database, but does so with minimal effort. In particular, this loading method does not require the database administrator to define a schema. Which file format do you recommend for this task?Option 1 is the correct answer.
Option 1, SQL format, exports the database as a series of SQL data definition commands. These commands can be run on another relational database without creating the schema first.

Option 2 can also be used, but requires column mapping in the created schema before loading the CSV. Database administrators want to avoid this task.

Also option 3 and option 4 are incorrect. Not an export file format option.
Question 82: 
Skipped
Your company uses Google Cloud billing accounts in multiple departments, and billing processing is mixed. To properly track your department's spending, you need to link your project to your billing account.
Who can perform this operation?
•	 
Billing account owner
•	 
Billing Account Viewer
•	 
Billing account user
(Correct)
•	 
Billing account creator
Explanation
Option 3 is the correct answer. Billing Account User (roles / billing.user) can link the project to the billing account. The permissions for this role are heavily restricted due to the need for increased security and are commonly used in combination with the project creator. These two roles allow you to create a new project linked to the billing account to which the role is granted. 
Option 1 is incorrect. The role of billing account owner does not exist.
Option 2 is incorrect. The billing account viewer (roles / billing.viewer) is the right to view cost information and transactions for billing accounts. You cannot link or unlink projects.
Option 4 is incorrect. The billing account creator (roles / billing.creator) is authorized to create new billing accounts and cannot link or unlink projects.
【reference】
https://cloud.google.com/billing/docs/how-to/billing-access/
Question 83: 
Skipped
Serverless computing is required to process files and run the backend of the website.
Please select two products from Google Cloud Platform below.
•	 
Kubernetes Engine and Compute Engine
•	 
App Engine and Cloud Functions
(Correct)
•	 
Cloud Functions and Compute Engine
•	 
Cloud Functions and Kubernetes Engine
Explanation
Option 2 is the correct answer.
App Engine is a serverless platform for running applications. Cloud Functions is a service for executing short-lived functions in response to events. Kubernetes Engine is a managed cluster service. Both Kubernetes Engine and Compute Engine require a server to be configured.
Question 84: 
Skipped
You have created a virtual machine. Which of the following system administration operations can you perform on this virtual machine?
•	 
File system configuration
•	 
Patches to operating system software
•	 
Change File and Directory Permissions
•	 
all of the above
(Correct)
Explanation
Option 4 is the correct answer.
After creating a virtual machine, system administrators can use all operations.
Question 85: 
Skipped
Which of the following is not a characteristic of a dedicated service offered by Google Cloud Platform?
•	 
Serverless, no need to configure servers or clusters
•	 
Offers specific functions such as text translation or image analysis
•	 
Requires user monitoring
(Correct)
•	 
Provides an API to access functionality of the service
Explanation
Option 3 is the correct answer.
Dedicated services are monitored by Google, so you don't have to. Dedicated services provide specific computing capabilities, but do not require users to configure resources. An API is also provided.
Question 86: 
Skipped
Why would an API developer want to use the Apigee API Platform? (Choose two)
•	 
To get the benefits of routing and rate limiting
(Correct)
•	 
Authentication service
(Correct)
•	 
Code version control
•	 
all of the above
Explanation
Options 1 and 2 are correct.
The Apigee API platform provides policy-based, rate-limited routing services that can handle traffic spikes. It also offers OAuth 2.0 and SAML authentication.
Question 87: 
Skipped
Your department deploys an application with a database backend. You are concerned about the read load on the database server and would like to make the data available in memory to reduce the time to respond to queries and reduce the load on the database server. Which Google Cloud service should you use to store data in memory?
•	 
Cloud SQL
•	 
Cloud Memorystore
(Correct)
•	 
Cloud Spanner
•	 
Cloud Datastore
Explanation
Option 2 is the correct answer. Cloud Mercorystore is the only service for storing data in memory caches.

Option 1 is incorrect. Cloud SQL is a relational database service and not a good option for your backend database.

Option 3 is incorrect. Cloud Spanner is a global relational database and is a good option when you need a globally consistent database.

Option 4 is incorrect. Cloud Datastore is a document database suitable for product catalogs, user profiles, and other semi-structured data.
Question 88: 
Skipped
A scientist at your company wants to use a machine learning library available only on Apache Spark.
Data scientists want to minimize the amount of administrative and development operations work.
How would you recommend proceeding in this case?
•	 
Use CloudSpark
•	 
Use Cloud Dataproc
(Correct)
•	 
Use BigQuery
•	 
Install Apache Spark on a cluster of virtual machines
Explanation
Option 2 is the correct answer.
Both options 2 and 4 would probably require running Spark to provide data scientists with access to the necessary machine libraries. However, for option 4, you must manage and monitor a cluster of servers. This involves more development, operations, and administration work than using Dataproc services.

Option 1 is incorrect. Cloud Spark is a fictitious product and does not exist on GCP.

Option 3 is incorrect. BigQuery is an extensible database, not a platform for running Spark.
Question 89: 
Skipped
A team of four members needs to set up a project that requires only general permissions to all resources.
You grant primitive roles to each member to give them varying levels of access, depending on their responsibilities in the project.
Which of the following is not included as a base role in Google Cloud Platform?
•	 
owner
•	 
publisher
(Correct)
•	 
Editor
•	 
Reader
Explanation
Option 2 is the correct answer.
Publisher is not a primitive role, Owner, Editor, and Reader are the three primitive roles in GCP.
Question 90: 
Skipped
Your company uses GCP to experiment with various features. You have no authority to incur costs.
How can you use GCP without incurring costs?
•	 
Impossible to do this. All services incur costs
•	 
You can pay using your personal credit card
•	 
Only free services available on GCP
(Correct)
•	 
Can only use serverless products that are free to use
Explanation
Option 3 is the correct answer.
GCP offers a free service tier for many products, and you can use these services without setting up a billing account. When a customer exceeds the usage allowed by the free tier, Google charges fees for Cloud Functions, App Engine, etc.
Question 91: 
Skipped
The company you work for is releasing a new online service based on a new user interface experience driven by a set of services running on a server. There is a separate set of services that manage authentication and authorization. The service's datastore set tracks account information. All three sets of services must be highly reliable and scale to meet demand. Which Google Cloud service would be best for deploying this?
•	 
Compute Engine only
•	 
Kubernetes Engine only
•	 
Compute Engine, Kubernetes Engine, App Engine flexible environment only,
(Correct)
•	 
Compute Engine, Kubernetes Engine, App Engine flexible environment or App Engine standard environment
Explanation
Option 3 is the correct answer.
If you have Docker installed on your virtual machine, Compute Engine can run Docker containers. Kubernetes and App Engine flexible environment support Docker containers.

App Engine standard environment provides a language-specific runtime environment. Customers cannot specify a custom Docker image to use.
Question 92: 
Skipped
How much node memory does Kubernetes require as overhead?
•	 
10GB to 20GB
•	 
1GB to 2GB
•	 
1.5GB
•	 
Scaled capacity that starts at 25% of initial memory and eventually decreases to 2% of limit memory as total memory increases
(Correct)
Explanation
Option 4 is the correct answer.
Kubernetes will use 25% of the memory up to 4 GB and slightly less for the next 4 GB. Above 128 GB of memory, the percentage of additional memory is reduced to 2%.
Question 93: 
Skipped
A colleague has asked you to help set up a test environment on Google Cloud. None of my colleagues have ever worked with Google Cloud. The colleague suggest you start with one virtual machine. Which of the following is the minimum information required for configuration?
•	 
Virtual machine name and machine type
•	 
Virtual machine name, machine type, region, zone
(Correct)
•	 
Virtual machine name, machine type, region, zone, CIDR block
•	 
Virtual machine name, machine type, region, zone, IP address
Explanation
Option 2 is the correct answer.
You can specify the virtual machine's name, region, zone, machine type, along with other parameters, all in the console.
Question 94: 
Skipped
You should configure your server with a high level of security implemented. You want to protect against attacks on your servers that attempt to inject rootkits (a type of malware that can modify your operating system). Which option should you choose when creating a virtual machine?
•	 
firewall
•	 
Shield VMs
(Correct)
•	 
Project-wide public SSH key
•	 
Boot disk integrity check
Explanation
Option 2 is the correct answer.
Shielded VM is advanced security controls and includes Integrity Monitoring. This is a check to ensure that the boot image has not been tampered with. So option 2 is the correct answer.

Option 1 is incorrect. Firewalls are used to control incoming and outgoing network traffic to a server or subnet.

Option 3 is incorrect. Project-wide public SSH keys are used to authenticate users across servers within a project.

Option 4 is incorrect. Boot disk consistency check is a non-existent feature.
Question 95: 
Skipped
Which of the following items can be used in the console to filter the list of virtual machine instances?
•	 
label only
•	 
Managed instance group members only
•	 
Label, Status, or Managed Instance Group Membership
(Correct)
•	 
label and status only
Explanation
Option 3 is the correct answer.
Labels, managed instance group members, and status can all be used for filtering. So option 3 is the correct answer.

You can also filter by internal IP, external IP, zone, network, deletion protection, managed instance group/unmanaged instance group member.
Question 96: 
Skipped
Which is the gcloud command for creating custom roles?
•	 
gcloud project roles create
•	 
gcloud iam roles create
(Correct)
•	 
gcloud project create roles
•	 
gcloud iam create roles
Explanation
Option 2 is the correct answer.

Option 1 is incorrect. You should use project instead of iam.

Option 3 is incorrect. You should use project instead of iam. Also, the order of create and roles is wrong.

Option 4 is incorrect. The order of create and roles is wrong.
Question 97: 
Skipped
You can specify any document when creating a policy to notify you of potential infrastructure problems. Why are you filling in the documents with that form?
•	 
Stored in Cloud Storage for future use
•	 
You and your colleagues understand the purpose of the policy
•	 
Can add information so someone can diagnose and fix the problem
•	 
Option 2 and Option 3
(Correct)
Explanation
Option 4 is the correct answer.
Option 4 is the correct answer because documentation is useful in documenting the purpose of the policy and providing guidance for resolving issues.
Option 1 is incorrect. Where a policy is stored has nothing to do with its effectiveness.

Option 2 and option 3 are also partially correct, but option 4 is the better answer.
Question 98: 
Skipped
You should store log entries for longer than Stackdriver Logging keeps them. Which is the best option for retaining log data?
•	 
No options. Stackdriver Logging deletes data after the data retention period expires
•	 
Create a sink to export log data using Stackdriver Logging's export feature
(Correct)
•	 
Create a Python script that uses the Stackdriver API to write data to Cloud Storage
•	 
Write a Python script that uses the Stackdriver API and writes data to BigQuery
Explanation
Option 2 is the correct answer.
The best approach is to use Stackdriver Logging's export feature to write log data to a sink. So option 2 is the correct answer.

Option 1 is incorrect. There is a way to export the data.

Options 3 and 4 are incorrect. Creating custom scripts requires more development and maintenance time than using Logging's export functionality.
Question 99: 
Skipped
You are managing your company’s first Google Cloud project. Project leads, developers, and internal testers will participate in the project, which includes sensitive information.
You need to ensure that only specific members of the development team have access to sensitive information. You want to assign the appropriate Identity and Access Management (IAM) roles that also require the least amount of maintenance. What should you do?
•	 
Assign a basic role to each user.
•	 
Create groups. Assign a basic role to each group, and then assign users to groups
•	 
Create groups. Assign a Custom role to each group, including those who should have access to sensitive data.
•	 
Create groups. Assign an IAM Predefined role to each group as required, including those who should have access to sensitive data.
(Correct)
Explanation
Option4 is correct because Predefined roles are fine-grained enough to set permissions for specific roles requiring sensitive data access. This solution also uses groups, which is the recommended practice for managing permissions for individual roles.

Option1 is not correct for two reasons: The recommended practice is to use groups and not to assign roles to each user. Beyond that, Basic Roles do not have enough granularity to account for access to sensitive data.

Option2 is not correct because Basic roles do not have enough granularity to account for access to sensitive data.

Option3 is not correct because creating and maintaining Custom roles will require more maintenance than using Predefined roles.


reference:
https://cloud.google.com/iam/docs/understanding-roles
https://cloud.google.com/iam/docs/understanding-custom-roles
Question 100: 
Skipped
You are implementing Cloud Storage for your organization. You need to follow your organization’s regulations. They include: 1) Archive data older than one year. 2) Delete data older than 5 years. 3) Use standard storage for all other data. You want to implement these guidelines automatically and in the simplest manner available. What should you do?
•	 
Set up Object Lifecycle management policies.
(Correct)
•	 
Run a script daily. Copy data that is older than one year to an archival bucket, and delete five-year-old data.
•	 
Run a script daily. Set storage class to ARCHIVE for data that is older than one year, and delete five-year-old data.
•	 
Set up default storage class for three buckets named: STANDARD, ARCHIVE, DELETED. Use a script to move the data in the appropriate bucket when its condition matches your company guidelines.
Explanation
Option1 is correct because Object Lifecycle allows you to automate the implementation of your organization’s data policy. Option2 is not correct because changing an object's storage class does not require copying the object to another bucket. Option3 is not correct because it requires custom programming. Option4 is not correct because moving an object to a DELETED bucket does not really delete it. reference: https://cloud.google.com/storage/docs/lifecycle https://cloud.google.com/storage/docs/storage-classes
Continue



Question 1: Incorrect
You are creating a virtual machine for new application development. In this application development, VMs with the same configuration are created and deleted repeatedly in order to perform the test multiple times. You need to configure it so as to minimize the steps taken to boot a properly configured VM.
Which setting do you want to use?
•	 
Find the best machine image provided by Google Cloud and create a VM. Install the non-standard library there and take a snapshot to use to launch other instances.
(Incorrect)
•	 
Install a non-standard library of images that is close to the VM you have set up. Create a machine image to use to launch other instances.
(Correct)
•	 
Find the best machine image provided by Google Cloud and create a VM. Install the non-standard library there and create a machine image to use to boot other instances.
•	 
Install the non-standard library, save it in Cloud Storage, and configure the VM. Create a snapshot from this VM that will be used to launch other instances.
Explanation
Option 2 is the correct answer. To minimize the steps required to boot a well-configured VM, you need to boot multiple virtual machines with a machine image. First create a machine image with a non-standard library of images that is close to what you need. Then launch an instance with that machine image.
A machine image is a Compute Engine resource that stores all configurations, metadata, permissions, and data from multiple disks in a virtual machine (VM) instance. Machine images can be used in many system maintenance, backup and restore, and instance cloning scenarios.
Options 1 and 3 are incorrect. The VM generated from the optimal machine image provided by Google Cloud is not the optimal virtual machine configuration for the user. You need to install your own library to optimize the VM.
Options 1 and 4 are incorrect. A snapshot is a backup used when restoring an optical disc. Not available when replicating a virtual machine.
Question 2: Incorrect
Your company is developing a large application on Google Cloud. Hosting this application requires running many instances at the same time. In order to avoid issues, it is necessary to set the instances so that they do not interfere with each other.
What should you do to achieve this?
•	 
Split them into instance groups.
•	 
Implement a distributed configuration with instance clusters.
(Incorrect)
•	 
Divide the instance into partition groups.
•	 
No special settings are needed.
(Correct)
Explanation
Option 4 is the correct answer. By default, Google Cloud Compute Engine instances are virtualized into their separate environments so they don't interfere with each other without you having to do anything.
Therefore, no special action is required.
【reference】 
https://cloud.google.com/compute/docs/instances/
https://cloud.google.com/compute
Question 3: 
Skipped
You are trying to list role permissions using the Google Cloud console. If you use the form that appears after you click the "Add" link in your IAM form, you'll see the New Member parameter.
Which item should be set for this parameter?
•	 
Individual users and roles
•	 
Groups and roles
•	 
Individual users or groups
(Correct)
•	 
Individual users
Explanation
Option 3 is the correct answer. You can add new members by specifying individual users (email addresses) and Google groups in the parameter "new members" on the IAM console screen.
Identity and Access Management (IAM) allows you to create and manage permissions for Google Cloud resources. IAM integrates access control for Google Cloud services into one system to provide consistent operations.
Options 1 and 2 are incorrect. You cannot enter a role in "New Member".
Option 4 is incorrect. The "New Member" parameter allows you to enter groups as well as individual users.
【reference】 
https://cloud.google.com/iam/docs
Question 4: 
Skipped
You are trying to list role permissions using the Google Cloud console.
Which of the following options is the incorrect process for checking the permissions of a custom role?
•	 
Run the gcloud iam roles describe command to list the permissions for the role.
•	 
Call the roles.get () REST API method to list the permissions of the role.
•	 
Search the permission reference to see if the role grants the permission.
(Correct)
•	 
On the IAM and admin screens, select Roles. Select the check box next to a role to display the permissions for that role.
Explanation
Option 3 is the correct answer. Only basic and predefined roles can be searched for permission references to see if a role grants permissions. This process for checking the permissions of a custom role is incorrect. Use one of the following methods to determine if a role contains the appropriate permissions:
■ Run the gcloud iam roles describe command to list the permissions for the role.
■ Invoke the roles.get () REST API method to list the permissions of the roles.
■ Basic and predefined roles only: Search the permission reference to see if the role grants the permission.
■ Predefined roles only: See the description of the predefined roles to see the permissions contained in the role. 
Also, select [Role] on the IAM and administrator screens. You can also select the check box next to a role to display the permissions for that role.
【reference】
https://cloud.google.com/iam/docs/understanding-roles?skip_cache=true
Question 5: 
Skipped
You want to implement a CI / CD environment that uses the mechanism of Docker. Select the wrong option for a parameter you can specify when creating a Kubernetes cluster.
•	 
project
•	 
zone
•	 
Machine type
•	 
Disc size
•	 
Engine type
(Correct)
Explanation
Option 5 is the wrong choice. The engine type is not used as a parameter when creating a cluster. A Kubernetes cluster is a collection of nodes that run containerized applications. You can containerize your application to package your application with application dependencies and multiple required services. Kubernetes Engine allows you to specify projects, regions, zones, machine types, number of nodes, etc. when creating a cluster.
It cannot be changed after the cluster is created. These choices affect cluster availability, version stability, and network. 
Options 1, 2, 3 and 4 are items used when creating a cluster in Kubernetes Engine. Kubernetes Engine allows you to specify projects, regions, zones, machine types, number of nodes, etc. when creating a cluster.
【reference】
https://cloud.google.com/sdk/gcloud/reference/container/clusters/create
Question 6: 
Skipped
You have a CI / CD environment that uses the Docker mechanism. You have just configured a cluster using Kubernetes. Which file contains configuration information about how to communicate with the Kubernetes cluster API?
•	 
config.yaml
•	 
kubeconfig.yaml
(Correct)
•	 
pod-file-name.yaml
•	 
kbconfig..yaml
Explanation
Option 2 is the correct answer. The name of the pod spec you created in the previous step. The configuration information about how to communicate with the API is stored in the kubeconfig file. If you run multiple clusters within your Google Cloud project, you need to choose which cluster will communicate with kubectl. To do this, you can set the default cluster for kubectl by setting the current context in the Kubernetes kubeconfig file. Kubernetes uses a YAML file called kubeconfig, and stores the kubectl cluster credentials. kubeconfig contains a list of contexts referenced by kubectl when command is excecuted. By default, this file is saved in $ HOME / .kube / config
Option 1 is incorrect. config.yaml is a commonly used configuration file name, but not a Kubernetes configuration file.
Option 3 is incorrect. pod-file-name.yaml is a file that defines the name of the pod specification created in the previous step.
Option 4 is incorrect. The file kbconfig..yaml does not exist.
【reference】 
https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl
Question 7: 
Skipped
You want to implement a mechanism in Google Cloud that simplifies the provisioning method of web applications. If you're developing an application with App Engine, which options are available to determine how to split traffic between application versions?
•	 
IP address, HTTP cookie, and random selection
•	 
HTTP Cookies and Random Selection
•	 
IP address and HTTP cookie
(Correct)
•	 
IP address and random selection
Explanation
Option 3 is the correct answer. App Engine allows you to choose whether to use IP address splitting or HTTP cookie splitting when splitting traffic between two or more versions of your application.  When you have specified two or more versions for splitting, you must choose whether to split traffic by using either an IP address or HTTP cookie. It's easier to set up an IP address split, but a cookie split is more precise.

■ IP address split For IP address splitting, when an application receives a request, it translates the IP address into a hash value in the range 0-999 and uses that number to route the request. 
■ HTTP cookie split In the case of HTTP cookie splitting, the application routes the request based on the value stored in the cookie called GOOGAPPUID in the HTTP request header. If the cookie GOOGAPPUID does not exist, the request will be routed randomly. 
The traffic split feature allows you to specify a traffic distribution ratio to distribute traffic to multiple versions of the same service.

【reference】
https://cloud.google.com/appengine/docs/standard/python3/splitting-traffic
Question 8: 
Skipped
The application needs to access the drive attached to the virtual machine that has random access to the files.
Which storage type do you use?
•	 
SSD storage only
•	 
Object storage
•	 
File storage
•	 
Block storage
(Correct)
Explanation
Option 4 is the correct answer. Drives connected to virtual machines use block storage as disks. Virtual storage, which is a block storage resource, provides persistent disk space that does not lose its contents when a virtual machine instance is stopped / destroyed. Random access to files is possible by using block storage. Option 1 is incorrect. The connected drive is either an SSD or a hard drive. Therefore, it is not limited to SSD storage only.
Option 2 is incorrect. Object storage is a storage type used in Cloud Storage, etc. It cannot be configured as a drive directly attached to the virtual machine.
Option 3 is incorrect. File storage is a storage type used when making a file system type storage configuration. It cannot be configured as a drive directly attached to the virtual machine.
[Reference]
https://cloud.google.com/compute/docs/disks/performance
Question 9: 
Skipped
As a Solutions Architect, you are building an application on Google Cloud. This application should provide maximum flexibility for efficient use of resources and reduce the operational management overhead of the orchestration platform.
Which of the following is the appropriate deployment model?
•	 
Use two large virtual machines and perform a distributed configuration with a load balancer.
•	 
Run in a single large virtual machine.
•	 
Use containers in managed clusters
(Correct)
•	 
Run in a single large virtual machine to configure autoscaling.
Explanation
Option 3 is the correct answer. Containers provide maximum flexibility for efficient use of cluster resources. The orchestration platform reduces operational overhead. Therefore, efficient use of resources is possible by using containers in the cluster that is managed by the user.
Google Cloud uses Kubernetes to perform container orchestration. Kubernetes creates containers to increase the reliability of your compute configuration, reducing the time and resources you spend on DevOps, while also reducing the load associated with these tasks.
Option 1 is incorrect. The load balancer is used to distribute traffic between instances. Configuring a load balancer does not avoid the overhead of orchestration operations. 
Option 2 is incorrect. It is not recommended to run in a single virtual machine as all services will go down if the server fails.
Option 4 is incorrect. As with option 2, it is not recommended to run in a single virtual machine as all services will go down if the server fails.
【reference】 
https://cloud.google.com/containers/
Question 10: 
Skipped
Your company has decided to build a data warehouse on Google Cloud to analyze business data. In that case, it is a requirement to minimize the time spent on system management. You also want database administrators to scale their data warehousing infrastructure as needed without changing the database configuration.
Which service should you use?
•	 
BigQuery
(Correct)
•	 
Bigtable
•	 
Cloud Dataflow
•	 
Spanner
Explanation
Option 1 is the correct answer. BigQuery is a data warehousing and analytics database managed service that minimizes administrative overhead. You don't even have to configure storage. You can also perform expansion automatically when you expand your data warehousing infrastructure.
Option 2 is incorrect. Bigtable is a NoSQL database and cannot execute the large analytic queries possible in a data warehouse. 
Option 3 is incorrect. Cloud Dataflow is a stream and batch processing service that cannot execute the large analytical queries possible in a data warehouse. 
Option 4 is incorrect. Spanner is a relational database designed for scaling. All the features required of relational databases, such as schemas, SQL queries, and ACID transactions, have been proven and can be scaled globally. It can be used universally as an application database, but it is not a service for use as a data warehouse.
【reference】
https://cloud.google.com/bigquery
Question 11: 
Skipped
You are configuring a relational database with the new Cloud SQL as the data layer for your web application. Which storage type do you use to store the database data files?
•	 
Object storage
•	 
Block storage
(Correct)
•	 
File storage
•	 
Disk storage
Explanation
Option 2 is the correct answer. CloudSQL uses block storage. Persistent storage on a block device is required as the storage destination for relational database data, and RDBMS is deployed by storing that data. Block storage is storage that can be used in the same way as a general hard disk. Similar to the hard disk installed in a PC, it can be occupied and used by a specific server.
Option 1 is incorrect. Object storage does not provide storage for databases because it does not provide data blocks or file system storage. Object storage is used for medium- to long-term storage of data such as Cloud Storage.
Option 3 is incorrect. File storage is used as storage for sharing file areas or for storage areas when performing high-speed data processing. It is not used for relational database storage.
Option 4 is incorrect. There is no storage type called disk storage. The disk used for storage is block storage.
Question 12: 
Skipped
When you tried to create a backup of the Datastore, you have received an error message stating that you do not have the proper permissions role to create the Datastore backup.
What permissions do you need for the backup?
•	 
Cloud Datastore Admin
•	 
Cloud Datastore Import Export Admin
(Correct)
•	 
Cloud Datastore data management Admin
•	 
Cloud Datastore Index Admin
Explanation
Option 2 is the correct answer. The permissions required to create a Datastore backup are Cloud Datastore Import Export Admin role. This gives you the two permissions you need to create a backup. This allows you to assign the user account an IAM role that grants datastore.databases.export permission (if you want to export data) or datastore.databases.import permission (if you want to import data).
Option 1 is incorrect. Datastore Admin grants general administrative privileges. This is too privileged and violates the principle of least privilege.
Option 3 is incorrect. No such authority exists.
Option 4 is incorrect. Datastore Index Admin is a permission to create a data index, and the permission target is different.
【reference】 
https://cloud.google.com/datastore/docs/export-import-entities
Question 13: 
Skipped
Your company has decided to connect Google Cloud to the data center that it uses as its on-premises environment to move towards the use of Google Cloud. You are setting up a router to connect your Google Cloud environment to your on-premises environment.
What unique identifier should you assign to the BGP protocol?
•	 
IP address
•	 
ASN
(Correct)
•	 
Mac address
•	 
VPC ID
Explanation
Option 2 is the correct answer. The autonomous system number (ASN) is the number used to identify the cloud router on your network. By setting the ASN, you can place and use the cloud router in the network. By entering Google ASN on the router creation screen when setting up a VPN connection, you can set to identify the cloud router on the network.
Option 1 is incorrect. Assigning an IP address does not allow you to assign a unique identifier to the BGP protocol.
Option 3 is incorrect. The MAC address is physically assigned to uniquely identify each carrier on the communication network, and a unique identifier cannot be assigned to the BGP protocol.
Option 4 is incorrect. Assigning a VPC ID does not allow you to assign a unique identifier to the BGP protocol.

[Reference] https://cloud.google.com/network-connectivity/docs/router/concepts/key-terms
Question 14: 
Skipped
You are preparing to deploy your application using App Engine. In that case, you need to set automatic scaling to increase redundancy.
Which metric do you use to scale out when target_throughput_utilization is used?
•	 
CPU usage
•	 
Memory utilization
•	 
Number of simultaneous requests
(Correct)
•	 
Disk IO
Explanation
Option 3 is the correct answer. App Engine calculates the number of instances needed to handle the current application traffic based on scaling settings such as target_cpu_utilization and target_throughput_utilization. Target_throughput_utilization is set at the same time as max_concurrent_requests to specify the maximum number of concurrent requests to the application before scaling up, and when the number of concurrent requests reaches a value equal to the product of max_concurrent_requests and target_throughput_utilization, the autoscaling scheduler will Launch a new instance. 
■ target_throughput_utilization
Optional.
Specify a value between 0.5 and 0.95. The default is 0.6. Used with max_concurrent_requests to specify when concurrent requests launch a new instance. 
■ max_concurrent_requests
Optional.
The number of concurrent requests that an autoscaling instance can accept. If this number is exceeded, the scheduler will create a new instance (default: 10, maximum: 80). Used with target_throughput_utilization to specify when a new instance is launched by a concurrent request.

[Reference] https://cloud.google.com/appengine/docs/standard/go/config/appref
Question 15: 
Skipped
Your company is developing a microservice type application using multiple VMs on Google Cloud. For coordination between components, it is necessary to grant permissions to the VMs that make up the component to allow them to write to the message queue.
Which setting destination should be used at that time?
•	 
User management service account
(Correct)
•	 
Google management service account
•	 
Google managed project
•	 
User-managed project
Explanation
Option 1 is the correct answer. A service account is a mechanism that authorizes an application or virtual machine to perform a task. It is possible to grant permissions to the application for which the service account is set to allow writing to the message queue.
Service accounts are special accounts used by applications and computing workloads, not users. Service accounts are managed by Identity and Access Management (IAM).
You can use two types of service accounts: user managed service accounts and Google managed service accounts. A user-managed service account connected to a Compute Engine instance provides credentials to applications running on the instance. These credentials are used by your application to authenticate to the Google Cloud API and authorize access to Google Cloud resources.
Option 2 is incorrect. Google Management Services accounts are created, managed by Google, and automatically assigned to your projects. These accounts represent multiple Google services, and each account has a certain level of access to your Google Cloud project. This is not used to grant VM resource permissions. 
Options 3 and 4 are incorrect. There is no project division such as Google managed projects or user managed projects.
Question 16: 
Skipped
You want to configure VMs in multiple subnets and use CLI tools to configure optimal traffic settings. Currently, you are using the gcloud command to create a firewall.
Which command is used to specify the subnet ?
•	 
--subnet
•	 
--vpc
•	 
--zone
•	 
--network
(Correct)
Explanation
Option 4 is the correct answer. You can specify the subnet when creating a firewall by using the'--network' command. It is mandatory to specify a specific subnet when setting up a firewall.
When you create a VPC firewall rule, you specify the VPC network and a set of components that define the behavior of the rule. Components allow you to target specific types of traffic based on the protocol, destination port, source, and destination of the traffic.
[Reference] https://cloud.google.com/vpc/docs/firewalls
Question 17: 
Skipped
We are preparing to develop a web application using Kubernetes cluster.
What is a single instance on a Kubernetes cluster called?
•	 
Pod
(Correct)
•	 
cluster
•	 
instance
•	 
volume
Explanation
Option 1 is the correct answer. Pods are the basic unit of execution for Kubernetes applications. A pod is the smallest and simplest unit in the Kubernetes object model and represents a process running in a cluster. Pods are basically made up of one container, but multiple containers can coexist.
Option 2 is incorrect. A cluster is a collection of nodes that run a containerized application.
Option 3 is incorrect. A single instance on a Kubernetes cluster is not called an "instance".
Option 4 is incorrect. A single instance on a Kubernetes cluster is not called a "volume".
Question 18: 
Skipped
You are setting up to create a virtual network on your custom subnet. Efficient traffic control is required by configuring firewalls and applying consistent firewalls throughout the organization.
Which setting should you use for this?
•	 
Associate the hierarchical firewall rule with an organization or folder.
•	 
Associate the VPC firewall policy with your organization or folder.
•	 
Associate the hierarchical firewall policy with an organization or folder.
(Correct)
•	 
Associate the VPC firewall rule with an organization or folder.
Explanation
Option 3 is the correct answer. Hierarchical firewall policies allow you to apply consistent firewall policies throughout your organization. Hierarchical firewall policies are containers for firewall rules, and by associating a policy with an organization or folder, all rules apply to the entire target organization or folder. Similar to Virtual Private Cloud (VPC) firewall rules, these policies include rules that allow you to explicitly deny or allow connections.
Option 1 is incorrect. You need to associate a hierarchical firewall policy with an organization or folder, not a hierarchical firewall rule.
Option 2 is incorrect. There is no feature called VPC firewall policy.
Option 4 is incorrect. VPC firewall rules apply to specific projects and networks. If you want to apply firewall rules to multiple VPC networks in your organization, take advantage of hierarchical firewall policies.
Question 19: 
Skipped
There is a limited number of projects that can be created from one Google Cloud account. The current account you are using has already created a large number of projects, which means you can no longer create new projects.
Which of the following options is the correct way to increase the project limit?
•	 
Submit additional scope tickets to Google Cloud Support.
•	 
Apply to TAM to relax the upper limit.
•	 
Enforce an additional contract with Google Cloud Support.
•	 
Submit a request in Manage Quotas on the Create Project form of the console.
(Correct)
Explanation
Option 4 is the correct answer. In Google Cloud, you can relax the limit by sending a limit relaxation request on the console. The easiest way to increase the project limit is to click the Manage Quotas link on the Create Project form in the console, enter a description about the need to increase the project limit, and submit your request. 
Option 1 is incorrect. Instead of submitting an additional scope ticket to Google Cloud Support, make a cap relaxation request on the console.
Option 2 is incorrect. Technical Account Manager (TAM) is a service where you can get advice from experts. It helps keep your critical Google Cloud services in optimal condition so that you can operate effectively and efficiently in the cloud. It is used for more advanced technical support, not basic support for Google Cloud such as support for implementing cap relaxation.
Option 3 is incorrect. There are no contractual terms such as implementing additional contracts for Google Cloud Support. You simply need to make a normal cap relaxation request.
[Reference] https://cloud.google.com/resource-manager/docs/limits
Question 20: 
Skipped
The company is developing an application that uses Cloud Datastore as a data layer to store images. You are trying to set up data integrity on Cloud Datastore. Cloud Datastore allows you to set strong consistency, but this has disadvantages.
What are the disadvantages of strong integrity?
•	 
I / O operations take longer than eventual consistency.
(Correct)
•	 
Simultaneous reading by multiple users will not be possible.
•	 
The load on the network increases.
•	 
Cost is higher than eventual consistency.
Explanation
Option 1 is the correct answer. By setting strong data integrity, you can ensure data integrity so that if there is data that has undergone a write operation or an update operation, the data before the update will not be read. However, with strong data consistency, I / O operations are more time consuming than eventual consistency models, as all duplicated copies must be synchronized before operations such as updates are completed. increase. This is a disadvantage.
Eventual consistency model is a consistency model that may cause a discrepancy between the data update process and the read process because the process is performed based on the loose consistency that the data should be synchronized as a result. It is not possible to prevent the data before the update from being read. On the other hand, you don't have to sync all the duplicated copies before operations such as updates are complete, which can speed things up.
Option 2 is incorrect. Even if you set strong data integrity, it can be read by multiple users at the same time. While matching the competition of each write process, the process is performed so that synchronization is successful. 
Option 3 is incorrect. The network load remains the same, but the database processing load is higher, such as synchronizing all duplicated copies before operations such as updates are complete.
Option 4 is incorrect. There is no difference in price between eventual consistency and a strong data consistency model.
[Reference]
https://cloud.google.com/datastore/docs/articles/balancing-strong-and-eventual-consistency-with-google-cloud-datastore
Question 21: 
Skipped
You are upgrading an application that uses App Engine. It is necessary to divide the traffic to the application according to the version.
Which command should you use?
•	 
gcloud app services browse
•	 
gcloud app services set-traffic
(Correct)
•	 
gcloud app services divide-traffic
•	 
gcloud services app set-traffic
Explanation
Option 2 is the correct answer. You can set the traffic splitting for the entire project version by running the `gcloud app services set-traffic` command. gcloud app services is a command to view and manage App Engine services. It is a command to set traffic division by setting gcloud app services set-traffic.
Option 1 is incorrect. gcloud app services browse opens the specified service in your browser.
Option 3 is incorrect. gcloud app services divide-traffic is not a valid command.
Option 4 is incorrect. gcloud services app set-traffic is not a valid command.
【reference】 https://cloud.google.com/sdk/gcloud/reference/app/services/set-traffic
Question 22: 
Skipped
You are preparing to develop a web application using a Kubernetes cluster. First you need to run the Docker image in your cluster.
Which command should you use for this?
•	 
cloud run
•	 
kubectl container run
•	 
gcloud container operations
•	 
kubectl run
(Correct)
Explanation
Option 4 is the correct answer. The'kubectl run'command is used for configuration processes such as creating a POD after a Kubernetes cluster has been created. This allows you to run Docker images in your cluster. You can check the resources that can be created from the generator page of kubectl run.
Option 1 is incorrect. cloud run is not a command, but a service that develops and deploys scalable containerized applications on a fully managed serverless platform.
Option 2 is incorrect. kubectl container run is not a valid command.
Option 3 is incorrect. gcloud container operations is a command to get and list the operations of your Google Kubernetes Engine cluster.
[Reference] https://kubernetes.io/docs/reference/kubectl/conventions/#generators?
Question 23: 
Skipped
You are using a Kubernetes cluster to develop your application. It is necessary to monitor the cluster in order to improve the operation management system.
What do you need to create in Cloud Monitoring?
•	 
sync
•	 
Metrics scope
(Correct)
•	 
Pod
•	 
Workspace
Explanation
Option 2 is the correct answer. Use metric scopes to monitor your Kubernetes clusters. Metric scopes are a feature that can be used to store all configuration content for dashboards, alert policies, uptime checks, notification channels, and group definitions. You can use this to monitor your project.

Option 1 is incorrect. The sync is a function for executing the log distribution process. You can use a sync to forward some or all of your logs to a selectable destination.
Option 3 is incorrect. A pod is a single instance of a process running in a cluster. This is part of Kubernetes and is not used for monitoring.
Option 4 is incorrect. In preparation for monitoring a kubernetes cluster, it was necessary to set up a workspace in Cloud Monitoring, but now it's replaced by metric scopes.

【reference】
https://cloud.google.com/blog/products/management-tools/using-stackdriver-workspaces-help-manage-your-hybrid-and-multicloud-environment
Question 24: 
Skipped
You ended up storing a lot of sensor data in Bigtable and performing data analysis. It is necessary to manipulate the data structure of Bigtable to get the appropriate data values.
Which of the following command line utility is used for this?
•	 
gcloud bigtable
•	 
bt
•	 
cbt
(Correct)
•	 
gsutil cbt
Explanation
Option 3 is the correct answer. The command line utility for manipulating Bigtable data structures is cbt. Bigtable is a fully managed, scalable NoSQL database service for large analytical and operational workloads with up to 99.999% availability. The cbt tool is then used as a command line interface for various operations in Cloud Bigtable. It is written in Go and uses the Go client library for Cloud Bigtable.
Option 1 is incorrect. gcloud is not used to interact with Bigtable's internal data structures.
Option 2 is incorrect. There is no command utility called bt.
Option 4 is incorrect. gsutil is a command for operating Cloud Storage.
【reference】 https://cloud.google.com/bigtable/docs/cbt-overview
Question 25: 
Skipped
You have set up autoscaling for your instance group.
Which information is not usable as a scale-out threshold?
•	 
Average CPU utilization
•	 
Traffic distribution rate
(Correct)
•	 
Load balancing capacity
•	 
Cloud Monitoring numbers
Explanation
Option 2 is the correct answer. You can configure an optional autoscaling policy to trigger the addition or removal of instances based on CPU utilization, Cloud Monitoring monitoring metrics, and load balancer load balancing capacity. Disk, network delay, and memory can trigger scaling when monitoring metrics for these resources are configured. Traffic distribution rate is not used as measurement.

[Index of target usage rate]
· Average CPU usage
· HTTP load balancing processing capacity based on either utilization or requests per second
· Cloud Monitoring indicators
Question 26: 
Skipped
You are developing an application that uses Cloud Dataproc to perform customer data analysis.  Which development languages are not available when configuring Spark?
•	 
Java
•	 
Python
•	 
Scala
•	 
Ruby
(Correct)
Explanation
Option 4 is the correct answer. Ruby is not available in Spark of Dataproc. Dataproc is a fully managed and scalable service for running over 30 open source tools and frameworks, including Apache Spark, Apache Flink, and Presto.
■ For Hadoop
The majority of developers use Java and Hive. 
■ For Spark
You can use Java, Scala, Python, and R. 

Therefore, options 1, 2 and 3 are the correct languages.
【reference】 
https://cloud.google.com/dataproc/docs/concepts/overview
Question 27: 
Skipped
You used App Engine to deploy your application. However, after deploying, the application has a unavoidable error and so it is necessary to deploy an updated version of your application. 　
Which method should you use to minimize the risk of service interruption?
•	 
Deploy the new version, divide and route the traffic from the current version to the new version, and gradually migrate by increasing the traffic volume to the new version.
(Correct)
•	 
Deploy the new version and switch from the current version to the new version in bulk.
•	 
Deploy the new version and reroute traffic from the current version to the new version.
•	 
Run the update of the old version and switch to the new version.
Explanation
Option 1 is the correct answer. Take advantage of the App Engine version and the ability to split traffic to deploy the new version . Then, by dividing and routing the traffic from the current version to the new version, you can gradually increase the traffic volume to the new version. This will make switching to the new version at less of a risk during migration.
The traffic split feature allows you to specify a traffic distribution ratio to distribute traffic to multiple versions of the same service. Due to traffic splitting, A / B testing between versions allows you to control the pace at which features are deployed. 
Option 2 is incorrect. The batch switching method can be significantly affected by problems with the new version or errors during migration.
Option 3 is incorrect. You should minimize the risk during migration by switching gradually rather than simply changing the routing destination.
Option 4 is incorrect. It is necessary to migrate by switching the routing, not by updating the previous version.
Question 28: 
Skipped
When using a cloud provider managed cluster, which of the following will the cloud provider manage for you?
•	 
monitoring
•	 
networking
•	 
Part of security administration tasks
•	 
all
(Correct)
Explanation
Option 4 is the correct answer.
When using a managed cluster, the cloud provider is responsible for monitoring the health of the nodes in the cluster, configuring network settings between the cluster's nodes, and configuring security controls such as firewalls.
Question 29: 
Skipped
What block size do all block storage systems use?
•	 
4KB
•	 
8KB
•	 
16KB
•	 
Different block sizes
(Correct)
Explanation
Option 4 is the correct answer.
The block size of block storage systems can vary. The block size is constructed when the filesystem is created. Generally, Linux uses a 4 KB block size.
Question 30: 
Skipped
Suppose your company is based in X and you want to run a virtual server for Y. What factors determine the cost per minute?
•	 
Time of day during which the virtual machine is running
•	 
Server features
(Correct)
•	 
Application to run
•	 
none of the above
Explanation
Option 2 is the correct answer.
Server features such as the number of virtual servers, the amount of memory, and the region in which the virtual machines run affect cost. Option 2 is the correct answer. Time of day is not a factor. The type of application running in the virtual machine is also not a factor.
Question 31: 
Skipped
You are now using containers to deploy microservices. You could install and manage Docker on my Compute Engine instance, but you would like Google Cloud to provide a container management service. Which two Google Cloud services can run containers in managed services?
•	 
App Engine standard environment and App Engine flexible environment
•	 
Kubernetes Engine and App Engine standard environment
•	 
Kubernetes Engine and App Engine flexible environment
(Correct)
•	 
App Engine standard environment and Cloud Functions
Explanation
Option 3 is the correct answer.
App Engine flexible environment lets you run containers in an App Engine PaaS environment. Kubernetes Engine is an orchestration platform for running containers. Both offer container management services.

App Engine standard environment runs applications in language-specific sandboxes. Not your typical container management system. Cloud Functions is a serverless service for executing code in response to events. We do not provide container services.
Question 32: 
Skipped
Which of the following services' resources can be configured and managed using the Cloud SDK?
•	 
Compute Engine
•	 
Cloud Storage
•	 
network firewall
•	 
all of the above
(Correct)
Explanation
Option 4 is the correct answer.
All three services, Compute Engine, Cloud Storage, and Network Firewall, can be managed and configured using the Cloud SDK.
Question 33: 
Skipped
Your company has deployed 100,000 Internet of Things (IoT) sensors to collect data on the condition of factory equipment. Each sensor collects data and sends it to the data store every 5 seconds.
The sensor runs continuously. Daily reports generate data on the maximum, minimum, and average values for each metric collected by each sensor. This application does not need to support transactions. Which database product should you recommend?
•	 
Cloud Spanner
•	 
Cloud Bigtable
(Correct)
•	 
Cloud SQL MySQL
•	 
Cloud SQL PostgreSQL
Explanation
Option 2 is the correct answer.
Bigtable is designed to store billions of rows of data. Collecting data from 100,000 sensors at 5 second intervals yields 6,000,000 data points per minute, or 8,640,000,000 data points per day. Spanner is a relational database and supports transactions, but they are not required. Cloud SQL. For MySQL and Cloud SQL PostgreSQL, this
It is difficult to scale to handle this level of read-demand performance.
Question 34: 
Skipped
You are designing an application that uses a set of services to transform data into a format suitable for a data warehouse. The transforming application writes to a message queue as it processes each input file. You don't want users to have permission to write to message queues. Which of the following should be used to allow an application to write to a message queue?
•	 
billing account
•	 
service account
(Correct)
•	 
messaging account
•	 
folder
Explanation
Option 2 is the correct answer.
Service accounts are designed to grant application or virtual machine privileges to perform tasks.

Option 1 is incorrect. A billing account associates a charge with a payment method.

Option 3 is incorrect. The messaging account is incorrect.

Option 4 is incorrect. Folders are part of the resource hierarchy and have nothing to do with the ability of an application to perform tasks.
Question 35: 
Skipped
Your company's finance department has asked you for advice on how to automate payments for Google Cloud services. What kind of account would you recommend in this case?
•	 
service account
•	 
billing account
(Correct)
•	 
resource account
•	 
credit account
Explanation
Option 2 is the correct answer.
Billing accounts are used to specify payment information and set up automatic payments. Service accounts are used to entitle virtual machines and are not related to billing and payments. Resource and credit accounts do not exist.
Question 36: 
Skipped
What parameters must you specify when creating a VM on Compute Engine?
•	 
projects and zones
(Correct)
•	 
Username and admin role
•	 
billing account
•	 
Cloud Storage bucket
Explanation
Option 1 is the correct answer. Virtual machines are created in projects that are part of the resource hierarchy. Since they are located in geographic regions and data centers, they are also designated as zones. Username and administrative role are not specified during creation.
The billing account is associated with the project and does not need to be specified when the virtual machine is created. Cloud Storage buckets are created separately from virtual machines. Not all virtual machines use storage buckets.
Question 37: 
Skipped
Which of the following applies to the limitations of preemptible VMs?
•	 
finish within 24 hours
•	 
Not always available. Availability varies by zone and region
•	 
Unable to migrate to normal VM
•	 
all of the above
(Correct)
Explanation
Option 4 is the correct answer.
Preemptible virtual machines stop after 24 hours. Google does not guarantee the availability of Preemptible Virtual Machines. After an instance starts as a preemptible machine, it cannot be migrated to a standard virtual machine. However, you can save the snapshot and use it to create a new standard instance.
Question 38: 
Skipped
What actions can you specify in the advanced boot disk configuration when creating a new virtual machine?
•	 
Add new disks, reformat existing disks, attach existing disks
•	 
Adding new disks and reformatting existing disks
•	 
Adding new disks and attaching existing disks
(Correct)
•	 
Reformatting an existing disk and attaching an existing disk
Explanation
Option 3 is the correct answer.
Two operations that can be specified when using a boot disk configuration are adding a new disk and attaching an existing disk. So option 3 is the correct answer.
Question 39: 
Skipped
You created an Ubuntu virtual machine. You want to log into my virtual machine and install some software packages. Which network service do you use to access your virtual machine?
•	 
FTP
•	 
SSH
(Correct)
•	 
RDP
•	 
ipconfig
Explanation
Option 2 is the correct answer.
SSH is a service for connecting to remote servers and logging into a terminal window. After logging in, you have access to the command line. So option 2 is the correct answer.

Option 1 is incorrect. FTP is a file transfer protocol. Users cannot be logged in to perform system administrator tasks.

Option 3 is incorrect. RDP is a protocol for remote access on Windows servers, not the Linux distribution Ubuntu.

Option 4 is incorrect. ipconfig is a command-line utility for configuring a device's IP stack and does not allow login to remote servers.
Question 40: 
Skipped
Use snapshots to save a copy of the 100 GB disk. Create a snapshot and then add 10 GB of data. Create a second snapshot. What is the total amount of storage used by the two snapshots (assuming no compression)?
•	 
210 GB. 100 GB for the first snapshot, 110 GB for the second snapshot
•	 
110GB. 100 GB for the first snapshot, 10 GB for the second snapshot
(Correct)
•	 
110GB. 110 GB for second snapshot (first snapshot is automatically deleted)
•	 
221GB. 100 GB for the first snapshot, 110 GB for the second snapshot, plus 10% (11 GB) of the second snapshot for metadata overhead
Explanation
Option 2 is the correct answer.
When you first create a snapshot, GCP creates a full copy of your data on your persistent disk. The next time you create a snapshot from that disk, GCP will only copy the data that has changed since the previous snapshot.

Option 1 is incorrect. GCP does not store a full copy of the second snapshot.

Option 3 is incorrect. The first snapshot is not automatically deleted.

Option 4 is incorrect. Subsequent snapshots will not experience the 10% load.
Question 41: 
Skipped
You are deploying an application that requires high scalability and high availability. Which Compute Engine component provides high scalability and high availability?
•	 
preemptible instance
•	 
instance group
(Correct)
•	 
Cloud Storage
•	 
GPUs
Explanation
Option 2 is the correct answer.
An instance group is a set of virtual machines that configure scaling and are used with a load balancer. This increases availability.

Option 1 is incorrect. Preemptible instances are not highly available because Google Cloud can shut them down at any time.

Option 3 is incorrect. Cloud Storage is not a Compute Engine component.

Option 4 is incorrect. GPUs are useful for improving throughput for compute-intensive operations, but are not relevant for high availability.
Question 42: 
Skipped
You have noticed that the performance of my application has degraded significantly. You recently changed some resource configurations in my Kubernetes cluster, so You suspect that this change caused a warning to the number of pods running in the cluster. Where can you find details on how many pods should be running?
•	 
Deployments
•	 
Stackdriver
•	 
ReplicaSet
(Correct)
•	 
Jobs
Explanation
Option 3 is the correct answer.
ReplicaSets are controllers that manage the correct number of pods. Option 3 is the correct answer.

Option 1 is incorrect. Deployments are the versions of application code running on the cluster.

Option 2 is incorrect. Stackdriver is a monitoring and logging service for monitoring but not controlling Kubernetes clusters.

Option 4 is incorrect. Jobs is a workload abstraction and is not related to the number of pods running in the cluster.
Question 43: 
Skipped
What command would you use to create 10 replicas of a Deployment named ch07-app-deploy?
•	 
kubectl upgrade deployment ch07-app-deploy--replicas=5
•	 
gcloud containers deployment cho7-app-deploy --replicas=5
•	 
kubectl scale deployment ch07-app-deploy --replicas=10
(Correct)
•	 
kubectl scale deployment ch07-app-deploy --pods=5
Explanation
Option 3 is the correct answer.
Option 1 uses upgrade instead of scale.
Option 2 is incorrect usage of gcloud .
Option 4 uses the incorrect parameter pods.
Question 44: 
Skipped
You have been tasked with diagnosing performance issues in an application running on multiple Kubernetes clusters. First, at a high level, you want to understand the details of the cluster. Which command should you use?
•	 
gcloud container list
•	 
gcloud container clusters list
(Correct)
•	 
gcloud clusters list
•	 
none of the above
Explanation
Option 2 is the correct answer.
Correct command is gcloud container clusters list.
Anything else is an invalid command.
Question 45: 
Skipped
What actions are available in the [ACTIONS] menu when viewing deployment details?
•	 
Scale Autoscale
•	 
Autoscale, Expose, Rolling Updates
(Correct)
•	 
Add, Modify, Delete
•	 
none of the above
Explanation
Option 2 is the correct answer.
There are four actions available for deployment (Autoscale, Expose, Rolling Update, Scale). So option 2 is the correct answer. Add, Modify, Delete cannot be selected.
Question 46: 
Skipped
Which kubectl command do you use to add a service?
•	 
run
(Correct)
•	 
start
•	 
initiate
•	 
deploy
Explanation
Option 1 is the correct answer.
kubectl run is the command used to start the deployment. Requires a name, image, and port specification for the deployment. Any other choice is not a valid kubectl command.
Question 47: 
Skipped
What command should you use to deploy an App Engine application from the command line?
•	 
gcloud components app deploy
•	 
gcloud app deploy
(Correct)
•	 
gcloud components instances deploy
•	 
gcloud app instance deploy
Explanation
Option 2 is the correct answer.
Options 1 and 3 are incorrect. This is because the gcloud components command is used to install the gcloud command for working with parts of App Engine such as the Python runtime environment.

Option 4 is incorrect. You don't need to specify an instance in the command.
Question 48: 
Skipped
You have deployed an app that hosts a service that provides the current time for any timezone. The project containing this code is current-time-zone. The service that provides the user interface is time-zone-ui. The service that performs the calculation is time-zone-calculate.
Which URL will users use to access this service?
•	 
current-time-zone.appengine.com
•	 
current-time-zone.appspot.com
(Correct)
•	 
time-zone-ui.appspot.com
•	 
time-zone-calculate.appspot.com
Explanation
Option 2 is the correct answer.
App Engine applications can be accessed via a URL with the project name followed by appspot.corn . So option 2 is the correct answer.

Option 1 is incorrect. The domain is not appengine.com.
Options 3 and 4 are incorrect. The name of the service is not used to refer to the application as a whole.
Question 49: 
Skipped
You work for a startup company. A major concern is cost. A small performance penalty is acceptable for cost savings. How should you configure scaling for my application running on App Engine?
•	 
Use dynamic instances with autoscaling and basic scaling
(Correct)
•	 
Use resident instances with autoscaling and basic scaling
•	 
Use dynamic instances with manual scaling
•	 
Use resident instances with manual scaling
Explanation
Option 1 is the correct answer.
Using dynamic instances with autoscaling or basic scaling automatically adjusts the number of instances to use based on load. So option 1 is the correct answer.

Option 2 is incorrect. Autoscaling and basic scaling only create dynamic instances.

Options 3 and 4 are incorrect. Manual scaling does not automatically adjust your instances. This can lead to more instances running than are needed at any given time.
Question 50: 
Skipped
When actions occur in GCP, such as writing a file to Cloud Storage or adding a message to a Cloud Pub/Sub topic, what are these actions called?
•	 
incident
•	 
event
(Correct)
•	 
trigger
•	 
log entry
Explanation
Option 2 is the correct answer.
These actions are called events in Google Cloud terminology.

Option 1 is incorrect. Incidents are security or performance related occurrences, but are not related to the expected standard actions that make up the event.

Option 3 is incorrect. A trigger is a declaration that a certain function will be executed when an event occurs. Log entries are related to applications that record data for important events.

Option 4 is incorrect. Log entries are useful for monitoring and compliance, but by themselves are not event-related actions.
Question 51: 
Skipped
You have stopped using Cloud Functions and would like to remove them from my project. Which command should you use from the command line to remove Cloud Functions?
•	 
gcloud functions delete
(Correct)
•	 
gcloud components function delete
•	 
gcloud components delete
•	 
gcloud delete functions
Explanation
Option 1 is the correct answer.

Option 2 is wrong because it uses components . Using components is required when installing or updating gcloud, but not when removing Cloud Functions. So options 2 and 3 are incorrect.
Option 4 is incorrect. In gcloud commands, the GCP entity type (functions in this case) precedes the operation name (delete in this case).
Question 52: 
Skipped
When specifying a Cloud Function in Cloud Storage, you must specify an event type such as finalize, delete, archive, etc. When specifying Cloud Functions in Cloud Pub/Sub, you don't need to specify the event type. Why is this.
•	 
Cloud Pub/Sub doesn't have event type triggers
•	 
Cloud Pub/Sub can only trigger one type of event when publishing a message.
(Correct)
•	 
Cloud Pub/Sub analyzes your function code to determine the correct event type.
•	 
Problem statement is incorrect. Cloud Pub/Sub functions also need to specify the event type
Explanation
Option 2 is the correct answer.
This is the only type of event that is triggered in Cloud Pub/Sub and is triggered when a message is published. So option 2 is the correct answer.

Option 1 is incorrect. Cloud Pub/Sub can trigger one event type.

Option 3 is incorrect. Cloud Pub/Sub does not analyze your code to determine when to run it.

Option 4 is incorrect. You don't need to specify the event type in your Cloud Pub/Sub function.
Question 53: 
Skipped
A software developer on their team wants your help to improve the query performance of their database application. A developer is using a Second Generation instance of Cloud SQL MySQL. Which option do you recommend?
•	 
Mercorystore and SSD persistent disks
(Correct)
•	 
Memorystore and HDD Persistent Disk
•	 
Datastore and SSD Persistent Disk
•	 
Datastore and HDD Persistent Disk
Explanation
Option 1 is the correct answer.
Mercorystore is a managed Redis cache. A cache can be used to store the results of queries. Queries that reference cached data can read data from the cache. This method is significantly faster than reading from persistent disk. SSD latency is significantly reduced over hard disk drives. SSDs are used in performance critical applications such as databases.

Options 2 and 4 are incorrect. HDD persistent disks provide the best performance in terms of IOPS.

Options 3 and 4 are incorrect. The data store is a managed NoSQL database and does not impact SQL query performance.
Question 54: 
Skipped
Which statement is incorrect about Cloud Storage?
•	 
Cloud Storage buckets can have retention periods
•	 
Lifecycle configuration can be used to change the storage class from Regional to Multi-Regional
(Correct)
•	 
Cloud Scorage does not provide block-level access to data in files stored in buckets
•	 
Cloud Storage is designed for durability
Explanation
Option 2 is the correct answer.
A lifecycle configuration can change the storage class from Regional to Nearline or Coldfine. Once a bucket is created as Regional or Multi-Regional, it cannot be changed to Multi-Regional or Regional. Option 2 is the correct answer.

Option 1 is correct, so it is the wrong choice. You can set the retention period when creating a bucket.

Option 3 is correct, so it is the wrong choice. Cloud Storage does not provide a file system, including access to internal data blocks.

Option 4 is correct, so it is the wrong choice. Cloud Storage is highly durable.
Question 55: 
Skipped
A product manager asked me for advice on a database service that could be an option for a new application. Support for transactions and tabular data is important. A database that supports common query tools is preferred. Which database would you recommend to a product manager?
•	 
BigQuery and Spanner
•	 
Cloud SQL and Spanner
(Correct)
•	 
Cloud SQL and Bigtable
•	 
Bigtable and Spanner
Explanation
Option 2 is the correct answer.
Cloud SQL and Spanner are relational databases and are well suited for applications with transactional processing. So option 2 is the correct answer.

Option 1 is incorrect. BigQuery is relational, but designed for data warehousing and analytics, not transaction processing.

Options 3 and 4 are incorrect. Bigtable is a column-oriented NoSQL database, not a relational database.
Question 56: 
Skipped
What is the correct command line to export data from Datastore?
•	 
gcloud datastore export '[NAMESPACE]' gs://[BUCKET_NAME]
•	 
gcloud datastore export gs://[BUCKET_NAME]
•	 
gcloud datastore export --namespaces="(NAMESPACE)' gs://(BUCKET_NAME]
(Correct)
•	 
gcloud datastore dump --namespaces='[NAMESPACE)' gs://[BUCKET_NAME]
Explanation
Option 3 is the correct answer.
The correct basic command is gcloud datastore export followed by the --namespaces parameter and the name of the Cloud Storage bucket where you want to store your export files.

Option 1 is incorrect. ---namespaces Missing parameter name.

Option 2 is incorrect. No namespace.

Option 4 is incorrect. You are using dump instead of export as a command or verb.
Question 57: 
Skipped
When you enter your query in the BigQuery query form, BigQuery analyzes your query.
What metric estimates are displayed?
•	 
Time required to enter query
•	 
query cost
•	 
amount of data scanned
(Correct)
•	 
Bytes passed between servers in a BigQuery cluster
Explanation
Option 3 is the correct answer.
BigQuery displays an estimate of the amount of data scanned. This is important because BigQuery charges for the data scanned by the query.

Option 1 is incorrect. Knowing how long it takes to enter a query is not useful.

Option 2 is incorrect. To estimate costs, you should use estimates of scanned data and pricing calculators.

Option 4 is incorrect. BigQuery does not create clusters, unlike Dataproc. No network I/O data is displayed.
Question 58: 
Skipped
You're using the Cloud Console and want to see some jobs running in BigQuery. Go to the BigQuery part of the console. Which menu item displays jobs when clicked?
•	 
Job History
(Correct)
•	 
Active Jobs
•	 
My Jobs
•	 
Job status cannot be viewed in the console. You have to use bq on the command line.
Explanation
Option 1 is the correct answer.
The menu option is "Job History".

Options 2 and 3 are incorrect. There are no Active Jobs or My Jobs options. Job History shows active jobs, completed jobs, and jobs with errors.

Option 4 is incorrect. You can get the job status in the console.
Question 59: 
Skipped
You want to estimate the cost of running BigQuery queries. Which two services should you use on Google Cloud Platform?
•	 
BigQuery and Billing
•	 
Billing and Pricing Calculator
•	 
BigQuery and the Pricing Calculator
(Correct)
Explanation
Option 3 is the correct answer.
BigQuery displays an estimate of the amount of data scanned. The Pricing Calculator will estimate the cost to scan that amount of data.

Option 1 and option 2 are incorrect. The billing service tracks changes as they occur. It is not used for the purpose of estimating future or potential charges.
Question 60: 
Skipped
A software developer has asked for your help in exporting data from a Cloud SQL database.
You mentioned which database to export and which bucket to store the export file in, but not what file format to use for the export file. What are the Export File Format options?
•	 
CSV and XML
•	 
CSV and JSON
•	 
JSON and SQL
•	 
CSV and SQL
(Correct)
Explanation
Option 4 is the correct answer.
When exporting a database from Cloud SQL, the export file format: options are CSV and SQL. So option 4 is the correct answer.

Option 1 is incorrect. XML is not an option.

Also option 2 and option 3 are incorrect. JSON is not an option.
Question 61: 
Skipped
For compliance, the team needs to back up Datastore data to an object storage system. Data is stored in the default namespace. What command would you use to export the Datastore default namespace to the bucket ace-exam-bucket1 ?
•	 
gcloud datastore export --namespaces="(default)" gs://ace-exam-bucket1
(Correct)
•	 
gcloud datastore export --namespaces="(default)" ace-exam-bucket1
•	 
gcloud datastore dump ---namespaces="(default)" gs://ace-exam-bucket1
•	 
gcloud datastore dump --namespaces="(default)" ace-exam-bucket1
Explanation
Option 1 is the correct answer.
Option 1 uses the correct command, gcloud datastore export followed by the namespace and bucket name.

Option 2 is incorrect. You are missing gs: in your bucket name.

Options 3 and 4 are incorrect. I'm using the dump command instead of export. Also, the option 4 bucket name does not have gs://.
Question 62: 
Skipped
As dictated by company policy, you should back up the Datastore database at least once a day. Auditors question whether the Datastore export is sufficient. Can you describe what the Datastore export command outputs?
•	 
1 entity file
•	 
1 metadata file
•	 
Folder with one metadata file and data
(Correct)
•	 
Folders with one metadata file, entity files, and data
Explanation
Option 3 is the correct answer.
The export process creates a metadata file containing information about the exported data and a folder containing the data. So option 3 is the correct answer.

Option 1 is incorrect. Export does not produce a single file. A metadata file and a folder containing the data will be generated.

Option 2 is incorrect. It does not contain data folders. Also option 4 is incorrect.
Question 63: 
Skipped
You have created multiple subnets. Most of them are sending logs to Stackdriver. One subnet is not sending logs. Which option may have been misconfigured while creating a subnet that was not forwarding logs?
•	 
Flow Logs
(Correct)
•	 
Private IP Access
•	 
Stackdriver Logging
•	 
Variable-Length Subnet Masking
Explanation
Option 1 is the correct answer.
The Flow Log option determines whether logs are sent to Stackdriver.

Option 2, Private IP Access, determines whether the virtual machine requires an external IP address to use Google services.
Question 64: 
Skipped
You want to create a cluster named udemy-cluster with two nodes.
Which command is correct to execute?
•	 
gcloud create container clusters name udemy-cluster --num-nodes : 2
•	 
gcloud beta container clusters create udemy-cluster --num-nodes : 2
•	 
gcloud create container clusters name udemy-cluster --num-nodes = 2
•	 
gcloud beta container clusters create udemy-cluster --num-nodes = 2
(Correct)
Explanation
Option 4 is the correct answer. You can create a cluster based on two nodes with the command gcloud beta container clusters create udemy-cluster --num-nodes = 2. Use the gcloud beta container clusters create [cluster name] command to create a cluster with this name. You can set the number of nodes with the additional command'--num-nodes = 2'.
Option 1 is incorrect. gcloud create container clusters name udemy-cluster --num-nodes : 2 is not a valid command. Must be gcloud beta container clusters create udemy-cluster. Also, in the node settings, add --num-nodes = 2 and the command.
Option 2 is incorrect. In the node settings, add --num-nodes = 2 and the command.
Option 3 is incorrect. gcloud create container clusters name udemy-cluster --num-nodes : 2 is not a valid command. Must be gcloud beta container clusters create udemy-cluster.

【reference】 
https://cloud.google.com/sdk/gcloud/reference/beta/container/clusters/create
Question 65: 
Skipped
A public IP address is assigned to the Linux VM instance, and SSH connection permission is set in the firewall rule. However, one of the instances cannot be connected to via SSH.
What is the most likely cause for this?
•	 
SSH is not supported on the instance.
•	 
You are using a persistent disk for the disk.
•	 
SSH protocol is not allowed in the instance communication settings.
•	 
The instance may be down.
(Correct)
Explanation
Option 4 is the correct answer. If you cannot connect to the Internet or SSH to a Linux instance that has an external IP address, the main reasons are as follows.
-The instance is stopped. 
-SSH communication is not permitted in the firewall settings.
-The IP address of the connection destination is incorrect.
-The key pair or path used for connection is incorrect.

Option 1 is incorrect. The Linux instance is incorrect because it supports SSH communication. 
Option 2 is the correct answer. The type of disk and the availability of SSH connection are irrelevant.
Option 3 is incorrect. There is no setting method called instance communication setting.
Question 66: 
Skipped
You are using Cloud DNS to set up your domain to publish your web application to the Internet. You need to store canonical name information.
Which record should you set?
•	 
CNAME record
(Correct)
•	 
AAAA record
•	 
MX record
•	 
A record
Explanation
Option 1 is the correct answer. A CNAME record is a record that defines an alias for a domain name or host name defined in DNS. Map the alias name as canonical name information to the CNAME record.
CNAME records are typically used to map a subdomain, such as www or mail, to the domain that hosts the content of that subdomain. For example, you can use a CNAME record to map the web address www.example.com to the actual website in the domain example.com.
Options 2 and 4 are incorrect. AAAA records and A records store domain name-to-IP address mappings.
Option 3 is incorrect. An MX record is a resource record that indicates a domain's mail exchange server.
Question 67: 
Skipped
You are implementing a web application on Google Cloud. You plan to use a VM for the WEB server and configure a Cloud Storage bucket for the data layer. The VM needs to read an object from the Cloud Storage bucket. This read operation is allowed by the IAM role granted to the VM's service account, but is denied in the scope assigned to the VM.
Can this VM perform bucket reads?
•	 
If approved on the bucket side, a read can be executed.
•	 
The read is not performed.
(Correct)
•	 
The read is performed.
•	 
A read is performed, but an error occurs on the bucket side.
Explanation
Option 2 is the correct answer. To read an object in a Cloud Storage bucket from a VM, both the scope assigned to the service account and the IAM role must allow operations for the read process to succeed. Therefore, in this case, the scope permission cannot be set and the read process from the VM cannot be executed. When you set up a new instance to run as a service account, the IAM role you assign to the service account specifies the access level for the service account.
If a service account does not have an IAM role, there are no API methods that you can run against the instance in that service account. In addition, the access scope of an instance defines the default OAuth scope for requests made using the gcloud tools and client libraries on that instance. As a result, access to API methods may be further restricted when authenticating with OAuth.
【reference】
https://cloud.google.com/compute/docs/access/service-accounts
Question 68: 
Skipped
You are preparing to template and manage the deployment method of cloud resources such as VM instances. In that case, use Deployment Manager to perform resource deployment.
What of the following is the incorrect component of each resource deployed by Deployment Manager?
•	 
Image
(Correct)
•	 
Type
•	 
Name
•	 
Properties
Explanation
Option 1 is the correct answer. image is not a basic component of Deployment Manager. Configuration with Deployment Manager describes all the resources needed for a single deployment. A configuration is a YAML-formatted file that lists the resources you create and their properties, respectively.  A section is required and includes a list of resources to create here. Each resource you deploy with Deployment Manager must contain three components:
■ name
A user-defined string to identify the resource (my-vm, project-data-disk, the-test-network, etc.).
■ type
The type of resource to be deployed (compute.v1.instance, compute.v1.disk, and so on). Basic resource types are listed and described in Supported Resource Types. 
■ properties
Resource type parameter. The value must match one of the resource type properties (for example, zone: asia-east1-a boot: true).
【reference】
https://cloud.google.com/deployment-manager/docs/fundamentals
Question 69: 
Skipped
As a Google Cloud administrator, you have set up monitoring using the Operations Suite and need to set up alert policies.
Which of the following elements cannot be specified in the alert policy?
•	 
conditions
•	 
notification
•	 
document
•	 
Duration
(Correct)
Explanation
Option 4 is the wrong element and so is the correct answer. You can specify conditions, notifications, and documents when you create an alert policy. The duration cannot be set.
Option 1 is the correct configuration element. The condition describes the resource to be monitored, the index of the resource, and the timing when the condition is satisfied. An alert policy requires at least one condition, but an alert policy can contain up to six conditions.
Option 2 is the correct setting element. Notifications lists whether you have configured this policy to notify you when an incident is opened and closed, or whether you have configured it to notify you only when an incident is opened. To change the notification behavior, edit the alert policy.
Option 3 is the correct setting element. "Document" can specify the document to be included in the notification.
【reference】
https://cloud.google.com/monitoring/alerts/using-alerting-ui
Question 70: 
Skipped
The development team will use the Compute Engine to utilize the VM. Therefore, you want to estimate the cost of using a VM.
Select the elements that do not affect the price of the VM from the following.
•	 
Server execution time
•	 
Instance type selected
•	 
Region where the server is installed
•	 
Zone where the server is installed
(Correct)
Explanation
Option 4 is the correct answer because it is an incorrect billing factor. The cost of a Compute Engine VM is affected by server characteristics such as the number of virtual servers, the amount of memory, and the region in which the virtual machine runs, but the zone in which the server is installed does not affect the cost.
Option 1 is the correct billing factor. All ComputeEngine vCPUs, GPUs, and GB memories are charged for at least one minute. For example, if you run a virtual machine for 30 seconds, you will be charged for 1 minute. After the first minute, your instance will be charged per second.
Option 2 is the correct billing factor. For example, VM generic machine types have predefined machine types and custom machine types available in each region. The predefined machine types have preset vCPU numbers and memory capacities, while custom machine types are billed according to resource-based pricing.
Option 3 is the correct billing factor. VMs are priced differently depending on the region, so the region you deploy determines the price.
【reference】
https://cloud.google.com/compute/vm-instance-pricing
Question 71: 
Skipped
You are preparing to use a virtual server with Compute Engine. At that time, it is necessary to set the scope.
Which command can be used to set the scope?
•	 
gcloud compute instances set-service-account
(Correct)
•	 
gcloud compute instances add-access-config
•	 
gcloud compute instances get-iam-policy
•	 
gcloud compute instances create
Explanation
Option 1 is the correct answer. The correct command to use when setting the scope of a virtual server is gcloud compute instances set-service-account. Gcloud compute instances are commands for reading and manipulating ComputeEngine virtual machine instances. Set the Compute Engine service account and access scope by using gcloud compute instances set-service-account.
Option 2 is incorrect. gcloud compute instances add-access-config creates a ComputeEngine virtual machine access configuration. It does not include access scope.
Option 3 is incorrect. gcloud compute instances get-iam-policy gets the IAM policy for your Compute Engine instance.
Option 4 is incorrect. gcloud compute instances create creates ComputeEngine virtual machine instances.
【reference】
https://cloud.google.com/sdk/gcloud/reference/compute/instances/set-service-account
Question 72: 
Skipped
You are developing a CI / CD environment using a Kubernetes cluster and developing an application. You need to notify your team members in case of an issue on your Kubernetes cluster.
Which notification method is available?
•	 
Set up SMS, Slack, and email using notification settings in the Operations Suite
(Correct)
•	 
Set up SMS, Slack, and email with Stackdriver alerts
•	 
Install Operations Suite Agent to configure SMS, Slack, and Email
•	 
Set up SMS, Slack, and email in operation alerts
Explanation
Option 1 is the correct answer. As a way to notify team members when something goes wrong on your Kubernetes cluster, you can use the notifications feature of the Operations Suite to set up SMS, Slack, and email. When you enable integration of Cloud Operations for GKE with Cloud Logging and Cloud Monitoring in your cluster, logs are stored in a dedicated persistent data store. This allows you to take advantage of Cloud Monitoring's operational suite to set up notifications via SMS, Slack, and email.
Option 2 is incorrect. Stackdriver alerts are an older feature of the Operations Suite and now run in the Operations Suite.
Option 3 is incorrect. The Operation Suite Agent is a function that is installed in a virtual machine and used to acquire log information.
Option 4 is incorrect. There is no function called operation alert. You can set SMS, Slack, and Email in the monitoring settings in the Operations Suite.
【reference】 
https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs
https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring/ui/create-alert
Question 73: 
Skipped
As an administrator of Google Cloud, you are setting permissions using roles. To do this, you need to display information about the predefined roles used by Google Cloud.
Which command should you use?
•	 
gcloud iam roles view [ROLE-ID]
•	 
gcloud iam roles list [ROLE-ID]
•	 
gcloud iam roles describe [ROLE-ID]
(Correct)
•	 
gcloud iam roles show [ROLE-ID]
Explanation
Option 3 is the correct answer. The correct command to display information about roles is gcloud iam roles describe [ROLE-ID]. describe is typically used to display more detailed information about a resource.
Option 2 is incorrect. gcloud iam roles list [ROLE-ID] uses list instead of describe. list is typically used to display a brief description of multiple resources.
Options 1 and 4 are incorrect. There are no commands that use View or show.

【reference】
https: //cloud.google.com/sdk/gcloud/reference/iam/roles/describe
Question 74: 
Skipped
You are setting up to develop a container application that leverages a Kubernetes cluster. It is necessary to duplicate each node pool into three zones in the control plane region.
How should you configure the cluster?
•	 
Use a multi-zone cluster in Autopilot mode.
•	 
Use a multi-zone cluster in Standard mode.
•	 
Use a region cluster in Standard mode.
(Correct)
•	 
Use a region cluster in Autopilot mode.
Explanation
Option 3 is the correct answer. You can use a region cluster in Standard mode to replicate each node pool to three zones in the control plane region.
When you create a Kubernetes cluster, choose one of the following operational modes:
Autopilot: Provides a fully provisioned managed cluster configuration. Cluster configuration options are automatically created for clusters created in Autopilot mode. Autopilot clusters are preconfigured with an optimized cluster configuration for production workloads.
Standard: Provides a high degree of configuration flexibility for the underlying infrastructure of the cluster. For clusters created using Standard mode, users determine the configuration required for production workloads. In Autopilot mode, a highly available configuration using regions is automatically applied, but in Standard mode, you can select a configuration using regions or zones from the following: 
■ Single zone cluster A single zone cluster contains one control plane operating in one zone. This control plane manages workloads on nodes running in the same zone. 
■ Multi-zone cluster A multi-zone cluster contains one replica of the control plane operating in one zone and multiple nodes operating in multiple zones. The workload continues to operate during cluster upgrades and when the zone in which the control plane operates is down. However, you cannot configure the cluster or the nodes and workloads within it until the control plane is available. Multi-zone clusters provide a consistent workload while balancing availability and cost. 
■ Region cluster A region cluster contains multiple replicas operating in multiple zones within a particular region. Nodes in a region cluster can run in multiple zones or a single zone, depending on the location of the configured nodes. By default, GKE replicates each node pool into three zones in the control plane region. When you create a cluster or add a new node pool, you can change the default configuration by specifying the zone in which the nodes of the cluster run. All zones must be in the same region as the control plane. 
Options 1 and 4 are incorrect. In Autopilot mode, highly available regional configurations are automatically applied. Therefore, you cannot select the cluster configuration.
Option 2 is incorrect. A multi-zone cluster contains one replica of the control plane operating in one zone and multiple nodes operating in multiple zones, but the redundancy of replicating the nodes into three zones is guaranteed. 
【reference】
https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler
Question 75: 
Skipped
The engineers in your team are currently developing applications using CloudRun. It is necessary to set up on CloudRun using the name of the configuration that created the container.
Where is the name of the configuration that created the container stored?
•	 
K_SERVICE environment variable
•	 
K_Configuration environment variable
(Correct)
•	 
K_REVISION environment variable
•	 
CloudRun metadata
Explanation
Option 2 is the correct answer. Name information such as containers in the Cloud Run configuration is stored in the K_Configuration environment variable. When Cloud Run starts the container, K_Configuration, K_Revision, K_Service, and Port are created as environment variables. In K_Configuration, specify the configuration in which the container was created. If you use a Docker container, use the CMD command in your Dockerfile configuration to specify the command to run at startup.
Option 1 is incorrect. Contains the name of the Cloud Run service that is running.
Option 3 is incorrect. Contains the name of the Cloud Run revision that is running.
Option 4 is incorrect. There is no configuration file called CloudRun metadata.
【reference】
https: //cloud.google.com/run/docs/reference/container-contract
Question 76: 
Skipped
You are using Compute Engine to start multiple VMs and need VM management.
Which of the following is an efficient setting to get data about CPU load from all instances every 5 minutes?
•	 
Monitor the VM's load_5m metric on the default monitoring screen of the operations suite.
•	 
Install the Monitoring agent on the VM to monitor the load_5m metrics.
(Correct)
•	 
Enable monitoring of the operations suite to monitor the load_5m metrics of the VM.
•	 
Install a monitoring script using the operation suite on the VM to monitor the load_5m metric.
Explanation
Option 2 is the correct answer. The Monitoring agent collects system and application metrics from virtual machine instances and sends the collected information to Cloud Monitoring. By default, the Monitoring agent collects disk, CPU, network, and process metrics. To monitor third-party applications. You need to configure the Monitoring agent for the server to get the agent metrics.
When you configure the Monitoring agent, the load_5m metric allows you to retrieve data about the CPU load from your instance every 5 minutes.
Question 77: 
Skipped
It has become necessary to optimize the Google Cloud cost of the company, and the team that manages the operation of Google Cloud was asked to analyze the cost. Which databases and storage can you use to export your Google Cloud billing data?
•	 
BigQuery and DataStore
•	 
DataStore and Bigtable
•	 
Cloud Storage and BigQuery
(Correct)
•	 
Cloud Storage and Bigtable
Explanation
Option 3 is the correct answer. You can export your billing data to a Cloud Storage bucket in CSV or JSON format. You can then use these files to see your billing details. You can also export your billing data to BigQuery for detailed analysis and visualization of your billing.

Options 1 and 2 are incorrect. DataStore cannot be specified as the destination for billing data exports.
Option 4 is incorrect. DataStore cannot be specified as the destination for billing data exports.
【reference】
https://cloud.google.com/billing/docs/how-to/export-data-bigquery
https://cloud.google.com/billing/docs/how-to/export-data-bigquery
Question 78: 
Skipped
You are configuring a relational database using the new Cloud SQL as the data layer of the web application and you want to back up your Cloud SQL database automatically.
Which parameter setting enables automatic backup?
•	 
Backup type selection
•	 
Backup destination settings
•	 
Backup lifecycle rule settings
•	 
Backup start time
(Correct)
Explanation
Option 4 is the correct answer. The only setting required to enable automatic backup is the backup start time. Follow the steps below to set up automatic backup in Google Cloud Console:
1. Go to Cloud SQL Instances
2. Click the instance name to open the instance overview page.
3. Select Backup from the SQL navigation menu.
4. Click Edit next to Settings.
5. Select a time frame for automatically backing up your data.
6. Click Save. 

Option 1 is incorrect. CloudSQL automatically creates a snapshot as a backup type. It does not need to be specified by the user.
Option 2 is incorrect. In CloudSQL, the backup destination is automatically set on the Google Cloud side as a managed service. Therefore, it does not need to be specified by the user.
Option 3 is incorrect. In CloudSQL, there are no settings such as backup life cycle rules.
【reference】 
https://cloud.google.com/sql/docs/mysql/backup-recovery/backing-up#disabling_automated_backups
Question 79: 
Skipped
Your company's Google Cloud needs cost optimization. So, as a Google Cloud administrator, you're organizing the unnecessary resources for removal.
Which command would use to delete the instance group template?
•	 
glcoud compute instance-templates remove
•	 
glcoud compute remove instance-templates
•	 
gcloud compute delete instance-templates
•	 
glcoud compute instance-templates delete
(Correct)
Explanation
Option 4 is the correct answer. You can delete the specified instance template by running the command `gcloud compute instance-templates delete [instance template name]`.
Options 1, 2 and 3 are not valid commands.
【reference】
https://cloud.google.com/sdk/gcloud/reference/compute/instance-templates/delete
Question 80: 
Skipped
You took a snapshot of a persistent disk with 10GB in use a few hours ago. An additional 5GB of data has been added to this persistent disk. Then you took the snapshot again. These snapshots are stored in a dedicated bucket and are costly depending on the amount of data.
What is the amount of data in the snapshot you added?
•	 
10GB
•	 
15GB
•	 
5GB
(Correct)
•	 
20GB
Explanation
Option 3 is the correct answer. Persistent disk snapshots only save increments from previous snapshots. Therefore, if you add another 5GB of data to the persistent disk and then take the snapshot again, only the additional 5GB will be taken as a snapshot.
The Compute Engine uses delta snapshots, so each snapshot contains only the data that has changed since the last snapshot. Deleting a snapshot does not delete all the data on that snapshot, because later snapshots may require the information stored in the previous snapshot.

【reference】
https://cloud.google.com/compute/docs/disks/create-snapshots
Question 81: 
Skipped
You want to use managed services to minimize operational management overhead. When using a managed service such as CloudSQL,
Which management items are managed by Google Cloud?
•	 
OS settings
•	 
Applying security patches
•	 
Infrastructure management
•	 
all of the above
(Correct)
Explanation
Option 4 is the correct answer. The managed service provides the main operation management items such as infrastructure, OS and software patch management on the Google Cloud side.
Managed services have limited user access, but can leave a lot of administrative responsibility and work to the vendor.
Question 82: 
Skipped
You are preparing to develop a new application using Google Cloud. In this configuration, you will use two VPCs with multiple subnets.
How many CIDRs do you need to define?
•	 
Set one CIDR for each subnet.
(Correct)
•	 
Set one CIDR for each region.
•	 
Set one CIDR for each VPC.
•	 
You can set one or more CIDRs for each zone.
Explanation
Option 1 is the correct answer. CIDR must be configured one for each subnet. This allows you to specify the number of IP addresses assigned within the subnet and the subnet range. In Google Cloud, your VPC is globally configured and has subnets within the region. Therefore, you don't need to assign an IP address range to your VPC, you only assign an IP address range to your subnet.
Option 2 is incorrect. You need to set CIDR for each subnet, not for each region. Regions are not networks, they are geographical areas and are not subject to CIDR. 
Option 3 is incorrect. You don't have to assign an IP address range to your VPC, just a subnet.
Option 4 is incorrect. You need to set CIDR for each subnet, not for each zone. Zones are a collection of data centers, not networks, and are not subject to CIDR.
【reference】
https://cloud.google.com/vpc/docs/alias-ip
Question 83: 
Skipped
You are managing instances on Compute Engine. Because some images have been deprecated, it is necessary to notify the user that the corresponding image will be deprecated.
What is the appropriate method for this?
•	 
Stop the image.
•	 
Delete the image so that it cannot be viewed.
•	 
Mark deprecated images
(Correct)
•	 
Increase the image version.
Explanation
Option 3 is the correct answer. If you set the status of the image to "DEPRECATED", you can still continue to use the image to create VMs, but you will get a warning. The purpose is to notify the user that the image is obsolete, not to delete the image itself. Because of this, you need to identify that the image will be obsolete while still making it available.
Option 1 is incorrect. Prohibiting access to an image is not appropriate as it will suddenly prevent users from creating VMs using this image. This does not serve the purpose of notifying.
Option 2 is incorrect. Deleting an image is not appropriate as it will suddenly prevent users from creating VMs with this image. This does not serve the purpose of notifying.
Option 4 is incorrect. Increasing the version of the image does not notify the user that the image is obsolete.
【reference】
https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images
Question 84: 
Skipped
Your company uses App Engine Standard to develop applications and you need to configure your application.
Which of the following elements is incorrect as a component of my application?
•	 
application
•	 
service
•	 
version
•	 
instance
•	 
project
(Correct)
Explanation
The option 5 project is the wrong component. An App Engine application has at least one service and can hold multiple versions depending on the billing status of the app. Each service consists of multiple versions, these versions are running on multiple instances.

Therefore, options 1, 2, 3 and 4 are the correct components.

【reference】
https://cloud.google.com/appengine/docs/standard/python3/an-overview-of-app-engine
Question 85: 
Skipped
You are developing an AI application by embedding a machine learning model on a VM instance and you have set the GPU for very high performance, but it seems that the GPU is not being used.
What is the cause?
•	 
GPU is not activated.
•	 
Low disk space available.
•	 
GPU drivers are not installed.
(Correct)
•	 
The instance may be down.
Explanation
Option 3 is the correct answer. In order to use the GPU in the VM, you need to install the GPU driver in the VM. For NVIDIA GPUs running Compute Engine, you need to use the following NVIDIA driver versions:
[For A100 GPU]
Linux: 450.80.02 or later
Windows: 452.77 or later

[For all other GPU types]
Linux: NVIDIA 410.79 or later
Windows: 426.00 or later driver

Option 1 is incorrect. If you set the GPU to connect to the VM, you do not need to activate it separately.
Option 2 is incorrect. GPU usage and disk space are irrelevant.
Option 4 is incorrect. Since learning can be performed without using the GPU, it is considered that the instance itself is running.
【reference】 
https://cloud.google.com/compute/docs/gpus#restrictions
https://cloud.google.com/compute/docs/gpus/install-drivers-gpu#minimum-driver
Question 86: 
Skipped
You are configuring a VPN to connect your office to Google Cloud.
Which section of the cloud console do you need to use to configure your VPN?
•	 
Create a VPN from the "VPN" item in the "VPC section"
•	 
Create a VPN from the "VPN" item in the "Google Cloud Connection Management Section"
•	 
Create a VPN from the "VPN" item in "Hybrid Connection"
(Correct)
•	 
Create a VPN from the "VPN" item in the "Subnet section"
Explanation
Option 3 is the correct answer. You can create a VPN from the "VPN" item in "Hybrid Connections" on the console. Here, there are three hybrid connection methods, Cloud Interconnect, Cloud VPN, and peering with Google, which include Dedicated Interconnect and Partner Interconnect, Cloud VPN when the required volume is low, and direct peering and carrier peering, respectively.
Cloud VPN is used when there are few data connections. Cloud VPN connects your on-premises or other public cloud networks to Google VPC securely and at low cost over the Internet via IPSec VPN. It addresses the needs of data bandwidth up to 3.0 Gbps. Cloud VPN's flexible routing options allow you to connect to different VPN gateways using static or dynamic routing. Highly available VPNs offer industry-leading SLAs and guarantee 99.99% uptime.
【reference】  https://cloud.google.com/hybrid-connectivity
Question 87: 
Skipped
At what level in the resource hierarchy can you create a Shared VPC?
•	 
folders and resources
•	 
organization and project
•	 
Organizations and Folders
(Correct)
•	 
folders and subnets
Explanation
Option 3 is the correct answer.
Shared VPCs can be created at the organization or folder level of the resource hierarchy.

Option 1 and option 2 are incorrect. Shared VPCs are not created at the resource or project level.

Option 4 is incorrect. Shared VPC does not apply to subnets that are resources in resource hierarchies.
Question 88: 
Skipped
You would like to implement inter-project communication between VPCs.
What features of VPC can be used to implement this?
•	 
VPC peering
(Correct)
•	 
Inter-project peering
•	 
VPN
•	 
interconnection
Explanation
Option 1 is the correct answer.
VPC peering is used for inter-project communication.
Option 2 is incorrect. There is no feature named Cross-Project Peering.

Options 3 and 4 are incorrect. They have nothing to do with linking your on-premises network to GCP's network.
Question 89: 
Skipped
Which parameter is used to make a DNS zone private?
•	 
--private
•	 
--visibility=private
(Correct)
•	 
--private=true
•	 
--status=private
Explanation
Option 2 is the correct answer.
The visibility parameter is a parameter that can be set privately. So option 2 is correct.

Option 1 is not a valid parameter.
Option 3 is incorrect. private is not a parameter.
Similarly, option 4 is also incorrect. status is not a valid parameter for making a DNS zone private.
Question 90: 
Skipped
Which load balancer achieves global load balancing?
•	 
HTTP(S) only
•	 
SSL proxy and TCP proxy only
•	 
HTTP(S), SSL proxy, and TCP proxy
(Correct)
•	 
Internal TCP/UDP, HTTP(S), SSL Proxy, and TCP Proxy
Explanation
Option 3 is the correct answer.
You can use HTTP(S), SSL proxy, and TCP proxy as global load balancers. So option 3 is the correct answer.

Option 1 and option 2 are incorrect. It does not contain one or more available global load balancers.

Also option 4 is incorrect. Internal TCP/UDP is a regional load balancer.
Question 91: 
Skipped
Which regional load balancer can load balance based on IP protocol, address, and port?
•	 
HTTP(S)
•	 
SSL proxy
•	 
TCP proxy
•	 
Network TCP/UDP
(Correct)
Explanation
Option 4 is the correct answer.
Network TCP/UDP allows load balancing based on IP protocol, address, and port. Option 4 is the correct answer.

Other options are all global load balancers. Not a regional load balancer.
Question 92: 
Skipped
You want to use Cloud Marketplace to deploy my WordPress site. You have noticed that you have multiple WordPress options. What is the reason?
•	 
File a ticket with Google Support as it's an error
•	 
Multiple vendors may provide the same application
(Correct)
•	 
Submit ticket to vendor for mistake
•	 
Options never look like this
Explanation
Option 2 is the correct answer.
Option 2 is the correct answer, as multiple vendors may offer the same application configuration. This allows users to choose the configuration that best suits their requirements.

Options 1 and 3 are incorrect. It's a feature of Cloud Marketplace. Option 4 is incorrect.
Question 93: 
Skipped
You used Cloud Marketplace to deploy my WordPress site. Now you want to deploy the database. You've noticed that the database configuration form is different than the one used in WordPress. What is the reason?
•	 
File a ticket with Google Support as it's an error
•	 
Opened another subform in Cloud Marketplace
•	 
Configuration properties are based on the application you are deploying, so they vary depending on the application you are deploying
(Correct)
•	 
this does not happen
Explanation
Option 3 is the correct answer.
Option 3 is the correct answer, as Cloud Marketpiace presents configuration options appropriate for the application you are deploying. For example, when deploying WordPress, you'll see an option to deploy PHP's management tools.

Option 1 is incorrect. This is a feature of Cloud Marketpiace.

Option 2 is incorrect. You are not necessarily using the wrong form.

Option 4 is incorrect. This is a feature of Cloud Marketplace.
Question 94: 
Skipped
You would like to define the configuration of a set of application resources as code.
What is the name of a GCP service that creates resources using a configuration file consisting of resource specifications defined in YAML syntax?
•	 
Compute Engine
•	 
Deployment manager
(Correct)
•	 
Marketplace Manager
•	 
Marketplace Deployer
Explanation
Option 2 is the correct answer.
Option 2 is correct because Deployment Manager is the name of the service for creating application resources using the YAML configuration file.

Option 1 is incorrect. However, you can use scripts with gcloud commands to deploy resources to Compute Engine.

Options 3 and 4 are incorrect. These are fictitious product names.
Question 95: 
Skipped
You would like to print the list of roles assigned to users in the project ace-exam-project.
Which gcloud command should you use?
•	 
gcloud iam get-iam-policy ace-exam-project
•	 
gcloud projects list ace-exam-project
•	 
gcloud projects get-iam-policy ace-exam-project
(Correct)
•	 
gcloud iam list ace-exam-project
Explanation
Option 3 is the correct answer.
Option 1 is incorrect. Resources are projects, not iam.

Option 2 is incorrect. detailed descriptions are not printed in list.

Option 4 is incorrect. iam and list are incorrect.
Question 96: 
Skipped
You are working with the form that appears after clicking the ADD link on the IAM form in IAM & admin in the Cloud Console. There is a parameter called New Members.
Which item do you enter for this parameter?
•	 
individual users only
•	 
individual user or group
(Correct)
•	 
role or individual user
•	 
role or group
Explanation
Option 2 is the correct answer.
New Members is the email address of the user or group.

Option 1 is incorrect. Contains no groups.

Options 3 and 4 are incorrect. No roles are added.
Question 97: 
Skipped
You have been assigned the App Engine Deployer role. Which operations can you perform?
•	 
Only write new versions of applications
•	 
Read only application configuration and settings
•	 
Read application configuration and settings and write new configurations
•	 
Read application configuration and settings and write new versions
(Correct)
Explanation
Option 4 is the correct answer.
Deployers can read application configurations and settings and create new application versions.

Option 1 is incorrect. No reading of configuration and settings.

Option 2 is incorrect. No new version written.

Option 3 is incorrect. You have a new configuration to write.
Question 98: 
Skipped
What is called the transformation of an indicator, sent over a period of time, into a new time series with data points at fixed intervals?
•	 
totalling
•	 
alignment
(Correct)
•	 
minimize
•	 
binding
Explanation
Option 2 is the correct answer.
Alignment is the process of separating data points into standard buckets, so option 2 is the correct answer.

Option 1 is incorrect. Aggregation is used to combine data points using statistical information such as mean values.

Options 3 and 4 are incorrect. These are not processes related to the metric data processing stream.
Question 99: 
Skipped
Now that we've created a condition for CPU usage, we want to receive notifications. Which of the following options are available?
•	 
email only
•	 
Pager Duty only
•	 
Hipchat and PagerDuty
•	 
Email, PagerDuty, Hipchat
(Correct)
Explanation
Option 4 is the correct answer.
All three options are valid notification channels for Stackdriver Monitoring, so option 4 is the correct answer. PagerDuty and HipChat are common development and operations tools.
Question 100: 
Skipped
What is Alert Fatigue? why is that a problem?
•	 
Too many alarm notifications are sent for events that don't require human intervention, and operations engineers begin to pay less attention to notifications
(Correct)
•	 
Too many alerts put unnecessary load on the system
•	 
Too few alerts leave operations engineers unaware of application and infrastructure health
•	 
Too many false positive alerts
Explanation
Option 1 is the correct answer.
Alert pass-through is a condition in which too many alert notifications are sent for events that do not require human intervention. So option 1 is the correct answer. This ultimately creates the risk that the DevOps engineers will start to pay less attention to the notifications.

Option 2 is incorrect. It is possible, but unlikely, that too many alerts can have a negative impact on performance.

Option 3 is also a potential problem, but not alert fatigue.

Option 4 is incorrect. Too many really meaningful alerts don't cause alert fatigue.
==


Question 1: 
Skipped
You company migrated its application to Google Cloud last year and this application has collected more that 50TB of data in Cloud Storage bucket. Now company wants to use this data for some kind of analysis to improve their operations. This analysis include both batch processing and stream processing.
Which solution approach will you suggest to your team in this case?
•	 
Use one Managed Instance Group for batch processing.
Use 2nd Managed Instance Group for Stream processing.
Store processed data in BigQuery.
Run BigQuery job for analysis.
•	 
Use one Managed Instance Group for batch processing.
Use Cloud Function for Stream processing.
Store processed data in BigQuery.
Run BigQuery job for analysis.
•	 
Use Cloud Dataflow for both batch processing and stream processing of data.
(Correct)
•	 
Use Dataproc cluster for both batch processing and stream processing of data.
Explanation
Cloud Dataflow is a fully-managed service that supports both stream data processing and batch data processing with equal reliability and expressiveness
It offers following features
> Fully managed data processing service
> Automated provisioning and management of processing resources
> Horizontal autoscaling of worker resources to maximize resource utilization
> OSS community-driven innovation with Apache Beam SDK
> Reliable and consistent exactly-once processing
Please read the following references for more information
DataFlow
Question 2: Incorrect
Your company is using a preemptible Compute Engine VM instance in Google Cloud for its application. Your team wants to perform some clean up activity for each VM instance which gets preempted by Google Cloud.
What solution will you suggest to your team for this use case?
•	 
Write a shell shutdown script.
Register this script as a xinetd service in Linux
Configure a StackDriver endpoint check to call the service.
(Incorrect)
•	 
Write a shell shutdown script.
Register this script as a xinetd service in Linux
Add-metadata for VM instance with the key shutdown-script-url and value as the service URL
•	 
Add a shell script named k11.shutdown in the /etc/rc.d/ directory.
•	 
Create a shutdown script.
While creating a new VM instance, use this script as the value for a new metadata entry with the key shutdown-script in the Cloud Platform Console
(Correct)
Explanation
You can specify Shutdown script as metadata on Cloud Console while creating a new VM instance.
 

Shutdown scripts is used to execute commands just before an instance is terminated or restarted, on a best-effort basis. This is useful if you rely on automated scripts to start up and shut down instances, allowing instances time to clean up or perform tasks, such as exporting logs, or syncing with other systems
Please read the following references for more information
Shutdown Script
Question 3: 
Skipped
Your company has deployed a microservice based application on Compute Engine VM instances in Google Cloud. Occasionally, some of these microservices becomes slow which causes other dependent microservice to fail and add many logging lines in logs.
You are generally not able to know when does this slowness occur. You need to debug your VM when this microservice slowness problem occur
What solution can you suggest for this use case?
•	 
Look for a expected log lines in Stackdriver Error Reporting dashboard after problem occur.
•	 
Configure a custom log metric in Stackdriver Logging
Create an alert to notify you when the number of log lines for above created metric cross a defined threshold during some time interval.
(Correct)
•	 
Deploy application on GKE cluster and use Service Mesh to debug faulty microservice.
•	 
Deploy application in App Engine in Flexible environment to avoid this problem.
App engine will autoscale based on application traffic.
Explanation
Since you know that there is a burst of log lines you can set up a metric that identifies those lines. Stackdriver will also allow you to set up a text, email or messaging alert that can notify promptly when the error is detected so you can check the system for debugging.

Please read the following references for more information
Log Based Metrics
Question 4: 
Skipped
How can you deploy an application using Compute Engine VM instance in Google Cloud so that your application can sustain zonal failure?
Choose all correct answers.
•	 
Deploy application using Managed Instance Group.
Do not use multizone option for Managed Instance Group
•	 
Deploy application using Managed Instance Group.
Use multizone option for Managed Instance Group which is spread over 2 zones.
Overprovision your VM instance by 100%.
(Correct)
•	 
Deploy application using autoscaled Unmanaged Instance Group.
Use multizone option for Managed Instance Group which is spread over 2 zones.
•	 
Deploy application using Regional Managed Instance Group which is spread over 3 zones..
Overprovision your VM instance by 50%.
(Correct)
Explanation
Option B is correct if one zone fails you still have 100% desired capacity in another zone.
Option D is correct because you have at least total 150% of desired capacity spread over 3 zones with each zone having 50% capacity. This way even after failure of one zone, You'll have 100% of desired capacity in two zones.
Question 5: 
Skipped
Your company wants to process and store very high volume of time series data coming from various IOT devices. This data volume can be of many Petabytes. You need to read and write this data in almost real-time.
Which Google Cloud service will you suggest for this use case?
•	 
Google Cloud Storage
•	 
BigQuery
•	 
Cloud Spanner
•	 
Cloud Bigtable
(Correct)
•	 
Cloud SQL
Explanation
Bigtable is ideal for storing very large amounts of data in a key-value store and supports high read and write throughput at low latency for fast access to large amounts of data.

Cloud BigQuery is not a good solution for fast changing data.
Cloud Storage is designed for object storage. It is not recommended for this type of data ingestion and collection.
Cloud SQL can not handle Petabyte scale of data

Please read the following references for more information
Cloud Bigtable
Question 6: 
Skipped
Your company has developed a data processing application using Apache Spark. Now, your team expects a sudden increase in the input workload for this application. So to meet this requirement they want to deploy this application in Google Cloud.
Which Google Cloud service will you recommend to your team for this use case?
•	 
Cloud Dataflow
•	 
GKE cluster
•	 
Cloud Dataproc
(Correct)
•	 
App Engine
Explanation
Dataproc is a fully managed and highly scalable service for running Apache Spark, Apache Flink, Presto, and 30+ open source tools and frameworks. Use Dataproc for data lake modernization, ETL, and secure data science, at planet scale, fully integrated with Google Cloud, at a fraction of the cost.
Please read the following references for more information
Cloud Dataproc
Question 7: 
Skipped
Your team has deployed a latest version of application on Google App Engine. Just after deploying this new version, customer started reporting complaints about slowness in application. This behavior was not observed with previous version?
What quick solution can you suggest to your team to fix it?
•	 
It is just a network speed. So contact ISP to fix this slowness issue.
•	 
Switch to previous version by using Migrate Traffic option in App Engine
Find the root cause of this issue with the help of Cloud Logging and Cloud Trace in lower environment like testing or staging.
(Correct)
•	 
Switch to previous version by using Migrate Traffic option in App Engine.
Deploy the latest version once again when there is less traffic for your application.
Find the root cause of this issue with the help of Cloud Logging and Cloud Trace in production environment.
•	 
Deploy application in GKE cluster
Explanation
Since there was no issue with previous version, so it make sense to rollback application immediately to previous version, so that customers are not impacted by this latest deployment.
After this, find the root cause of this issue with the help of Cloud Logging and Cloud Trace in lower environment like testing or staging.

Please read the following references for more information
Traffic Migration in App Engine
Question 8: 
Skipped
There is a VPN connection between your company's on-premises data center and Google Cloud, and your company has very high speed internet connection. Your networking team wants to increase VPN connection speed.
What solution will you suggest to your networking team in this case?
•	 
Configure another VPN tunnel between on-premises data center and Google Cloud
(Correct)
•	 
Configure Dedicated Interconnect between on-premises data center and Google Cloud
•	 
Submit a request to Google Cloud support team to increase VPN speed limit.
•	 
Use high speed routing in existing VPN
Explanation
Each VPN tunnel has a max speed of 3 Gbps, and you can create multiple VPN tunnels to increase bandwidth.

Please read the following references for more information
VPN Tunnel Network Bandwidth
Question 9: 
Skipped
Your team wants to store audit logs for log term users and they want to give read only access to external auditors for this log data.
As per best practices of giving least privilege access, what solutions will you suggest for your team? Choose 2 correct choices.
•	 
Configure Cloud Storage Bucket as export sink for audit logs
(Correct)
•	 
Configure BigQuery  as export sink for audit logs
•	 
Create a  group for all auditors
Give view access to this group for Cloud Logging.
•	 
Create a signed URL to the Cloud export destination for auditors to access.
(Correct)
Explanation
There are 3 type of export destinations for Cloud Logs to:
> Cloud Storage
> Cloud Pub/Sub
> BigQuery.
BigQuery can be used as sink for exporting the logs but it is not cost effective for long term storage. Better solution will be to export the logs to Cloud Storage Bucket and then use BigQuery to run any analysis job on this data. BigQuery can read data stored in Cloud Storage Bucket.

You can give read only access either by creating a Group for auditor and giving permissions via ACL to access object for this group or you can control access via signed URL's.
Since your team wants to give read only access for external auditors, so signed URL is better choice

Please read the following references for more information
Cloud Logging Export Destinations
Question 10: 
Skipped
You have successfully launched a compute Engine VM instance successfully in Google Cloud and deployed production application on this VM. But you are not able to connect to this instance via SSH connection.
How can you debug this issue without affecting the production application traffic? Select all correct choices.
•	 
Use Startup Script to collect information in Cloud Logging
•	 
Create snapshot of existing VM.
Use this snapshot to create another VM.
(Correct)
•	 
Use netcat to try to connect to port 22
(Correct)
•	 
Check the serial console output.
(Correct)
Explanation
Please read the following reference to find the possible common failure scenario for SSH connection and how to fix those.
Troubleshooting SSH
Question 11: 
Skipped
Your company wants to move their development environment VMs to GCP. These VMs need to stopped and started multiple times in a day. So developers need to persist the state of the VM.
What solution will you suggest for your team to migrate their development environment to GCP? You also need to provide cost visibility to finance team.
Choose 2 correct answers.
•	 
Use persistent disk with VM instance.
Take snapshot of VM before stopping it.
Use this snapshot again while restarting VM instance.
•	 
Create VM instance in GCP that use persistent disk with --no-auto-delete flag
(Correct)
•	 
Use Google BigQuery Billing Export and Labels to associate cost to groups.
(Correct)
•	 
Store the state of VM in Cloud Storage Bucket before terminating VM instance
•	 
Store the state of VM in Cloud Datastore before terminating VM instance
Explanation
If you use Persistent disk with --no-auto-delete flag with a VM, then persistence disk will not be deleted on VM termination.
This way disk contents are saved between start and stop. When the instance in stop status, you are only got charged for very low-cost disk storage

Billing export to BigQuery allows you to save your billing information automatically in a BigQuery dataset. You can then access your billing data from BigQuery. Labels will appear as columns in BigQuery dataset.

Please read the following references for more information
https://cloud.google.com/sdk/gcloud/reference/compute/instances/set-disk-auto-delete
Question 12: 
Skipped
Your company wants to move their applications from on-premises data center to GCP. Their security team wants detailed visibility of all the projects in the organization. You have configure yourself as the org admin in IAM.
Which IAM roles should be given to the security team?
•	 
Org viewer, project owner
•	 
Org admin, project browser
•	 
Org viewer, project viewer
(Correct)
•	 
Project owner, network admin
Explanation
Option C gives only view only permission to security team for all the resources in the organization.
There is no role with name Project Browser. So Option B is incorrect.
Other roles gives write access as well to cloud resource so Option A and Option D are incorrect.
Question 13: 
Skipped
Your team has deployed an application Google App Engine. Now your team wants to test new feature of application with in production environment for limited number of customers. Other customers should continue to use same version.
What solution will you suggest to your team for this use case?
•	 
Deploy the new application version temporarily when application has least traffic
Rollback this new version after some time.
•	 
Create another App Engine Application in 2nd project.
Deploy application in 2nd project's App Engine.
•	 
Configure new service in App Engine Application.
Forward subset of customer's request to this new service.
•	 
Deploy a new version of the application
Use traffic splitting to send some portion of traffic to this new version.
(Correct)
Explanation
In App engine multiple versions of same application can be deployed simultaneously and we can easily control how much and what traffic should go to which version by using traffic Splitting feature of App Engine.
Question 14: 
Skipped
Your company has developed a business critical application that is replicated across two zones in GCP using autoscaled managed instance groups on all layers, except the database. Currently, this application meets 99.99% availability for small number of its customers. However in next quarter due to festival season, your company is expecting a very high load on this application.
What testing strategy will you suggest in this case to ensure the system maintains the SLA during festival season as well?
•	 
Capture existing users input and replay captured user load multiple times.
Increase group's size each day until autoscale logic is triggered on all layers.
At the same time, terminate random resources on both zones.
•	 
Create customized random user input.
Replay customized load until autoscale logic is triggered on at least one layer
At the same time, terminate random resources on both zones.
(Correct)
•	 
Capture existing users input and replay captured user load until autoscale is triggered on all layers.
At the same time, terminate random resources on both zones.
•	 
Capture existing users input and replay captured user load until resource utilization crosses 70%.
Determine estimated number of users based on existing users' usage of the app.
Deploy enough resources in Cloud to handle 150% of expected load.
Explanation
Ideally test environment should be similar to expected production environment. Only in case of Option B, we are trying to simulate this by generating customized random input data for enough number of user that can trigger autoscaling in system.
In this Option B, We are also testing, how system will behave in case some of the instances go down due to any reason.
Question 15: 
Skipped
Your company wants to do penetration security scanning for their test and development environments in GCP. This security scanning should be done from an end user perspective.
How this penetration testing can be conducted in GCP?
•	 
Ask Google to perform penetration security scanning on behalf of your company.
•	 
Use the on-premises scanners to conduct penetration testing on the cloud environments by routing traffic over the public internet.
(Correct)
•	 
Use the on-premises scanners to conduct penetration testing on the cloud environments by routing traffic over the VPN.
•	 
Deploy the security scanners into the cloud environments and conduct penetration testing within each environment.
Explanation
Since we need to do this security scanning from end user perspective, so traffic coming via public internet will best simulate end user traffic. So it make complete sense to have scanners outside GCP to simulate real world traffic for the application
Question 16: 
Skipped
How can you prepare a backup/rollback plan for your application that is distributed across a large managed instance group while deploying new version of the application?
•	 
Use the Rolling Update feature to deploy/roll back versions with different managed instance group templates.
(Correct)
•	 
Create a regular snapshot for all VM instances by using snapshot scheduling.
•	 
Take regular back of critical data in all VM instances in Cloud Storage bucket.
•	 
Configure a scheduler that will trigger a Cloud Function to take snapshot of disk of all VM instances.
Explanation
With Rolling Update feature of Managed Instance Group, we can use different instance template to deploy new version of application.

Please read the following references for more information
Rolling Update in Managed Instance Group
Question 17: 
Skipped
Your cab fleet management company has has equipped sensors  in its cab to collet telemetry data. Next year they want to use the data to train machine learning models. Your team is exploring options to store this data in Cloud with minimum cost.
What solution in Google Cloud will your suggest to your team?
•	 
Compress the data in hourly basis
Store the compressed data snapshot in GCS Nearline Bucket
•	 
Compress the data in hourly basis
Store the compressed data snapshot in GCS Coldline Bucket
(Correct)
•	 
Push the streamed data from sensors in real time to Cloud Pub/Sub
Create a Dataflow job to read this streamed data and store it in BigQuery
•	 
Push the streamed data from sensors in real time to Cloud Pub/Sub
Create a Dataproc job to read this streamed data and store it in Bigtable
Explanation
Coldline GCS bucket is cheapest option to store infrequently used data that will be needed after one year.

Please read the following references for more information
Cloud Storage Classes
Question 18: 
Skipped
Your team wants to use GCS bucket as backup option for AWS S3 Bucket data. So they want to sync data from S3 Bucket to GCS Bucket.
How can it be achieved?
•	 
Use gsutil cp command at regular interval to copy data from S3 Bucket to GCS Bucket
•	 
Use gsutil rsync command to sync data from S3 Bucket to GCS Bucket
(Correct)
•	 
Use gsutil -m cp command to sync data from S3 Bucket to GCS Bucket
•	 
Use Storage Transfer Service to keep both the source and destination in sync.
Explanation
Option B & Option D, both look right choice but I will prefer Option B here to sync data continuously.
For one time Copy Storage Transfer Service is more preferable.

Please read the following references for more information
gsutil rsync command
Storage Transfer Service
Question 19: 
Skipped
Which Google Cloud storage solution can be used to migrate petabyte scale of data store in SAN (Storage Area Network)?
•	 
GCS Bucket
•	 
Persistent Disk
(Correct)
•	 
BigTable
•	 
BigQuery
Explanation
Both SAN and Persistent disk use block storage to store data. So Persistent disk (Option B) is best choice here.
Question 20: 
Skipped
During Covid Period, Your team needs to develop a web application for its client. Their client want to collect donation via this website. Customer expects a very high traffic for this website. So your team needs a solution that can support very high write capability.
What solution will you suggest to your team in this case?
•	 
Use Cloud SQL with high CPU and Memory capacity
•	 
Use Memcache as write through cache to store the write operations until write operations are committed in Cloud SQL
•	 
Use Pub/Sub to hold write operations
Read task from Pub/Sub topic and commit write in database.
(Correct)
•	 
Install MySQL on Compute Engine Instance managed via autoscaling Managed Instance Group.
Explanation
Cloud Pub/Sub is highly available and highly scalable system that can support very high speed write operations.  So Option B is correct choice here.
Otherwise MySql database will reach to its own vertical scaling limit in all other options.
Question 21: 
Skipped
Which data storage will you recommend to store relational data in Google Cloud form anywhere in the world and how will you configure scaling for this service in case of traffic increase?
•	 
Use Cloud Spanner to store relational data from anywhere in the world.
Increase number of nodes if 75% of storage space is occupied.
•	 
Use Cloud Spanner to store relational data from anywhere in the world.
Increase number of nodes if CPU utilization goes beyond 75% for certain specified period.
(Correct)
•	 
Use Bigtable to store relational data from anywhere in the world.
Increase number of nodes if 75% of storage space is occupied.
•	 
Use Bigtable to store relational data from anywhere in the world.
Increase number of nodes if CPU utilization goes beyond 75% for certain specified period.
Explanation
Cloud Spanner is a fully managed, mission-critical, relational database service that offers transactional consistency at global scale, schemas, SQL (ANSI 2011 with extensions), and automatic, synchronous replication for high availability.
And you can configure the autoscaling of Cloud Spanner nodes count based on the CPU utilization.

Please read the following references for more information
Cloud Spanner
Cloud Spanner Documentation
Autoscaling Cloud Spanner

Question 22: 
Skipped
Your team wants to host payment gateway application in GCP using GKE cluster. Team wants to know if GKE cluster is suitable for hosting their application. They need to ensure that their solution is PCI DSS compliant.
What should you do?
•	 
All service in GCP are PCI DSS compliant. So it is safe to host such payment gateway in GCP.
•	 
In addition to GCP and GKE, which are PCI DSS complaint, team must also ensure that their application is compliant with regulations.
(Correct)
•	 
Team must deploy application on Compute Engine Instance using single tenant host.
•	 
Team should use APP Engine to host their application because APP engine is Google managed and PCI compliant service.
Explanation
GKE service is PCI compliant.
Google Cloud undergoes at least an annual third-party audit to certify individual products against the PCI DSS. This means that these services provide an infrastructure upon which customers may build their own services or applications which store, process, or transmit cardholder data.
It is important to note that customers are still responsible for ensuring that their applications are PCI DSS compliant.

Please read the following references for more information
PCI Compliance in GCP
Question 23: 
Skipped
Your team has configured a CI/CD pipeline for automated deployment. This pipeline is triggered whenever a code is merged into master branch of Git repository. Latest release of critical financial application deployed by your team has caused an outage. So your manager wants that release artifact must be verified before production deployment.
What solution will you suggest to your team?
•	 
Enable Blue/Green deployment for existing CI/CD pipeline so that it is easy to rollback in case of failure.
•	 
Deploy application on App Engine.
Use traffic splitting feature of App Engine to forward 5% of traffic to new version of application.
•	 
Configure the CI/CD solution to monitor tags in the repository.
Deploy non-production tags to staging environments.
Once testing is done, create production tags and deploy them to the production environment.
(Correct)
•	 
Deploy new version with current CI/CD pipeline.
Test the application with live traffic.
Deploy the older version if test case fails.
Explanation
Release artifact must be tested in non production environment before doing deployment in production. Option C fulfill this requirement by doing deployment automatically based on repository tags.
Question 24: 
Skipped
Your team has added new VMs in existing Unmanaged Instance Group to speed up the nightly batch process. But nearly half of the newly added VMs failed to run next batch processing task scheduled in night.
Your manager want to collect details for this failure. Which three actions should you take?
•	 
Check in Activity log, if failure occurred due to live migration event.
(Correct)
•	 
Check module logs in Cloud Logging
•	 
Check serial console logs of failed VM instances.
(Correct)
•	 
Create image of one of the failed VM instance.
Run a new VM on local server with this image.
Check the logs on your local server.
•	 
Adjust the Cloud Logging timeline to match the failure time and observe the failed server metrics.
(Correct)
Explanation
Since failure happened with newly added VMs, so it is possible that this failure might have happened due to live Migration Event. VM might have stopped responding due to this live migration

A virtual machine (VM) instance has four virtual serial ports. The instance's operating system, BIOS, and other system-level entities often write output to the serial ports, which makes serial port output useful for troubleshooting crashes, failed boots, startup issues, or shutdown issues.

You should also check Cloud Logging logs for the same time when these VM failure happened. You might get some hint there, related to this failure.

Please read the following references for more information
View Serial Port Output
Question 25: 
Skipped
What all requirements you need to fulfill to do penetration testing in GCP?
•	 
GCP does not allow to do penetration testing.
•	 
Follow the Google Cloud Platform Acceptable Use Policy and Terms of Service.
You also need to ensure that penetration testing only affect your projects.
No need to contact or notify Google for this.
(Correct)
•	 
Fill the request form for this and submit it to GCP Support team for approval.
•	 
Report vulnerabilities found during this testing to GCP Support team.
Explanation
Please read the following references for more information
Google Cloud Security FAQ
Question 26: 
Skipped
Your team developed an application that needs to store objects in GCS bucket. What method will you suggest to your team for this as per Google Cloud recommended practices?
•	 
Assign Storage Object Creator role to each user of the application .
•	 
Assign Storage Object Creator role to each team member.
•	 
Create a service account for the application and grant it the Storage Object Creator role.
(Correct)
•	 
Set up a new Account in  Cloud IAM and grant it the Storage Object Creator role.
Explanation
Please read the following references for more information
Service Account in GCP
Question 27: 
Skipped
Your company is using Cloud Logging service to store logs from all of its applications. Your team needs to provide access to these logs to company's auditors without compromising any data security so that auditors can perform quick data analysis in streamlined manner.
What solution will you suggest to your team to share logs with auditors?
•	 
Create a Cloud Function that will read logs from Cloud Logging and store them in Cloud SQL.
Create a new IAM user and grant it the required permissions to execute read only queries for Cloud SQL data.
Ask auditors to use this new account to read logs information from Cloud SQL
•	 
Export Cloud Logging data to BigQuery.
In BigQuery, create views for required data and restrict them to the audit team with ACLs
(Correct)
•	 
Export Cloud Logging data to GCS Bucket
Set up IAM rules to enable the audit team access to read log files in the bucket.
•	 
Export Cloud Logging data to Bigtable
Set up IAM rules to enable the audit team access to read log data from Bigtable.
Explanation
Both GCS Bucket and BigQuery can be used as Sink for Cloud Logging logs data. But I will prefer Option B (BigQuery) more because it is easy to do analysis in BigQuery by using SQL like queries.

By using BigQuery ACLs and views, you can set required access levels for auditors.

Please read the following references for more information
BigQuery Table Access Control
BigQuery View
Question 28: 
Skipped
Your team has developed a very critical application and they planned for no single point of failure while deploying application in GCP.
Still team face application outage due to database activity. Team was using Cloud SQL with replication in Google Cloud to store its critical data. But the replica was not promoted to primary.
What suggestion can you give to your team to avoid such issues in future?
•	 
Create more than 1 DB replicas in other regions.
•	 
Do regular failover activity in planned manner to ensure that failover is working as expected.
(Correct)
•	 
Increase CPU capacity of Cloud SQL instance.
•	 
Take regular data backup and store this in GCS bucket.
Explanation
This is a good practice to review your DR strategy at regular interval and make required changes in this strategy as per the observations.
Question 29: 
Skipped
Your company's security team has configured IAM policies at various levels like VMs, projects, folders and organization. What is the effective policy that applies for a particular VM in this case?
•	 
The effective policy for the VM is the policy assigned directly to the VM.
•	 
The effective policy for the VM is the policy assigned directly to the VM and restricted by the policies of its parent resource.
•	 
The effective policy for the VM is a union of the policy assigned to the VM and the policies it inherits from its parent resource.
(Correct)
•	 
The effective policy for the VM is an intersection of the policy assigned to the VM and the policies it inherits from its parent resource
Explanation
IAM policies can be set at different levels of the resource hierarchy. Resources inherit the policies from its ancestors. The effective policy for a resource is the union of the policy set at that resource and the policy inherited from its ancestors.

Please read the following references for more information
IAM Resource Hierarchy Control
Question 30: 
Skipped
Which Google Cloud service will you suggest to your team to store over 500 TB or a petabyte of NoSQL data for a workload that need high-speed transactions and analytics.
•	 
Cloud Datastore
•	 
Cloud Bigtable
(Correct)
•	 
Cloud Spanner
•	 
Cloud BigQuery
Explanation
Bigtable is a scalable, fully-managed NoSQL wide-column database that is suitable for both real-time access and analytics workloads. Bigtable is ideal for very large NoSQL datasets and is useful for high-speed transactions and analysis. It integrates well with ML. Dataproc, and analytics. So Option B is correct choice in this case.

BigQuery does not support high speed transactions.
Datastore can not support Petabyte scale of data.
Question 31: 
Skipped
Your team has set it's monthly budget for Google Cloud Project. Your project manager wants to be informed automatically of the project spend so that corrective action can be taken on time to control the cost.
What solution will you suggest for this use case?
•	 
Link a credit card with payment profile with a monthly limit equal to your budget.
•	 
Set up budget alerts in billing section in Google Cloud Console for 50%, 90%, and 100% of the total monthly budget.
(Correct)
•	 
Create a custom metric in Cloud Monitoring to keep a check on project cost and configure notification using this metric to send notification via email.
•	 
Configure billing export to BigQuery.
Write a Cloud Function which will query billing data from BigQuery at regular interval and send notification via email in case specified threshold limit of total cost is breached.
Explanation
You can avoid surprises on your bill by creating Cloud Billing budgets to monitor all of your Google Cloud charges in one place. A budget enables you to track your actual Google Cloud spend against your planned spend. After you've set a budget amount, you set budget alert threshold rules that are used to trigger email notifications. Budget alert emails help you stay informed about how your spend is tracking against your budget. You can also use budgets to automate cost control responses.

Please read the following references for more information
Budget And Alerts
Question 32: 
Skipped
Your network team observed that they can SSH into a VM instance from another instance in the same VPC by using internal IP address, but they are not able to do SSH by using external IP address of VM instances.
What could be the reason for this?
•	 
Service account do not have required permissions to do SSH via external IP.
•	 
Nat Gateway is not configured properly in VPC.
•	 
Firewall allows SSH traffic only via internal IP.
(Correct)
•	 
Internet Gateway is not able to reach to VM external IP address because custom subnet route is not defined.
Explanation
Firewall rules in VPC control which traffic is allowed or denied in VPC.

Please read the following references for more information
Firewall Rules
Question 33: 
Skipped
You development team want to store all server events to GCS Bucket for some analysis.
The event records size can vary from 100 KB to 20 MB and are expected to peak at 3,000 events per second.
How can you minimize data loss. Which process should you implement?
•	 
Append metadata to file body.
Compress individual files.
Name files with a random prefix pattern.
Save files to one bucket
(Correct)
•	 
Batch every 3000 events with a single manifest file for metadata.
Compress event files and manifest file into a single archive file.
Name files using serverName-EventSequence pattern.
Create a new bucket if bucket is older than 1 day and save the single archive file to the new bucket. Otherwise, save the single archive file to existing bucket.
•	 
Compress individual files.
Name files with serverName-EventSequence pattern.
Save files to one bucket
Set custom metadata headers for each object after saving.
•	 
Append metadata to file body.
Compress individual files.
Name files with serverName-Timestamp pattern.
Create a new bucket if bucket is older than 1 hour and save individual files to the new bucket. Otherwise, save files to existing bucket
Explanation
Avoid using sequential object names such as timestamp-based object names if you are uploading many objects in parallel. Objects with sequential names are stored consecutively, so they are likely to hit the same backend server. When this happens, throughput is constrained. In order to achieve optimal throughput, add the hash of the sequence number as part of the object name to make it non-sequential
Compressing and combining smaller files info fewer larger files is also a best practice for speeding up transfer speeds because it saves network bandwidth and space in Google Cloud Storage.
Please read the following references for more information
GCS best practices
Question 34: 
Skipped
Your team has created a GKE cluster with one node pool named as 'main-node-pool'. Now your team wants to increase number of nodes in that node pool.
Which command will be used to increase node pool size?
•	 
gcloud container clusters resize mycluster --node-pool 'main-node-pool' --num-nodes 30
(Correct)
•	 
gcloud container clusters resize mycluster --node-pool 'main-node-pool' --size 30
•	 
Node pool size is fixed.
To increase node pool size, create a new GKE cluster with bigger node pool.
•	 
gcloud container clusters update mycluster --node-pool 'main-node-pool' --num-nodes 30
Explanation
Please read the following references for more information
Container Cluster Resize
Question 35: 
Skipped
Your team needs to store very large number of small files to GCS Bucket from an on-premises data center. There is fast network available between on-premises data center and Google Cloud.
How can you speed up the transfer of your files?
•	 
Use the -r option for large transfers
•	 
Combine multiple small files into one big file and then save this big file
•	 
Compress and combine files before transferring.
Use the -m option for multi-threading file transfers.
(Correct)
•	 
Use Dedicated Interconnect between on-premises data center and Google Cloud
Explanation
gsutil -m option allows you to upload files using multithreading

Compressing and combining smaller files info fewer larger files is also a best practice for speeding up transfer speeds because it saves network bandwidth and space in Google Cloud Storage

Please read the following references for more information
gsutil Copy Command (Read about -z and -m options on this page)
Question 36: 
Skipped
Your team wants to minimize the production deployments rollback for a web application. Almost 70% improvement is achieved just by fixing their Test case and adding automated test cases.
How can you improve this further so that there are minimum rollback of production deployment? Choose 2 answers.
•	 
Use Blue/Green Deployment
(Correct)
•	 
Use microservice based architecture in place of monolithic web application
(Correct)
•	 
Use NoSql database rather than SQL database
•	 
Do deployment using canary release.
Explanation
With Blue green deployment, good quality of testing can be done easily as both environments are similar to each other.
With microservice based approach, it is easy to release small functionality to production rather than using one big release in case of monolithic application. It will reduce possibility of error.
Question 37: 
Skipped
What is the best practice for separating responsibilities and access for production and development environments?
•	 
Separate project for each environment, each team only has access to their project.
(Correct)
•	 
Separate project for each environment, both teams have access to both projects
•	 
Both environments use the same project, but different VPC's.
•	 
Both environments use the same project, just note which resources are in use by which group.
Explanation
As best practices:
• Separate account must be used for each environment which is associated with different group of users. You should use project to isolate user access to resource, and not to manage users.
• Shared VPC allows each team to individually manage their own application resources, while enabling each application to communicate between each other securely over RFC1918 address space.
VPC is used for securing communication and not for resource isolation. Project must be used for resource isolation.
Question 38: 
Skipped
Your company wants to migrate several production applications to Google Cloud. These are 50+ departments in our company and each department has its own IAM requirements.
As per Google's recommendation, What solution will you suggest to your team to set up required IAM controls? Make sure that it is easy to manage these IAM rules.
•	 
Create one organization for each department.
Create multiple folders under each organization.
•	 
Create single organization.
Create separate folder for each department in this organization.
(Correct)
•	 
Create one organization for each department.
•	 
Create single organization.
Create separate project for each department in this organization.
Explanation
 
This is how organization structure should look like.

Please read the following references for more information
Resource Hierarchy
Question 39: 
Skipped
Your company is online gaming company. Your customer base include both free user and paid user. Free use base is quite large It is nearly to 20 million free users. To increase its revenue, your team wants to convert free users into paid users. For this your team needs to send some promotional message to all free users.
You team is not sure about click-through rate for such promotional mail. It could vary between 500 click to 1 million click in a day.
What solution will you suggest for your team in this case?
•	 
Store user information in Cloud SQL
Use Google Compute Engine VM instance to host web application.
•	 
Use Cloud Datastore to store user data
Use App Engine to host web application
(Correct)
•	 
Store user information in Cloud SQL
Use Cloud Function to host Web Application
•	 
Use Cloud Datastore to store user data
Use Cloud Function to host Web Application
Explanation
Cloud Datastore is more appropriate to store User Data. Datastore is a highly scalable NoSQL database for your applications. Datastore automatically handles sharding and replication, providing you with a highly available and durable database that scales automatically to handle your applications load. Datastore provides a myriad of capabilities such as ACID transactions, SQL-like queries, indexes, and much more.

Also single VM instance will not be able to handle very high click through rate. In case of high traffic we need autoscaling solution. So App Engine is most appropriate choice among all given options.

Cloud Function is good for event driven processing rather than hosting web application.

Please read the following references for more information
Datastore
App Engine
Question 40: 
Skipped
Your company has recently migrated first set of its on-premises Java based web applications to GCP using lift and shift approach by using Google Compute Engine VM instance. But using this approach caused them lots of operation overhead.
Which Google Cloud services will you recommend to your team for such migration for next set of Java based web application? Choose 2 answers.
•	 
Use Cloud function
•	 
Use App Engine Standard Environment
(Correct)
•	 
Use App Engine Flexible Environment
•	 
Use GKE Cluster
(Correct)
•	 
Use Compute Engine VM instances managed by Managed Instance Group.
Explanation
App Engine allows you to build highly scalable applications on a fully managed serverless platform.

Both MIG and GKE cluster looks good choice for this case but GKE cluster is better choice because it requires less overhead and do autoscaling in more efficient way.

Please read the following references for more information
App Engine
GKE Cluster
Question 41: 
Skipped
Your team want to migrate Petabyte scale of financial data to Google Cloud. This data will be used for analysis by business team who is expert is writing SQL queries.
Which Google Cloud solution will you suggest to your team to store this kind of data so that it is easy for business team to run analysis queries?
•	 
BigQuery
(Correct)
•	 
Cloud SQL
•	 
Cloud Spanner
•	 
Cloud Storage
•	 
Cloud Datastore
Explanation
BigQuery is a serverless, cost-effective and multicloud data warehouse designed to help you turn big data into valuable business insights.

Cloud SQL can not handle Petabyte scale of data.
BigQuery is more efficient for analysis for very large dataset than Cloud Spanner or Bigtable.
Cloud Datastore is No SQL database.
So BigQuery (Option A) is best option among all these choices
Question 42: 
Skipped
Your company developed an application to process various kind of data from sensors which are installed in taxis across entire country. This data will be used for car maintenance later on.
Which Google Cloud solution will you suggest to your team to store this kind of information?
•	 
CSV files in Cloud Storage Bucket
•	 
Cloud SQL
•	 
NAS
•	 
Cloud Datastore
(Correct)
Explanation
Since there is no fixed schema for such kind of data and it is coming at very high speed, So Cloud Datastore (Option D) will be best choice among all given options
Question 43: 
Skipped
An electricity distribution company is a customer for your company. Your customer has various regional centers from where various financial reporting data is collected in customer's head quarter. Reporting data collected from different regions might have some discrepancies. Your customer has asked for help to resolve these discrepancies.
How can you help your customer using Google Cloud services?
•	 
Store all reporting data in GCS bucket
Set up a Cloud dataflow pipeline to process and clean all the reporting data
•	 
Use Cloud Dataprep to clean all data stored in on-premises data center
•	 
Store all reporting data in GCS bucket
Use Cloud Dataprep to clean data stored in GCS bucket
(Correct)
•	 
Store all reporting data in GCS bucket
Set up a Cloud Dataproc pipeline to process and clean all the reporting data
Explanation
Cloud Dataprep is an intelligent cloud data service to visually explore, clean, and prepare data for analysis and machine learning.

Please read the following references for more information
Cloud Dataprep
Question 44: 
Skipped
Your network team wants to set up a connection between on-premises data center and Google Cloud. They want routes to get automatically updated whenever there is a network topology change.
What is the minimal configuration you need to implement?
•	 
2 Cloud VPN Gateways and 1 Peer Gateway
•	 
Cloud VPN Gateway, 1 Peer Gateway, and 1 Cloud Router
(Correct)
•	 
2 Peer Gateways and 1 Cloud Router
•	 
2 Cloud VPN Gateways and 1 Cloud Router
Explanation
We can achieve this requirement as shown below
 

Please read the following references for more information
VPN Network Topologies
Question 45: 
Skipped
Your team wants to deploy web application in Google Cloud using Compute Engine VM instances. Team wants to secure its internal backend servers from public access.
What will you suggest to your team to reduce their external exposure? You also need to ensure that team is able to access resources for maintenance purpose
•	 
Assign temporary public IP only during maintenance.
Remove the external IP address after maintenance activity.
•	 
Remove the external IP address.
Use a bastion host to access internal-only resources
(Correct)
•	 
Remove the external IP address
Use VPN connection for maintenance work.
•	 
Hide the external IP address behind a load balancer
Access the resources via Load Balancer IP
Explanation
If no public IP is assigned to resource, then resource can not be access by public over internet.
Bastion hosts provide an external facing point of entry into a network containing private network instances

 

Please read the following references for more information
Connecting VM via Bastion Host
Question 46: 
Skipped
Your team has hosted its e-gifting application on GKE cluster. Due to festival in next week, team expects traffic increase for this application. So your team lead wants that GKE cluster node pool should scale on demand but with maximum limit of 15 nodes.
How should you configure the cluster?
•	 
Use command:
gcloud container clusters resize <cluster identifier> --size 15.
•	 
Use command:
gcloud container clusters update <cluster identifier> --enable-autoscaling --min-nodes=1 --max-nodes=15.
(Correct)
•	 
Attach tags to the VM instance to enable autoscaling and set max nodes to 15 by running:
gcloud compute instances add-tags <instance name> --tags enable-autoscaling max-nodes-15.
•	 
Delete the existing cluster and create new GKE cluster by running:
gcloud container clusters create <cluster identifier> --enable-autoscaling --min-nodes=1 --max-nodes=15

Deploy application on this new cluster.
Explanation
Please read the following references to learn about various command options to update GKE cluster
GKE Cluster Update Command
Question 47: 
Skipped
Your company has developed a financial application and hosted this application in GCP using Compute Engine Instances. Now your team wants to add a new feature in this application. But they want to test this feature with new customers first in production before rolling it out for all customers.
How can this be achieved in Google Cloud with same DNS and SSL certificates?
•	 
Use two load balancer for two applications versions.
Same DNS record will be used for both load balancer
•	 
Use URL based routing provided by Load Balancer in GCP to route the traffic from new customers to latest version and for other customer use current version
(Correct)
•	 
Move application to GKE cluster and use different node pool for different versions.
•	 
Replace the old version with latest version when there is least traffic on application so that least number of existing customers are impacted
Explanation
In this case, you can configure 2 backed services for different version of application and route the traffic to these backend services based on URL Map

Please read the following references for more information
URL Map Concepts
Question 48: 
Skipped
Your team is using Postgres database hosted in Computed Engine VM instance in Google Cloud. Your DBA observed that this VM will run out of disk space very soon.
How can this problem be solved in Google Cloud?
•	 
Stop the VM instance
Increase the size of SSD persistent disk.
Restart the VM
•	 
Create snapshot of existing disk.
Create a new VM instance that use this snapshot.
Restart the database
•	 
Increase the size of SSD persistent disk by using command : resize2fs
(Correct)
•	 
Attach additional persistent disk of larger size to VM instance.
Move database files on this disk.
Explanation
You can increase the size of your persistent disk when your virtual machine (VM) instance requires additional storage space. You can only resize a persistent disk to increase its size. You cannot reduce the size of a persistent disk.
You can resize disks at any time, whether or not the disk is attached to a running VM.
To increase size of existing disk, first increase the disk size and then resize the file system or partition.

Please read the following references for more information
How to resize persistent disk
Question 49: 
Skipped
Your team has deployed its web application on GKE cluster. Now team wants to change machine type from n1-standard-1 to n1-standard-4 for this cluster.
How can it be done?
•	 
Create a new node pool with new machine type.
Migrate traffic to new node pool
(Correct)
•	 
Use command
gcloud container clusters update mycluster --machine-type n1-standard-4
•	 
Use command
gcloud container clusters resize mycluster --machine-type n1-standard-4
•	 
Create a new GKE cluster with new machine type
Explanation
Node pool machine type can not be changed after creation in GKE cluster.
So correct option is to create a new node pool in the same cluster and migrate the workload to the new pool.

Please read the following references for more information
Migrating workloads to different machine types
Question 50: 
Skipped
Your team has developed a Python application and want to run this application in a sandboxed managed environment with the ability to scale up in seconds in case of traffic increase for this application.
Which Google Cloud service will you suggest to your team for such deployment?
•	 
App Engine Standard Environment
(Correct)
•	 
App Engine Flexible Environment
•	 
Kubernetes Engine
•	 
Compute Engine
•	 
Cloud Function
Explanation
App Engine Standard Environment supports Python.
It can scale down to zero instance in case of no activity
It can scale up in seconds to meet application traffic demand.

Please read the following references for more information
App Engine Environments





Question 1: Correct
Your team wants to deploy you application servers in GCP. But database will resides in on premise data center. So your team needs to setup a hybrid networking.
What suggestion will you give to your team to set up VPC in GCP?
•	 
Setup VPC in a way that both Primary and Secondary IP ranges of the VPC do not overlap with the on-premises VLAN.
(Correct)
•	 
Setup the VPC with same IP range as on-premises VLAN.
•	 
Set up the Secondary IP range of the VPC in GCP to use the same Secondary IP range as on-premises VLAN
Make sure than Primary IP range of VPC do not overlap with On-Premises Primary IP range.
•	 
Set up the Primary IP range of the VPC in GCP to use the same IP range as on-premises VLAN
Make sure than Secondary IP range of VPC do not overlap with On-Premises Secondary IP range.
Explanation
In two connected network, IP address range should not overlap. Otherwise it is not possible to find the device for the given IP address.
So Option A, is correct choice here.
Question 2: 
Skipped
Your team has created an autoscaling instance group to serve web traffic for an application. This instance group is used as backend service for HTTP(S) load balancer.
But you noticed that this deployment configuration is not working as expected. All virtual machine (VM) instances in instance group are being terminated and re-launched every minute. Public IP address is not assigned to any instance. You are getting correct response from each instance using the curl command.
How can you fix above problem?
•	 
Make sure that a firewall rule exists to allow source traffic on HTTP(S) to reach the load balancer.
•	 
Assign a public IP to each instance.
Add a firewall rule in VPC to allow HTTP(s) traffic from load balancer to instance public IP.
•	 
Ensure that a firewall rule exists to allow load balancer health checks to reach the instances in the instance group.
(Correct)
•	 
Create a network tag for the load balancer.
Create another network tag for VM instances.
Configure a firewall rule to allow HTTP(S) traffic with above created load balancer network tag as the source and the instance network tag as the destination.
Explanation
Option C is correct here. Since instances are getting terminated and relaunched, it means load balancer is considering instance to be unhealthy. It could be because load balancer is not able to reach to compute engine instance.
So to fix this, we must ensure that a firewall rule exists to allow load balancer health checks to reach the instances in the instance group.
Question 3: 
Skipped
Your company want to scale its Apache spark and Hadoop jobs, which are currently hosted in on-premises data center. Because of limited capacity in on-premises data center, your team is exploring various Cloud options.
What GCP offering can you suggest to your team in such case? Make sure this solution must be cost effective and should not require much overhead or re writing of an existing code.
•	 
Use GKE cluster and set up your Hadoop cluster there.
•	 
Set up your own Hadoop cluster using Google Managed Instances.
Manage these instances via Managed Instance Group.
•	 
Set up your own Hadoop cluster using Google Managed Instances.
Manage these instances via Unmanaged Instance Group.
•	 
Configure Cloud Dataflow.
•	 
Use Cloud Dataproc to configure Hadoop cluster in Google Cloud.
(Correct)
Explanation
Dataproc is a fully managed and highly scalable service for running Apache Spark, Apache Flink, Presto, and 30+ open source tools and frameworks. You can use Dataproc for data lake modernization, ETL, and secure data science, at planet scale, fully integrated with Google Cloud, at a fraction of the cost.
Please read the following references for more information
Cloud Dataproc
Question 4: 
Skipped
Your team wants to migrate existing microservices based application to Google Cloud using Compute Engine Instances. Team is not aware about how to aggregate logs from all the microservices in Google Cloud. Currently team was using some custom solution for log aggregation in on-premises data center but this custom utility can not be used in Google Cloud.
What solution will you suggest to your team in this case?
•	 
Install Istio service mesh on Compute Engine Instances.
•	 
Write new custom solution for log aggregation that can run in GCP.
•	 
Install Cloud Logging agent on each Compute Engine Instance via Startup Script.
Use Cloud Logging service to manage logs collected by these logging agents.
(Correct)
•	 
Install Cloud logging agent on each Compute Engine Instance via SSH logging in to VM.
Use Cloud logging service to manage logs collected by these logging agents.
Explanation
Cloud Logging allows you to store, search, analyze, monitor, and alert on logging data and events from Google Cloud and Amazon Web Services.
Logging agents can be easily installed on each VM automatically via Startup Script.

Please read the following references for more information
Cloud Logging
Question 5: 
Skipped
Your company is running primary MySQL instance in on-premises data center. Team has also configured Cloud SQL instance as failover instance for on-premises database instance. Sometimes operation team noticed that replication time between on-premises database and Cloud SQL database is more that defined SLA. It is mainly due to network issue between on-premises data center and Google Cloud.
What solution will you suggest to solve this issue?
•	 
Use Cloud Function to synchronize both databases
•	 
Setup Dataflow job to replicate the data from on-premises database to Cloud SQL instance
•	 
Set up a Dedicated Interconnect between on-premises data center and Google Cloud.
Also configure multiple VPN connections between on-premises data center and Google Cloud.
VPN connections should be used in case Dedicated Interconnect is not working.
(Correct)
•	 
Increase the bandwidth of existing connection between on-premises data center and Google Cloud.
Explanation
Since issue is primarily because of occasional network problem, so it's better to have backup up VPN network configured between on-premises data center and Google Cloud.
This VPN network will be used if Dedicated Interconnect is not working for any reason.
Question 6: 
Skipped
Your company wants to migrate its existing microservices based application to Google Cloud. For this they need to setup CI/CD pipeline in GCP. Your security team follows very high standard security practices and do no want to compromise this on Cloud as well.
By considering right security practices, what solution will you suggest to your team to set up CI/CD pipeline in GCP? Select all correct answers.
•	 
Use Jenkin to manage CI/CD in Google Cloud.
•	 
Use signed binaries from private repositories for CI/CD pipeline.
•	 
Ask team to do manual security check of source code before doing any commit.
•	 
Configure static source code security scan with in CI/CD pipeline. Stop the pipeline execution if security scan check fails.
(Correct)
•	 
Configure security vulnerability scan and deploy changes only if there is no issue in this scan.
(Correct)
Explanation
Source code security scan and security vulnerability scan can help in finding any security loophole in application. So it must be configured with CI/CD pipeline.
Question 7: 
Skipped
Your team wants to migrated one of its critical financial application to Google Cloud. This application is used internally with in the organization. Your security team will not allow to store users sensitive information like login credentials in Google Cloud.
In such case, what solution will you suggest so that users of this application can log in to this application after migration to Google Cloud. 
•	 
Convince team to store passwords in KMS(Key management service)
•	 
Create Cloud IAM user in cloud for each application user.
Assign required permissions to each member individually to invoke application APIs.
•	 
Create Cloud IAM user in cloud for each application user.
Add all users in a single Group.
Assign required permissions to group to invoke application APIs.
•	 
Use Google Cloud Directory Sync (GCDS) to create users in Google Cloud.
•	 
Use federated authentication by using SAML to authenticate users from on-premises identity provider.
(Correct)
Explanation
Since security team does not allow to store users data in Google Cloud so, Option E is the only correct choice here. SAML is industry standard protocol for federated authentication from external Identity Provider.
Question 8: 
Skipped
Your team need to build a chat application. This chat application needs to be deployed in Google Cloud.
As a Google Cloud architect, what solution will you suggest to your team to ensure security of communication happening via this chat application?
•	 
Add User Id in message header to ensure that message is coming from right legitimate user only.
•	 
Encrypt all customer's messages using customer managed private key.
•	 
Use two way asymmetric key encryption to encrypt all messages from both side of communication.
(Correct)
•	 
Use HTTPS protocol for communication.
Explanation
HTTP(S) is stateless protocol so it is generally not preferred for chat application.
User Id information in message header can be seen by an intruder.
One side message encryption is not sufficient to secure whole communication.
So only correct choice here is Option C. In this case both side of communication is encrypted.
Question 9: 
Skipped
You company has deployed a latest version of an application in App Engine Standard Environment. But just after deployment customers reported major issue due to high latency.
How can you help your team in this case?
•	 
Rollback to previous version.
Use latest version only during non peak hours.
•	 
Switch to GKE and deploy latest version in GKE cluster
•	 
Configure autoscaling for latest version in App Engine Standard Environment
•	 
Rollback to previous version.
Deploy latest version in non-production environment.
Check application logs in Cloud logging to find the root cause of the issue.
(Correct)
Explanation
With App Engine you can easily switch to previous version of application by using the traffic migration feature .
Cloud logging is a managed service that allow you to securely store, search, analyze, and alert on all of your log data and events.

Please read the following references for more information
Traffic Migration in App Engine
Question 10: 
Skipped
You company want to use BigQuery in one project in GCP as its data warehousing solution so that business user can run analytical queries. But underlying data for analytics is spread over multiple projects.
What role should be given to business user so that user can execute analytical query on BigQuery dataset but user should not be able to do any update operation on any underlying data?
•	 
Add all users to a group.
Grant the group, the role of BigQuery user on the billing project and BigQuery dataViewer on the projects that contain the data.
•	 
Add all users to a group.
Grant the group, the roles of BigQuery dataViewer on the billing project and BigQuery user on the projects that contain the data.
•	 
Add all users to a group.
Grant the group, the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the projects that contain the data.
(Correct)
•	 
Add all users to a group.
Grant the group the roles of BigQuery dataViewer on the billing project and BigQuery jobUser on the projects that contain the data.
Explanation
To execute query on BigQuery dataset user need BigQuery JobUser/user role. Both these roles gives permission to make a query on BigQuery dataset
But since actual underlying data is spread across different projects, so user also need data BigQuery dataViewer on the projects that contain the data.
So Option C is the correct answer here.
Please read the following references for more information
Shared Access via BigQuery data viewer role
Big Query Access Control
Question 11: 
Skipped
Your team has hosted a web application to serve static content on Google Compute Engine using Managed Instance Group. Your team wants to use Cloud CDN to reduce latency for the customer.
What suggestion can you give to your team to improvise the cache hit ratio?
•	 
Do customization in cache key to remove the protocol information
(Correct)
•	 
Use very short expiration duration
•	 
Add header HTTP(S) header (Cache-Region) in request that points to the closest region for the application customer.
•	 
Serve static content from Cloud Storage bucket. Use Global Load Balancer with Cloud Storage Bucket as its backend service.
Explanation
By using short expiration duration for key will decrease the cache hit ratio
Cloud CDN does not handle header Cache-Region. So there is no point in using any such header.
Cloud storage bucket is slower than CDN in serving content
So only Option A is correct choice here. This both Http and Https will use same key.
By default, Cloud CDN uses the complete request URL to build the cache key. For performance and scalability, it’s important to optimize cache hit ratio. To help optimize your cache hit ratio, you can use custom cache keys .
 
Please read the following references for more information
Using custom cache keys to improve cache hit ratio
Question 12: 
Skipped
Your company has deployed an application in Compute Engine instances via Managed Instance Group. Operation team observed latency in scaling up of this instance group. Team found that this latency is because of time taken up by startup script to install all dependencies in any new instance.
What suggestion can you give to your team to shorten the instance start time so that scaling can happen quickly?
•	 
Create a new Instance Template that use VM instance with more compute and memory capacity
Create a new Managed Instance Group manually from Cloud Console that use this instance template
•	 
Create a new Instance Template that use VM instance with more compute and memory capacity
Create a new Managed Instance Group manually via Deployment Manager that use this instance template
•	 
Create a docker container image
Deploy application in App Engine Flexible Environment
•	 
Create a Boot Disk Image of VM with all dependencies preinstalled.
Create Instance Template to use this new Image to launch a Compute Engine Instance
Create a new Managed Instance Group that use this Instance Template.
(Correct)
Explanation
In Option A, Option B and Option C, time taken to install dependencies at the time of VM start will not reduce.
Only by create VM Boot Disk Image, time taken to install dependencies at the time of VM start can be reduced. Because in this case, all the dependencies are install and configured in Disk Image. So Option D is correct choice here.
Question 13: 
Skipped
Your team has developed an application that need to store some critical documents in Cloud Storage Regional Bucket. Application support team observed errors in application with HTTP status codes of 429 and 5xx.
What suggestion can you give to your team in this case?
•	 
Use Dual Region Cloud Storage Bucket to store the Data
•	 
Retry the operation for these errors with exponential backoff.
(Correct)
•	 
Configure Health Check for Cloud Storage Bucket end point.
Use application only if Health check is in healthy state.
•	 
Use Cloud CDN on top of Cloud Storage Bucket.
Explanation
HTTP status code of 429 means too many request.
HTTP status code of 5xx means internal server error on Cloud Storage Service.
So in both case it make sense to retry the operation again with exponential backoff. So Option B is correct choice here.

Cloud CDN can just improve the read queries. So Option D is not the correct choice here.
Question 14: 
Skipped
Your team wants to deploy an application in GKE cluster in GCP. This application needs to authenticate users with an Active Directory which is hosted in on-premises data center.
What solution can you suggest to your team which requires minimal effort?
•	 
Authenticate users against the Active Directory domain controller with the help of Admin Directory API.
•	 
Synchronize Active Directory usernames with cloud identities using Google Cloud Directory Sync.
Use SAML for Single Sign On.
(Correct)
•	 
Connect to on-premises Active Directory domain controller as an identity provider by using Cloud Identity-Aware Proxy
•	 
Create Replica of Active Directory (AD) domain controller in Compute Engine.
Synchronize on-premises Active Directory domain controller using  Google Cloud Directory Syn
Explanation
Since on-premises Active Directory is the source of truth, so we can use this by synchronizing it with Google Cloud Identity.
For this synchronization, we can use Google Cloud Directory Sync that allow us to synchronize users, groups, and other data from an Active Directory/LDAP service.

Please read the following references for more information
GCDS Introduction
Deciding where to deploy GCDS
Question 15: 
Skipped
Your team wants to migrate on-premises business critical application to GCP. This application use Microsoft SQL Server to store its data. Since it is a business critical application, so there should be no downtime in case of zonal failure in Google Cloud.
What solution will you suggest to your team in this case?
•	 
Use regional Cloud SQL instance with high availability enabled.
(Correct)
•	 
Use Cloud Spanner instance with a regional instance configuration.
•	 
Use Compute Engines in different zones to host Set up SQL Server.
Configure Always On Availability Groups using Windows Failover Clustering.
•	 
Set up SQL Server Always On Availability Groups using Windows Failover Clustering.
Place nodes in different subnets.
Explanation
Earlier SQL Server was not supported in Cloud SQL. In that case Option C (Use Compute Engines in different zones to host Set up SQL Server.) was correct.
Please read the following references for this.
Architecting Microsoft SQL Server with GCP


But with latest support of SQL Server as a managed service in Cloud Option A is more appropriate choice.
Please read the following references for this.
Overview of the high availability configuration (google.com)
Question 16: 
Skipped
Your application support team wants to centralized collection of all admin activity and VM system logs within your project.
What solution will you suggest to collect these logs from both VMs and services?
•	 
No need to do anything. Both Admin activity and VM system logs are automatically collected in Cloud Logging.
•	 
For most of the services, Admin activity logs are automatically collected by Cloud Logging.
To collect VM system logs, install Cloud Logging agent on VM instances.
(Correct)
•	 
Write a custom log collector and send all logs to Cloud Pub/Sub topic.
Subscribe to that topic and store all logs in Cloud Storage Bucket.
•	 
Create another VM.
Install Cloud Logging agent on this VM.
Forward all logs from all VM to this new VM.
Explanation
Admin Activity audit logs are always written. You can't configure, exclude, or disable them. Even if you disable the Cloud Logging API, Admin Activity audit logs are still generated.
But VM system logs are not automatically collected. To collect such logs from each VM, Cloud Logging agent must be installed on each VM.

Please read the following references for more information
Admin Activity
Cloud Logging Agent Installation
Question 17: 
Skipped
Your team wants to migrate an application from on-premises to Google Cloud. Team wants to host this application in 2 regions to provide high availability against the regional failure in GCP. All the application servers are behind External HTTP(S) load balancer.
But this application needs to access on premises database. For this networking team wants to set up VPN connection between on-premises data center and Google Cloud.
What solution will you suggest to networking team in this case?
•	 
Host application in different VPC for region.
Establish VPC peering between on-premises data center and both VPC in two regions.
•	 
Create a VPC which span all regions in Google Cloud.
Host application in this VPC in 2 different regions.
Establish VPC peering between on-premises data center and VPC in Google Cloud
•	 
Configure one Global Cloud VPN Gateway.
Establish at least on VPN tunnel between each region and on-premises data center.
•	 
Configure one Cloud VPN Gateway in each region.
Establish at least on VPN tunnel between each region and on-premises data center.
(Correct)
Explanation
VPC peering is used to establish connectivity between 2 VPC in Google Cloud only. It is not used to establish connectivity with any network outside GCP.

Google cloud does not provide any VPN gateway which can be used by multiple regions.
GCP provide 2 types of VPN Gateway
1. HA VPN
2. Classic VPN
Both these VPN Gateways are regional gateways.
So Option D is correct choice here.

Please read the following references for more information
Cloud VPN

Question 18: 
Skipped
How can you deploy a latest version of an application on GKE cluster with very minimal downtime?
•	 
Use rolling update feature of Managed Instance Group created for GKE cluster.
•	 
Delete previous service and created a new service.
•	 
Create new Deployment by creating new deployment.yaml file.
Delete the previous deployment.
Update existing service to use new deployment.
•	 
Update the image of an existing deployment.
(Correct)
Explanation
kubectl set image <deployment> <New Deployment Image>
This command can be used to update an existing deployment in GKE cluster.

Please read the following references for more information
Application Update in GKE cluster
Question 19: 
Skipped
Your networking team needs to configure firewall rules in a VPC so that any application hosted in this VPC can connect to MYSql database hosted in on-premises data center.
What firewall rules should networking team configure for this use case?
•	 
Add an egress rule with priority 1000 to allow outbound traffic for MySql server from all instances in VPC.
Use implied deny egress rule to block all other outbound traffic.
•	 
Add an egress rule with priority 1000 to allow outbound traffic for MySql server from all instances in VPC.
Set priority of implied deny egress rule to 1500 to block all other traffic.
•	 
Add an egress rule with priority 1000 to allow outbound traffic for MySql server from all instances in VPC.
Add an egress rule with priority 1500 to block all other outbound traffic.
(Correct)
•	 
Add an egress rule with priority 1500 to allow outbound traffic for MySql server from all instances in VPC.
Add an egress rule with priority 1000 to block all other traffic.
Explanation
In VPC firewall rules, lower priority rules are applied first. It means rule with priority order 1000 has more preference that rule with priority order of 1500.
And by default there are two implied rules
1. Allow all egress traffic
2. Deny all incoming traffic
Also implied rules have lowest priority of 65535, and it can not be changed by user.
As per this explanation, Option C is the correct choice.

Please read the following references for more information
VPC Firewall
Question 20: 
Skipped
Your team has deployed a business critical application on a single Compute Engine instance. But during peak traffic customers complaint about latency.
What suggestion will you give to your team to fix this issue?
•	 
Add more Compute Engine VM instances during peak hours on daily basis
•	 
1. Create a disk snapshot of running VM instance.
2. Create an Image from this snapshot.
3. Create a Managed Instance group from this new mage.
4. Configure autoscaling in Managed Instance Group
•	 
1. Create a disk snapshot of running VM instance.
2. Create an Image from this snapshot.
3. Create an instance template which use this new image
4. Create a Managed Instance group that use this new instance template.
5. Configure autoscaling in Managed Instance Group
(Correct)
•	 
1. Create a disk snapshot of running VM instance.
2. Create an instance template which use this snapshot.
3. Create a Managed Instance group that use this new instance template
4. Configure autoscaling in Managed Instance Group
Explanation
Instance template can not be created from snapshot. So Option D is incorrect
Managed Instance Group can not be created from Disk Image directly. So Option B is incorrect.
Option A is not an automated way to fix the issue.
Option C is the correct sequence to create an autoscaling Managed Instance Group in GCP.
Question 21: 
Skipped
Which Google Cloud service do not cost anything in case of no business activity?
•	 
Cloud App Engine Flexible Environment
•	 
GKE Cluster
•	 
Compute Engine Instace
•	 
Cloud Function
(Correct)
Explanation
Cloud Function, Cloud Run are two completely Google Managed serverless offering from GCP to host application.
App Engine Standard environment also scale down to zero instance in case of no activity. But App Engine Flexible environment needs at least 1 instance even if there is no instance.
Compute engine and GKE cluster also does not scale down to zero instance in case of no activity. In case of GKE cluster, at least one Pod will keep running for system activities.

So here only correct option is Option D (Cloud Function).
Question 22: 
Skipped
Your team wants to host an application in Google Compute Engine instance. This application need to subscribe to Cloud/Pub sub topic and process the message thereafter.
What suggestion will you give to your team in this case to host application?
•	 
Create a Service Account.
Give required roles to this service account to subscribe to Cloud Pub/Sub topic.
Run the VM instance using this service account.
(Correct)
•	 
Deploy application in GKE cluster instead of Compute Engine Instance.
•	 
Create a new IAM user(Member)
Give required roles to this user account to subscribe to Cloud Pub/Sub topic.
Use this user credentials in application to be hosted on Compute Engine Instance.
•	 
Deploy application using Cloud Function.
Configure Cloud Function to subscribe to Cloud Pub/Sub topic.
Explanation
A service account is a special kind of account used by an application or a virtual machine (VM) instance, not a person. Applications use service accounts to make authorized API calls, authorized as either the service account itself, or as Google Workspace or Cloud Identity users through domain-wide delegation.
For example, a Compute Engine VM can run as a service account, and that account can be given permissions to access the resources it needs. This way the service account is the identity of the service, and the service account's permissions control which resources the service can access.
A service account is identified by its email address, which is unique to the account.

Please read the following references for more information
Service Account


Question 23: 
Skipped
Your business team has many users. All of them execute BigQuery Jobs for analysis purpose.
How can you find queries executed on BigQuery datasets by each user in last 2 months?
•	 
Configure BigQuery to publish an event on Cloud Pub/Sub topic whenever any job is executed.
Subscribe to this Cloud Pub/Sub topic and store these logs in Cloud Storage.
Process this Cloud Storage data to find all the queries in last 2 month.
•	 
Run query on the JOBS table to get the required information.
•	 
Check all queries executed by query in Billing Section of Cloud Console.
It will give information about each query executed and cost of each query execution.
•	 
Use Cloud Audit Logging to view Cloud Audit Logs, and create a filter on the query operation to get the required information.
(Correct)
Explanation
Audit logs of BigQuery contains information about query executed and who executed this query. So Option D is correct choice here.
Question 24: 
Skipped
How can you migrate on-premises MySql database to Google Cloud? You need to make sure that database is never out of storage space and CPU usage remain below a certain threshold value and keep the replication lag minimum.
•	 
Attach persistent disk to Cloud Sql instance manually when Storage drops below a certain value.
Add a caching layer by using Memorystore. It will reduce data read latency for application.
Configure alert in Cloud Monitoring to track replication latency.
If replication latency goes beyond a certain threshold value, increase compute capacity of Cloud SQL instance
•	 
Use automatic storage increase feature of Cloud SQL.
Use Compute Optimized machine type for Cloud SQL instance to Keep CPU utilization low.
Configure alert in Cloud Monitoring to track replication latency.
If replication latency goes beyond a certain threshold value, increase memory capacity of Cloud SQL instance
•	 
Attach persistent disk to Cloud Sql instance manually when Storage space drops below a certain value.
Use Memorystore as caching layer to reduce CPU load on database.
Use Compute Optimized machine type for Cloud SQL instance to Keep replication lag low.
•	 
Use automatic storage increase feature of Cloud SQL.
Configure alerts in Cloud Monitoring to track CPU usage and replication lag.
If CPU usage threshold alert is triggered, then switch to existing Cloud Sql Instance to high compute capacity Cloud Sql instance.
If replication lag threshold alert is triggered, then shard the database.
(Correct)
Explanation
Option D is the only option that fulfill all the required conditions. So this is correct choice.
Increase in compute capacity or memory will not help in improving replication lag.
Question 25: 
Skipped
Your company has hosted gaming servers on Compute Engine instances in GCP. Your operation team want to track various performance related metrics with low latency for these gaming servers.
What suggestion will you give to your team to meet this requirement?
•	 
Configure custom metrics in Cloud Monitoring for these game servers.
Create a Dashboard on Cloud Monitoring Console to see the results.
(Correct)
•	 
Use BigTable to store time series data collected from game servers.
Use DataStudio to visualize this data.
•	 
Ingest data collected from game servers at regular interval into BigQuery.
Use DataStudio to visualize this data.
•	 
Store data collected from game servers at regular interval into Datastore.
Execute adhoc queries to analyze this data.
Explanation
Cloud monitoring is GCP offering that allows you to
> Collect metrics from multicloud and hybrid infrastructure in real time
> Enable SRE best practices extensively used by Google including SLOs and SLIs
> Visualize insights via dashboards and charts, and generate alerts

Please read the following references for more information
Cloud Monitoring
Question 26: 
Skipped
Your team has developed a recommendation engine to improve the sales for its customer. This recommendation engine has become very popular. Now your team wants to further improve machine learning algorithm to improve the recommendations.
What solutions will you suggest to your team in this case?
•	 
Use compute optimized machine with high memory
•	 
Store historical data along with past recommendations in BigQuery.
Use this information to train machine learning models.
(Correct)
•	 
Store historical data along with past recommendations in BigQuery.
Give permission to customers to execute queries on BigQuery to analyze this data.
•	 
Store historical data along with past recommendations in BigTable.
Give permission to customers to execute queries on BigTableto analyze this data.
Explanation
Increase in compute or memory capacity will not help team in making machine learning better. It will just improvise hardware infrastructure used for computation. May be it can give faster result but quality of results will not increase with this. So Option A is incorrect.
Giving direct access to any datastore to customer is never a recommended practice. So Option C and Option D are incorrect.
Option B, is the only correct choice to improve machine learning algorithm based on past experiences.
Question 27: 
Skipped
Your company wants to stores all customer information in Cloud Storage.
What solution will you suggest to your team to sanitize this data to remove sensitive information before it is placed in Cloud Storage?
•	 
Use SHA256 hashing to hash customer's data before storing in Cloud Storage
•	 
Use the Data Loss Prevention API to automatically detect and redact sensitive data
(Correct)
•	 
Use .boto configuration file to encrypt customer data.
•	 
Filter PII data by using Cloud Dataprep.
Explanation
Cloud Data Loss Prevention API is fully managed service designed to help you discover, classify, and protect your most sensitive data.
With Data Loss Prevention API you can
> Take charge of your data on or off cloud
> Inspect your data to gain valuable insights and make informed decisions to secure your data
> Effectively reduce data risk with de-identification methods like masking and tokenization
> Seamlessly inspect and transform structured and unstructured data

Please read the following references for more information
Cloud Data Loss Prevention | Google Cloud
Question 28: 
Skipped
Your company want to develop an image recognition application. For this they want to store images in Cloud storage. Development team wants to allow their customers to upload images for 1 day only but most of their customers do not have Google account.
What solution will you suggest to your team to solve this concern?
•	 
Create a web application to upload image.
Customers will login to this application and upload their images.
Stop the application after 1 day.
•	 
Use a signed URL that allows users to upload images to Cloud Storage that expires after 1 day.
(Correct)
•	 
Make cloud storage bucket public, so that all customers can do read and write operations directly on this bucket.
Make Cloud Storage bucket private after 1 day.
•	 
Host a FTP server with permission to store image files there.
Set up data processing pipeline using Cloud Dataflow to ingest data from FTP server.
Stop FTP server after 1 day.
Explanation
A signed URL is a URL that provides limited permission and time to make a request. Signed URLs contain authentication information in their query string, allowing users without credentials to perform specific actions on a resource.

Please read the following references for more information
Signed URL
Question 29: 
Skipped
You have created 4 snapshots of your Compute Engine VM instance boot disk. By mistake snapshot 2 and snapshot 3 were deleted. What will happen to 4th snapshot now?
•	 
Data from snapshot 3, necessary for continuance, will be transferred to snapshot 4. 
(Correct)
•	 
Snapshot 4 can not be used any more.
•	 
All later snapshots, including Snapshot 4, are automatically deleted as well.
•	 
The data from snapshot 3 necessary for continuance will be transferred to snapshot 4, however snapshot 2's contents were transferred to snapshot 1.
Explanation
When you delete a snapshot, Compute Engine immediately marks the snapshot as DELETED in the system. If the snapshot has no dependent snapshots, it is deleted outright. However, if the snapshot does have dependent snapshots:
1) Any data that is required for restoring other snapshots is moved into the next snapshot, increasing its size.
2) Any data that is not required for restoring other snapshots is deleted. This lowers the total size of all your snapshots.
3) The next snapshot no longer references the snapshot marked for deletion, and instead references the snapshot before it.
Deleting a snapshot does not necessarily delete all the data on the snapshot. If any data on a snapshot that is marked for deletion is needed for restoring subsequent snapshots, that data is moved into the next corresponding snapshot. To definitively delete data from your snapshots, you should delete all snapshots.
 

Please read the following references for more information
Restore and Delete Snapshot
Question 30: 
Skipped
Your team has hosted a microservices based application in Google Cloud. In this application some APIs are responding very slow because these API invoke multiple microservices internally to complete the request.
Your team is interested in knowing which microservice is actually slowing down the request processing.
What solution will you suggest to your team in this case?
•	 
Add logging statement in each API to track time taken by each microservice.
Check the logs for each microservice to find time taken by each API call.
•	 
Set maximum request processing timeout for application. It will cause API invocation fail faster.
•	 
Use Cloud Trace to collect collects latency data for each API from your applications
(Correct)
•	 
Create alerts in Cloud Monitoring to find when your API latencies are high.
Explanation
Cloud Trace is a distributed tracing system that collects latency data from your applications and displays it in the Google Cloud Console. You can track how requests propagate through your application and receive detailed near real-time performance insights. Cloud Trace automatically analyzes all of your application's traces to generate in-depth latency reports to surface performance degradations, and can capture traces from all of your VMs, containers, or App Engine projects.

It can help you in answering following kind of questions
• How long does it take to handle a given request?
. Why is it taking so long to handle a request?
• Why do some of the requests take longer than others?
• What is the overall latency of requests for the application?

Please read the following references for more information
Cloud Trace
Question 31: 
Skipped
Your company has developed an application in native C++. Currently test suite for this application runs on virtual machine in on-premises data center. Since company has very limited number of resource in on-premises environment, so complete test suite run take many hours. Now company wants to migrate its testing infrastructure to Google Cloud, so that it can run test suite faster.
What suggestion will you give to your team for this migration process? Make sure that minimum effort is required for this migration.
•	 
Deploy application using Cloud Function
•	 
Deploy application on App Engine Standard Environment.
•	 
Deploy application on Compute Engine instances which are managed by auto scaling Managed Instance Group.
(Correct)
•	 
Deploy application on Compute Engine instances which are managed by Unmanaged Instance Group.
Explanation
Cloud function is good for event based processing. So Option A is incorrect.
App Engine Standard Environment does not support C++. So Option B is incorrect.
Unmanaged Instance Group does not support auto scaling, so Option D is incorrect.
With auto scaling Managed Instance Group, number of instance will be scaled automatically based on the traffic load. So Option C is correct.
Please read the following references for more information
Instance Groups
Question 32: 
Skipped
Your company has developed an application using Compute Engine in Google Cloud. Your operation team wants to take regular back up of disk so that team can create VM in other Cloud projects as well.
What solution will you suggest to your team in this case?
•	 
Use VM migration tool to create VM in other project.
•	 
Create snapshot of boot disk.
Create an image from snapshot.
Share image with other projects.
Create VM instance from image.
(Correct)
•	 
Create snapshot of boot disk.
Share snapshot with other projects.
Create VM instance from snapshot.
•	 
Create snapshot of boot disk.
Share snapshot in Cloud Storage Bucket.
Read snapshot from Cloud Storage Bucket and create image from this in other projects.
Create VM instance from image.
Explanation
Option B represent the right steps to create VM instance in other projects.

Please read the following references for more information
Create Instance From Images
Question 33: 
Skipped
Your company wants to use Cloud Sql instance as backup replica of on-premises MySql database. Current on-premise database is 5 TB in size and there can be very frequent large update on this database.
Networking team wants to use just private address space for communication between two databases.
What kind of networking solution will you suggest to networking team in this case?
•	 
Use VPC peering between on-premises network and Google Cloud VPC
•	 
Setup VPN connection between on-premises network and Google Cloud VPC
•	 
Use Dedicated Interconnect between on-premises network and Google Cloud VPC
(Correct)
•	 
Communication with Cloud Sql instance using private IP is not possible.
Explanation
VPN connection and Dedicated Interconnect can be used for communication using private IP addresses between two networks. But since this 5 TB database, and large updates are frequent so Dedicated Interconnect is better choice than VPN. So Option C is correct answer.

VPC peering can used between two VPC with in Google cloud only. So Option A is incorrect.
Question 34: 
Skipped
Your company has many databases that run on a single MySQL instance. How can you take backups of a specific database at regular intervals?
While taking backup, you need to ensure that this activity gets completed as soon as possible without any impact on disk performance.
•	 
Mount a Local SSD disk as the backup location.
Once backup is complete, use gsutil to move the backup to Google Cloud Storage.
(Correct)
•	 
Write a shell script that use the gcloud tool to take regular backups using persistent disk snapshots.
Configure a cron job to execute above created script.
•	 
Mount additional persistent disk volumes onto each virtual machine (VM) instance in a RAID10 array.
Use VM to create snapshots and store snapshot in Cloud Storage
•	 
Use gcsfuse to mount a Google Cloud Storage bucket as a volume directly on the instance.
Write backups to the mounted location using mysqldump
Explanation
Option A is correct here because Local disk is very fast in terms of IO. So backup can be taken very fast.
Other option like persistent disk or GCS fuse is slow as compare to local disk. So it is not recommended option in this case.
Question 35: 
Skipped
Your company wants to build an application for a customer who has chain of hotels across the world. This application need to capture important information about conference halls usage. Some sensors are installed in each conference hall that will collect data about the hall every second. Since these hotels could be in remote areas as well so internet connectivity is not consistent everywhere.
As a Google Cloud architect, What solution will you suggest to upload the data collected by sensors in Google Cloud?
•	 
Host a custom application on Compute Engine VM instance.
Ensure a persistent connection to a VM instance from each sensor.
Write messages to a custom application.
•	 
Allow each device to store data in separate table in Cloud SQL directly
•	 
Sensors should poll for connectivity to Cloud Pub/Sub and the publish the latest messages at regular interval.
Use common topic for all devices.
(Correct)
•	 
Ensure a persistent connection between sensors and App Engine application.
Sensor will send data to application hosted in App Engine and this application will store messages in Cloud Datastore.
Explanation
Data coming from sensors is a example of stream data coming from IOT device and Cloud Pub/Sub is highly scalable and highly available service to receive stream input data.
Question 36: 
Skipped
Your organization has developed a 3-tier web application.  This application is deployed in the same VPC in GCP and each tier can scale independently.
You have to ensure that Web tier can not access database directly. Database should be accessible from API tier only. In the same way, API tier should be accessible from web tier only.
How will you fulfill these requirements?
•	 
Associate network tags to each tier.
All instances in same tier will have same network tags.
Configure firewall rules to allow the desired traffic flow using above tags..
(Correct)
•	 
Add tags to each tier and set up routes to allow the desired traffic flow.
•	 
Install and configure custom firewall application on each VMs.
•	 
Deploy each tier inside a different subnetwork.
Explanation
In Google Cloud VPC, we can create firewall rules based on the network tags. In this case we create separate tags for each layer like 'WEB', 'API' and 'DATABASE', and then we can use these tags to create firewall rules to achieve desired network traffic flow.

Please read the following references for more information
https://cloud.google.com/vpc/docs/using-vpc
https://cloud.google.com/vpc/docs/routes
https://cloud.google.com/vpc/docs/add-remove-network-tags
Question 37: 
Skipped
You want to run 2 different version of same API with same SSL and DNS records in place to serve both APIs. Latest version of API is just for new customers and testing team only.
How can you fulfill these requirements?
•	 
Use separate backend services for each API path behind the load balancer.
(Correct)
•	 
Configure the old API to forward traffic to the new API based on the path.
•	 
Redirect old clients to use a new endpoint for the new API.
•	 
Use different load balancer for different versions.
Explanation
To use both API simultaneously, we can create a different back end service for each API and then use url path based routing to route traffic to each backend instances

Please read the following references for more information
https://cloud.google.com/load-balancing/docs/https/url-map
https://cloud.google.com/load-balancing/docs/backend-service
https://cloud.google.com/load-balancing/docs/https/global-forwarding-rules
Question 38: 
Skipped
Your company has set up multiple VPC in Google Cloud. Security team found that VMs in one VPC have publicly open SSH port.
This is serious security risk for the company.
How can you help team in finding who created this VPC network?
•	 
Use Cloud Logging to find the user who created the VMs inside VPC.
•	 
Look for Create VM entries in the Activity page on Cloud Console.
•	 
SSH to the instance and identify who previously logged in to the system, and check with them.
•	 
Use Cloud Logging Console to find  Create Insert entry under GCE Network logs.
(Correct)
Explanation
Google Cloud Logging service provides real-time log management and analysis at enterprise scale.
Cloud logging can hold logs from services running both in Google Cloud and in your on-premises data center. It allows to securely store, search, analyze, and alert on all of your log data and events

Please read the following references for more information
https://cloud.google.com/logging
Question 39: 
Skipped
Your team manages several different microservices. Each microservice use different programming languages and need different runtime.
How can you set up development environments from these microservices which is similar to respective production environments.
What solution can you suggest without any code change in application?
•	 
Use App Engine
•	 
Use Cloud Function
•	 
Containers
(Correct)
•	 
Virtual Machines
Explanation
In such case we can create Docker container which will create environment similar to production environment for all the microservice. So Containers is correct answer for this.
Question 40: 
Skipped
You are developing an application to handle critical transaction.
How can you ensure that all transactions are processed in the same order they are received?
You also need to ensure each transaction is processed exactly once.
How can you full fill these requirements?
•	 
Use Cloud Pub/Sub for FIFO and Cloud DataFlow for exactly-once
(Correct)
•	 
Use Cloud Pub/Sub for FIFO and exactly-once processing.
•	 
Use Cloud Pub/Sub for FIFO and Custom Application for exactly-once processing.
•	 
Use Cloud Pub/Sub for FIFO and Dataproc for exactly-once processing.
Explanation
With Cloud Pub/Sub FIFO all transaction will be sequenced in the same order as transactions are received and Cloud Dataflow will ensure exactly once processing.

Pub/Sub Message Ordering (https://cloud.google.com/pubsub/docs/ordering)
If messages have the same ordering key and are in the same region, you can enable message ordering and receive the messages in the order that the Pub/Sub service receives them.
Pub/Sub delivers each message at least once, so the Pub/Sub service might redeliver messages. When you receive messages in order and the Pub/Sub service redelivers a message with an ordering key, Pub/Sub maintains order by also redelivering the subsequent messages with the same ordering key. The Pub/Sub service redelivers these messages in the order that it originally received them.

Please read the following references for more information
Dataflow exactly once processing
Cloud Pub/Sub message ordering
Question 41: 
Skipped
Your company has its on-premises data center that is running out of space.
How can you migrate 150 TB of data from on-premise data center data center to Cloud Storage in Google Cloud? There is 1Gbps connection between on-premises data center and Google Cloud.
What should you do as per Google's recommendation ?
•	 
Use Data transfer appliance to decrypt and transfer data to Cloud Storage.
(Correct)
•	 
Use Cloud Pub/Sub to transfer the data.
•	 
Use gsutil -m command line utility to transfer the data.
•	 
Set up data ingestion pipeline using Cloud Data Flow that will read data from input source and store it in GCS bucket.
Explanation
As per Google's recommendation, gsutil can be used to transfer data up to 1TB. Beyond this Transfer Appliance is more preferred way to transfer the data.
 
Please read the following references for more information
Data Migration Options to migrate data to Cloud storage
Question 42: 
Skipped
Your company wants to migrate to Google Cloud. They have a requirement to store different kind of data like user session data, pictures, Virtual machine boot volumes, Virtual Machine data volumes, various logs etc.
Which Google Cloud services your would like to use for such use case?
•	 
Use Local disk to store customer data
Use GCS bucket to store all other kind of data
•	 
Use Cloud SQL to store user session data and use Memcache on top of it to improve performance
Use Local disk for Virtual machine boot volumes & Virtual Machine data volumes
Use GCS bucket for images and logs data
•	 
Use Datastore to store user session data and use Memcache on top of it to improve performance
Use GCS bucket to store all other kind of data like Virtual machine boot volumes, Virtual Machine data volumes, pictures and application logs
(Correct)
•	 
Use Persistent Disk to store user session data and use Memcache on top of it to improve performance
Use instances with local SSDs for storing VM boot volumes and VM data volumes.
Use Cloud Storage for application logs and images.
Explanation
Datastore is used well suited to store user data and Memcache can further improve the read operation.
Since Cloud Storage is Object database so it can be used to all backup like boot volume, data volume, pictures and application logs.
Question 43: 
Skipped
Your company is running an application that need many Virtual Machines. This application can autoscale based on application traffic and it needs to handles customer's sensitive data.
Application security team wants that any non essential traffic between instances must be blocked and static IP addresses must be avoided.
As per Google's recommendation, how would you like to design architecture of this application.
•	 
Set up Organization Policy to allow just the essential traffic between VM and deny all other traffic
•	 
Define IAM policy for VMs service accounts to deny traffic from other VMs.
•	 
Associate network tags with VMs
Set up firewall rules based on these network tags to allow just the essential traffic and deny all other traffic
(Correct)
•	 
Create each VM in separate subnet in Google Cloud
Explanation
Same network tags can be associated with a set of VMs and the firewall rules based on these network tags can control allowed and denied traffic. Such rules will be applicable to all the VMs associated with respective VM.

Please read the following references for more information
Add or remove network tags
VPC Firewall
Question 44: 
Skipped
Your friend need to configuring a Google Kubernetes Engine cluster to scale cluster nodes up and down based on CPU utilization. What should you suggest?
•	 
Enable autoscaling on Managed Instance Group (MIG) for the GKE cluster.
Enable Horizontal Pod Autoscaler based on CPU utilization.
•	 
Enable autoscaling on Managed Instance Group (MIG) for the GKE cluster.
Update deployment to set appropriate values for CPU utilization threshold.
•	 
Enable GKE Cluster Autoscaler.
Enable Horizontal Pod Autoscaler based on CPU utilization.
(Correct)
•	 
Enable GKE Cluster Autoscaler.
Update deployment to set appropriate values for maxUnavailable and maxSurge.
Explanation
GKE cluster supports horizontal autoscaling both in terms of node and pods.

Please read the following references for more information
GKE cluster autoscaler
Question 45: 
Skipped
You have a critical application running on App Engine standard environment and you have done some bug fix for your application.
How can you validate the bug fixes with live traffic for a small set of users before migrating whole traffic to newer version?
•	 
Use rolling update feature of Managed Instance Group to deploy latest code on few instances only.
After validation, deploy a full rollout.
•	 
Deploy the fix as a new application in App Engine Standard environment.
Split traffic between the two applications using HTTP(s) load balancer.
•	 
Deploy the fix as a new application in App Engine Flexible environment.
Split traffic between the two applications using HTTP(s) load balancer.
•	 
Deploy a new version in the App Engine application
Use traffic splitting to distribute traffic across the old and new versions.
(Correct)
Explanation
By using traffic splitting feature, we can migrate some portion of traffic to new version and validate the bug fix there.

Please read the following references for more information
Splitting Traffic in App Engine
Question 46: 
Skipped
Your company has an application that is used by customers across the world. This application can be downloaded from a dedicated website running on virtual machines in US-Central-1 region. Some customers have complained about slowness when downloading the application.
As per Google recommended practices. How can you improve this latency to download application?
•	 
Save application versions in Multiple Multi-Regional Cloud Storage buckets, one bucket per multi-region.
(Correct)
•	 
Save application versions in multiple Regional Cloud Storage buckets, one bucket per zone per region.
•	 
Save application versions in a single Regional Cloud Storage bucket, one bucket per zone of the region.
•	 
Save application versions in a Multi-Regional Cloud Storage bucket.
Explanation
Since this application can be downloaded by a customer across the world, so Option A is the correct choice here.
Question 47: 
Skipped
One team in your company is running MySql database on Compute Engine instance in Google Cloud. Team observed some slowness in I/O operations in database. Currently team is using n1-standard-16 virtual machine with 100 GB of SSD zonal persistent disk.
What solution will you suggest to your team to improve the performance of database?
•	 
Increase Virtual Machine's memory
•	 
Configure autoscaling on Compute Engine VM instance
•	 
Dynamically resize persistent disk to 200GB
(Correct)
•	 
Use n1-standard-32 Compute Engine VM instance
Explanation
Option C is correct because persistent disk performance is based on the total persistent disk capacity attached to an instance and the number of vCPUs that the instance has. Incrementing the persistent disk capacity will increment its throughput and IOPS, which in turn improve the performance of MySQL.

Please read the following references for more information
https://cloud.google.com/compute/docs/disks/#pdspecs
https://cloud.google.com/compute/docs/disks/performance
Question 48: 
Skipped
You company is developing an application for traffic controlling agencies. This application need to process the data collected from various speed sensors. Millions of speed sensors are installed at various points across the whole country which will check speed of vehicle and number plate of the vehicles.  These sensors could be in remote areas as well, and internet connectivity in those remote area is not consistent.
What solution will you suggest in this case so that application can collect information from so many devices and process that data?
•	 
Create a custom application which will read information from sensors at regular interval and provide this data to your application via REST API.
•	 
Configure each sensor to poll Cloud Pub/Sub for the connectivity and publish data in a shared topic.
(Correct)
•	 
Allow sensors to store data directly into Cloud Datastore.
•	 
Allow sensors to invoke Cloud Function and Cloud Function in turn will send data to your application.
Explanation
Cloud Pub/Sub is Google Managed service that can easily support data ingestion at any scale, so Option B is correct choice here.
Question 49: 
Skipped
Your company has developed a web crawling application written in C++. This application hits the URL and process the response content. Currently this application is deployed in on-premises data center. It is observed that this application is not able to meet the SLA for response processing in case of peak load. So, team is looking for various Cloud based solution to solve this issue.
What solution can you suggest in this case? How can they leverage GCP to solve this issue?
•	 
Deploy application in Compute Engine VM instance.
Add those instances in Unmanaged Instance Group.
•	 
Deploy application in App Engine standard environment.
•	 
Use Cloud Dataflow pipeline for response processing
•	 
Deploy application in Compute Engine VM instance via Managed Instance Group.
Configure Autoscaling in Managed Instance Group.
(Correct)
Explanation
By Configuring Autoscaling in Managed Instance Group, numbers of instances will adjust automatically based on the load on instances. So Option D is correct choice here.
Question 50: 
Skipped
Your team has developed some REST APIs to expose your application data. This application is deployed in GCP. How can you enable delegated authorization so that 3rd parties can invoke your APIs?
•	 
Use OAuth2.0
(Correct)
•	 
Use SAML2.0
•	 
Develop custom Single Sign On Application and deploy it in Compute Engine Instances.
•	 
Create a service account with required access to invoke your application API.
3rd party services should use this service account
Explanation
OAuth2.0 is industry standard protocol for delegated authorization.

Question 1: Incorrect
In your company, there is one application that is running on Compute Engine VM instances having Linux operation system. These VM instances are launched using managed instance group. This application is facing some performance issues on one of the VM instance that host this application and you need to find the cause of this performance issue.
You observed that when application is under heavy load, then some application requests are getting dropped and there is a single application process that is consuming all the available CPU on misbehaving instance.
Autoscaling of VM instance has reached the upper limit of instances and there is normal load on all other related systems. Database is also working fine.
To solve this production issue, what can you suggest?
•	 
Create an autoscaling metric based on percentage of memory used and use this metric to do autoscaling
•	 
Restart the misbehaving instances on a regular scheduled basis.
•	 
Increase the maximum number of instances limit in the autoscaling group.
(Correct)
•	 
Login the misbehaving instance via SSH connection and restart the application process.
(Incorrect)
Explanation
Since existing application instances are using full CPU and autoscaling maximum limit is also reached, so to enable more processing, maximum limit of autoscaling group must be increased.
This way it will create more compute engine instance to handle increasing load on existing application infrastructure.
Question 2: Correct
Your company has an application that performs some complex calculation. This application exposes web API for these calculations. Currently this application runs on a single GKE cluster in us-central1 region only.
Now based on the demand of the application, Your company want to expand its business and need to offer Web API to its customers in Asia region as well.
As Google Cloud architect, What solution would you like to suggest?
•	 
Create a second GKE cluster in Asia region.
Use service of type LoadBalancer to expose web API from both regions.
Add the public IP address to the Cloud DNS zone.
(Correct)
•	 
Use global HTTP(s) load balancer
•	 
Increase the memory and computation capacity in same GKE cluster.
•	 
Create a second GKE cluster in Asia region.
Create a global HTTP(s) Load Balancer via kubectl.
Explanation
When you create a Service of type LoadBalancer, a Google Cloud controller wakes up and configures a network load balancer in your project. The load balancer has a stable IP address that is accessible from outside of your project.

Note that a network load balancer is not a proxy server. It forwards packets with no change to the source and destination IP addresses.

Please read the following reference for more information
https://cloud.google.com/kubernetes-engine/docs/concepts/service
Question 3: Incorrect
You want to migrate Hadoop jobs without modifying the underlying infrastructure.
How can you achieve this with minimum cost and minimum infrastructure management effort?
•	 
Create a Dataproc cluster using standard worker instances.
(Incorrect)
•	 
Create a Dataproc cluster using preemptible worker instances
(Correct)
•	 
Setup a Hadoop cluster manually on Compute Engine VM instances.
•	 
Setup a Hadoop cluster manually using preemptible Compute Engine VM instances
Explanation
Existing Hadoop jobs can be easily migrated to Dataproc cluster and Preemptible Compute Engine VM instances can be used to reduce the cost.
Question 4: Incorrect
Your company has microservices based application running on Anthos clusters. This cluster has both Anthos Service Mesh and Anthos Config Management configured.
Application user feels latency in application response
How can you find the microservice that is causing the latency in response?
•	 
Use Anthos Config Management to create a namespace selector that select the relevant cluster namespace.
Filter workloads on GCP cloud console for the namespace.
Analyze the configurations of the filtered workloads.
•	 
Collect and inspect the telemetry for request latency data between microservice by using Service Mesh visualization in GCP Cloud Console.
(Correct)
•	 
Create a Cluster Selector selecting the relevant cluster via Anthos Config Management.
Filter workloads on GCP cloud console for the above cluster.
Analyze the configurations of the filtered workloads.
(Incorrect)
•	 
Reinstall Istio service mesh using the default Istio profile.
Collect and inspect the telemetry for request latency data between microservice in the Cloud Console.
Explanation
Anthos Service Mesh is a suite of tools that helps you monitor and manage a reliable service mesh on-premises or on Google Cloud.
Anthos Service Mesh’s robust tracing, monitoring, and logging features give you deep insights into how your services are performing, how that performance affects other processes, and any issues that might exist.

Please read the following reference for more information
https://cloud.google.com/service-mesh/docs/overview

Question 5: Correct
Your company want to run multiple VM instances in one VPC without public IP address assigned to any VM. There in no connection between on-premises data center and Google cloud.
How can any team member in your company can connect to VM instance via SSH connection in such case?
•	 
Configure Cloud NAT in each subnet of VPC where the VM is hosted.
First connect to Cloud NAT IP via SSH and then connect to VM instances in VPC.
•	 
Create an Unmanaged Instance Group.
Add all instances in to above group.
Use this instance group as backed for TCP load balancer.
Use TCP load balancer IP to connect to VM instances in VPC.
•	 
Create a bastion host in same VPC network.
Assign public IP to this bastion host VM instance.
Start the bastion host when team member need to connect to VMs with private IP in your VPC.
Connect to bastion host via SSH using public IP of bastion host.
Connect to other VMs via SSH using their private IP from this bastion host.
Stop the bastion host after work is done.
(Correct)
•	 
Use Private Google Access to connect to VMs via their private IP 
Explanation
Bastion hosts provide an external facing point of entry into a network containing private network instances. This host can provide a single point of fortification or audit and can be started and stopped to enable or disable inbound SSH. By using a bastion host, you can connect to an VM that does not have an external IP address. This approach allows you to connect to a development environment or manage the database instance for your external application without configuring additional firewall rules.
 

Please read following reference for more information
https://cloud.google.com/solutions/connecting-securely
Question 6: Incorrect
Your company wants to store some secured files in GCS bucket from on-premises data center.
Since its a secured file, so they want to ensure that file stored in GCS bucket is exactly same copy of file stored in on-premises data center.
How can you achieve this with minimal cost and effort?
•	 
Upload all the files to GCS bucket via gsutil -m command.
Computes CRC32C hashes via custom application in on-premises data center.
Collect CRC32C hashes of uploaded files via gsutil ls -L gs://[YOUR_BUCKET_NAME}.
Compare the hashes.
•	 
Upload all the files to GCS bucket via gsutil -m command.
Download the uploaded files via gsutil cp command .
Use Linux diff command to compare the files content.
(Incorrect)
•	 
Use Linux shasum to compute a digest of the files in on-premises data center.
Upload all the files to GCS bucket via gsutil -m command.
Download the uploaded files via gsutil cp command .
Use Linux shasum to compute a digest of the downloaded files.
Compare the hashes.
•	 
Upload all the files to GCS bucket via gsutil -m command.
Calculate CRC32C hashes of all on premise files via gsutil hash -c FILE_NAME.
Collect CRC32C hashes of uploaded files via gsutil ls -L gs://[YOUR_BUCKET_NAME}.
Compare the hashes.
(Correct)
Explanation
1.	gsutil -m 
It allows to upload files in parallel using multithreading.
1.	gsutil hash [-c] [-h] [-m] filename...
The hash command calculates hashes on a local file that can be used to compare with gsutil ls -L output. If a specific hash option is not provided, this command calculates all gsutil-supported hashes for the file.

Please read following references for more information
https://cloud.google.com/storage/docs/gsutil/commands/hash
https://cloud.google.com/storage/docs/gsutil/addlhelp/TopLevelCommandLineOptions

Question 7: Incorrect
You want to deploy application using microservice architecture in GKE but you do not want to expose these microservices end points to external world. All these microservices will be used inside cluster only. All these microservice can have different number of replicas.
Now suggest a way so that all these microservices can communicate with each other in uniform and consistent manner. 
•	 
Create Deployment for each microservice to deploy in GKE cluster.
Use Service to expose the deployment in the cluster.
One microservices should talk to other microservice in cluster via Service DNS name.
(Correct)
•	 
Create Deployment for each microservice to deploy in GKE cluster.
Use Ingress to expose the deployment in the cluster.
One microservice should talk to other microservice in cluster via Ingress IP address.
(Incorrect)
•	 
Create Pod for each microservice to deploy in GKE cluster.
Use Service to expose the deployment in the cluster.
One microservices should talk to other microservice in cluster via Service DNS name.
•	 
Create Pod for each microservice to deploy in GKE cluster.
Use Ingress to expose the deployment in the cluster.
One microservice should talk to other microservice in cluster via Ingress IP address.
Explanation
Deployments represent a set of multiple, identical Pods with no unique identities. A Deployment runs multiple replicas of your application and automatically replaces any instances that fail or become unresponsive. In this way, Deployments help ensure that one or more instances of your application are available to serve user requests. Deployments are managed by the Kubernetes Deployment controller.
In GKE, Service is use to group a set of Pod endpoints into a single resource. You can configure various ways to access the grouping. By default, you get a stable cluster IP address that clients inside the cluster can use to contact Pods in the Service. A client sends a request to the stable IP address, and the request is routed to one of the Pods in the Service.
A Service also provides load balancing. Clients call a single, stable IP address, and their requests are balanced across the Pods that are members of the Service.

Please read following references for more information
https://cloud.google.com/kubernetes-engine/docs/concepts/deployment
https://cloud.google.com/kubernetes-engine/docs/concepts/service
Question 8: Correct
Your company has applications that processes some sensitive data for its clients. You want to migrate such application onto Google Cloud.
As per your company policies, all the applications should run on physically separated hardware. There is also a requirement that workloads of different clients can not run on same machine.
As Google Cloud architect, you used a sole tenant node group and created a node for each client. 
How can you meet the requirement to deploy the workloads of each client on the host machine dedicated to respective host.
•	 
Create an organization policy that does not allow any two applications to run on same VM host machine.
•	 
While creating VM instances associate different network tags based on node group name with VM instances to make sure that workload of each client is deployed on the host machine dedicated to respective host.
•	 
While creating VM instances use node affinity labels based on the node group to make sure that workloads of each client is deployed on the host machine dedicated to respective host.
•	 
While creating VM instances use node affinity labels based on the node name to make sure that workloads of each client is deployed on the host machine dedicated to respective host.
(Correct)
Explanation
For dedicated hardware, we  must go with sole tenant node. Sole-tenancy lets you have exclusive access to a sole-tenant node, which is a physical Compute Engine server that is dedicated to hosting only your project's VMs. Use sole-tenant nodes to keep your VMs physically separated from VMs in other projects, or to group your VMs together on the same host hardware
 

Within a sole-tenant node, you can provision multiple VMs on machine types of various sizes, which lets you efficiently use the underlying resources of the dedicated host hardware. Also, because you aren't sharing the host hardware with other projects, you can meet security or compliance requirements with workloads that require physical isolation from other workloads or VMs.

Affinity labels let you logically group nodes and node groups, and later, when provisioning VMs, you can specify affinity labels on the VMs to schedule VMs on a specific set of nodes or node groups

When you create a VM, you request sole-tenancy by specifying node affinity or anti-affinity, referencing one or more node affinity labels. You specify custom node affinity labels when you create a node template, and Compute Engine automatically includes some default affinity labels on each node. By specifying affinity when you create a VM, you can schedule VMs together on a specific node or nodes in a node group. By specifying anti-affinity when you create a VM, you can ensure that certain VMs are not scheduled together on the same node or nodes in a node group.

You can create custom node affinity labels when you create a node template. These affinity labels are assigned to all nodes in node groups created from the node template. You can't add more custom affinity labels to nodes in a node group after the node group has been created.

Provisioning a VM on a sole-tenant node requires the following:
Creating a sole-tenant node template. The sole-tenant node template specifies uniform properties for all of the sole-tenant nodes in a sole-tenant node group.
Creating a sole-tenant node group using the previously created sole-tenant node template.
Creating VMs and provisioning them on a sole-tenant node group.

Please read following references for more information
https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes
https://cloud.google.com/compute/docs/nodes/provisioning-sole-tenant-vms
https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes#node_affinity_and_anti-affinity
https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes#custom_affinity_labels
https://cloud.google.com/compute/docs/nodes/provisioning-sole-tenant-vms#create_a_sole-tenant_node_template
https://cloud.google.com/compute/docs/nodes/provisioning-sole-tenant-vms
Question 9: Correct
Your company has a monolithic application. This application has lots of issues in terms of reliability, and being a monolithic application, it is difficult to do any enhancements in this application.
Now this team want to break this monolithic application into multiple microservices but they want to minimize the operation overhead of microservice architecture. So this team is more interested in using fully managed service for this microservice architecture.
How can this team convince their top management to provide approval for this transition from monolithic to microservice architecture. What are the advantages that this team can highlight?
•	 
Microservice architecture will decouple infrastructure from application.
Development and release cycle of new features will become easy.
Infrastructure management will become easier by using managed service in cloud.
It is easy to set up CI/CD pipeline for each microservice in cloud.
A/B testing can be done easily for each microservice.
Microservices are easy to scale.
(Correct)
•	 
Docker image can be created for monolithic application.
This docker image can be easily deployed in Kubernetes cluster.
•	 
VM instances can be easily migrated to cloud in automated manner and it will be cost effective as well
•	 
Microservice will cost less in cloud as compare to monolithic.
Infrastructure management will become easier.
Explanation
All the reason mentioned in choice A are obvious advantages of using microservice.

Please read following reference for more information
https://cloud.google.com/architecture/migrating-a-monolithic-app-to-microservices-gke
Question 10: Incorrect
You need to migrate your application running in on-premises data center to Google Cloud. You are not able to select right machine configuration in terms of CPU and memory in cloud. In on-premises data center this application was optimally configured for efficient usage of resources. This application has been getting used uniformly throughout the week since multiple weeks.
As Google Cloud architect how can you optimize the resource usage of VM instance running in cloud?
You also need to take care of costing of VM as well.
•	 
Select the machine type having CPU and memory configuration similar to on-premises instance.
Use this machine type to create VM instance in Google Cloud.
Configure the Cloud Monitoring agent and deploy the third party application on these VM instances.
Do load testing of your application in cloud for usual traffic that you were getting in on-premises data center.
Follow the recommendations shown in Cloud Console for rightsizing of the VM instances running in cloud..
(Correct)
•	 
Create multiple compute engine instances with varying CPU and memory options.
Configure the Cloud Monitoring agent and deploy the third party application on these VM instances.
Do load testing of your application in cloud for peak traffic.
Use  load testing results to tune in the configuration of CPU and memory
•	 
Create an image of the third-party application running in on-premises virtual machine.
Create an instance template with the smallest available machine type that use image created in previous step.
Create a managed instance group.
Use average CPU utilization to autoscale the number of instances in the group.
Optimize the number of instances running based on the average CPU utilization threshold.
(Incorrect)
•	 
Use App Engine Flexible environment to deploy the third-party application using a Dockerfile.
Configure CPU and memory options similar to your application's current on-premises virtual machine in the app.yaml file
Explanation
Since this application is already configured in optimal manner in on-premises data center so it make sense to use similar configurations in GCP as well. So Option A looks like the right option.
Option B is incorrect because we can not create a VM with varying number of CPU and memory.
Option C look incorrect because only CPU is considered for optimal configuration of application.
Option D is not correct because it is a simple lift and shift approach to migrate to cloud. In this case, first application might need to be containerized using Docker. This is an extra development effort so this is not preferred.
Question 11: Incorrect
You need to run an application on Compute Engine VM instance in GCP that use Ubuntu Linux operating system. This application needs very complex configurations in order to run it properly.
How can you ensure that you can install Ubuntu distribution updates with minimal manual intervention whenever such update is available.?
•	 
Create a Compute Engine instance using a latest image of Ubuntu Linux operating system.
Install and configure the application on this VM.
Use OS patch management to install available updates.
•	 
Create a Compute Engine instance template using a latest image of Ubuntu Linux operating system.
Create an instance from this template.
Install and configure the application as part of the startup script.
Repeat the process whenever a new Google-managed Ubuntu image becomes available
(Correct)
•	 
Create a Compute Engine instance using a latest image of Ubuntu Linux operating system.
Install and configure the application on the instance after connecting to this instance via SSH.
Repeat this process whenever a new Google Cloud managed Ubuntu image becomes available
(Incorrect)
•	 
Create a Docker container image with Ubuntu as the base image.
Install and configure the application as part of Docker image creation process.
Host the container on GKE.
Restart the container whenever a new update is available.
Explanation
Option A and Option C, are incorrect because in both options, application configurations has to be done manually again and again with release of new Ubuntu Image. Since application configuration is very extensive, so manual configuration of application repeatedly is not the right choice. It is both time consuming and error prone because there can be some mistake in doing the manual configuration.

Option D, is incorrect because it requires to create a Docker image that contains both OS and application configuration. It is not a good idea to have OS in docker image. In addition to this, creating a docker image is additional development effort as well.

Option B is the correct choice because in this option Startup Script is used to install and configure application. Startup Script execution is an automated process which will be executed automatically whenever new instance is created.
With combination of instance template and Startup Script, we can create any number of compute engine instances very quickly with exactly same configurations.
Question 12: Incorrect
Your company is designing its data lake on Google Cloud and wants to develop different ingestion pipelines to collect unstructured data from different sources. After the data is stored in Google Cloud, it will be processed in several data pipelines to build a recommendation engine for the end users on the website. The structure of the data retrieved from the source system can change at any time. The data must be stored exactly as it was retrieved for reprocessing purpose in case the data structure is incompatible with the current processing pipelines.
How will you design an architecture to support the use case after you retrieve the data?
•	 
Send the data through the processing pipeline, and then store the processed data in a Cloud Storage bucket for reprocessing.
•	 
Send the data through the processing pipeline, and then store the processed data in a BigQuery table for reprocessing.
(Incorrect)
•	 
Store the data in a BigQuery table.
Design the processing pipelines to retrieve the data from the table.
•	 
Store the data in a Cloud Storage bucket.
Design the processing pipelines to retrieve the data from the bucket.
(Correct)
Explanation
Option A and Option B are incorrect because in both cases, processed data is stored. As per requirement, our solution must store the input data as it is in raw format for processing purpose because structure of the data retrieved from the source system can change at any time.
Option C is incorrect because we can not store raw data in BigQuery. BigQuery store only structured data.
Option D is correct choice here because in this option we are storing raw data in Cloud Storage bucket and then creating data processing pipeline using this raw data as input.
Question 13: Correct
Your company is running a solution in App Engine Standard environment. The project that contains the App Engine application has a VPC network and this VPC network is connected with company's on-premises environment via a Cloud VPN tunnel.
What solution can you suggest so that the App Engine application can access database running inside the on-premises environment?
•	 
Configure serverless VPC access.
(Correct)
•	 
Configure private services access
•	 
Configure private Google access.
•	 
Configure private Google access for on-premises hosts only.
Explanation
Serverless VPC Access enables you to connect from a serverless environment on Google Cloud directly to your VPC network. This connection makes it possible for your serverless environment to access resources in your VPC network via internal IP addresses.
With Serverless VPC Access, you create a connector in your Google Cloud project and attach it to a VPC network. You then configure your serverless services (such as Cloud Run services, App Engine apps, or Cloud Functions) to use the connector for internal network traffic.
Serverless VPC Access only allows requests to be initiated by the serverless environment. Requests initiated by a VM must use the external address of your serverless service

Please read following reference for more information
https://cloud.google.com/vpc/docs/serverless-vpc-access
Question 14: Incorrect
Your company is using Compute Engine instances to host its applications. There are 3 different environments: production, staging, and development environments.
The production environment is very critical and used for whole day, while other non production environments are critical during office time only.
What solution can you suggest for optimum usage of these environments which can save cost during non working office hours?
•	 
Use regular VM instances for the production environment
Use preemptible VMs for non production environments.
(Incorrect)
•	 
Create Cloud Functions to stop the non production environments after office hours and start them just before office hours.
Use Cloud Scheduler to invoke these Cloud Functions
(Correct)
•	 
Create a shell script that uses the gcloud command to change the machine type of the non production VM instances to a smaller machine type outside of office hours.
Create a cron job on one of the production environment VM instance that will execute this shell script at regular scheduled time to automate the task
•	 
Deploy the non production VM instances using a Managed Instance Group and enable autoscaling.
Explanation
Option A is incorrect because preemptible VM can be stopped any time with short notice of 30 seconds. So in this case it is not good choice even for development and staging environment as well.
Option C is incorrect because even if machine type is changed after office hours, it will still incur some cost and It's not good practice to schedule such script on production environment. Ideally production environment should not have access to any other environment. So this script will not work even if it is scheduled on production environment.
Option D look like a close call but it is also incorrect because minimum number of instances in case of autoscaling group is one.
Option B is most accurate choice here because it will stop all the VMs in development and staging environment after office hours and start them just before office hours.
Question 15: Incorrect
Your company has an enterprise application running on GCE VM instances. This application has requirement of high availability and high performance. The application has been deployed on two instances in two zones in the same region where one instance is active instance and other instance runs in passive mode.
This application writes data to a persistent disk and you need to make this application fault tolerant so that if there is a zonal failure in one zone, then data should be immediately made available to the other instance running in 2nd zone.
As Google Cloud architect how can you maximize performance of this application with minimum downtime and minimum data loss?
•	 
1. Attach a regional SSD persistent disk to the first instance.
2. In case of a zone outage, force-attach the disk to the other instance.
(Correct)
•	 
1. Create a GCS bucket.
2. Mount this bucket into the first instance with gcs-fuse.
3. If zonal failure happens, then mount the same GCS bucket to the second instance with gcs-fuse
(Incorrect)
•	 
1. Create a persistent SSD disk for first instance.
2. Create a snapshot of this SSD persistent disk every 30 minutes.
3. If zonal failure happens, then recreate a persistent SSD disk from last snapshot and attach that disk with the second instance running in 2nd zone
•	 
1. Use a local SSD disk with the first instance.
2. Attach a persistent disk with 2nd VM instance running in 2nd zone.
2. Execute rsync command every 30 minutes where the target is a persistent SSD disk attached with 2nd VM instance
3. If zonal failure happens, then use the second instance.
Explanation
Option C and Option D are incorrect because in both cases, 30 minutes of data can still be lost because snapshot or rsync happen at an interval of 30 minutes.
Option B is incorrect because using Cloud Storage Bucket as disk via gcs-fuse is not a good choice as it will be very slow as compare to persistent disk
Option A is correct choice here because in case of regional persistent disk, data is automatically copied in multiple zones. In case of zone outage; just start a new VM in another zone and force-attach the existing disk; will work perfectly fine.
Question 16: Correct
Your team want to deploy a web application in GKE cluster. You need to ensure that application is highly scalable so that it can handle the high number of concurrent users.
How can you ensure that latency of this application stays below a certain threshold?
•	 
Do load testing of application for expected number of concurrent users via load testing tool and inspect the results.
•	 
Enable autoscaling on the GKE cluster and enable horizontal pod autoscaling on your application deployments.
Validate the autoscaling of cluster via curl request.
(Correct)
•	 
Create multiple GKE cluster in every cloud region in GCP and deploy application in each cluster.
Use global HTTP(S) load balancer to load balance the traffic among all clusters.
•	 
Use Service Mesh to understand the latency between the different microservices.
Explanation
Option B is correct choice here because once horizonal pod autoscaling is configured then GKE cluster will automatically adjust number of pods required for your application based on the application traffic.
When you first deploy your workload to a Kubernetes cluster, you may not be sure about its resource requirements and how those requirements might change based on usage patterns, external dependencies, or other factors.
Horizontal Pod autoscaling helps to ensure that your workload functions consistently in different situations, and allows you to control costs by only paying for extra capacity when you need it.
It's not always easy to predict the indicators that show whether your workload is under-resourced or under-utilized. The Horizontal Pod Autoscaler can automatically scale the number of Pods in your workload.

There is no need to deploy same application on multiple cluster like Option C. It is unnecessary pain.

Please read following reference for more information
https://cloud.google.com/kubernetes-engine/docs/concepts/horizontalpodautoscaler
Question 17: Correct
Your company has developed a recommendation engine for its customers which is exposed via a REST API for its customers where user will provide a user ID as input and the REST API will return a list of recommendations as output for the given user ID.
You need to manage the API lifecycle. You also need to ensure stability for your customers in case the API makes backward-incompatible changes.
As a Google Cloud architect what solution you would like to suggest as per Google Cloud recommendations?
•	 
Create a distribution list of all customers.
Use this distribution list to inform customers of an upcoming backward incompatible change at least one month before replacing the old API with the new API.
•	 
Use a versioning strategy that will add the suffix 'DEPRECATED' to the current API version number in case API changes are backward-incompatible.
Use the current version number for the new API.
•	 
Use a versioning strategy for the APIs that increases the version number on every backward incompatible change.
(Correct)
•	 
Generate API documentation in automated manner.
Set up CI/CD pipeline in way, that will update the public API documentation whenever new API is deployed.
Explanation
Option C is correct choice here because increasing the version number will inform client that new API might introduce breaking changes.

Please read following references for more information
https://cloud.google.com/apis/design/compatibility
https://cloud.google.com/apis/design/versioning
Question 18: Incorrect
Your company is developing a microservices application on GKE cluster.  You need to simulate and validate how application will respond in case one of the microservice crash all of a sudden.
As Google Cloud Architect, what solution you would like to suggest?
•	 
Destroy one node in GKE cluster to validate required failure scenario
•	 
Configure Istio in GKE cluster.
Use traffic management feature of Istio to divert the traffic away from a crashing microservice.
(Incorrect)
•	 
Add bug in one of the microservice code and redeploy that service in cluster
•	 
Use Istio's fault injection feature.
With this fault injection feature, you can simulate the behavior where one of the microservice is faulty
(Correct)
Explanation
Istio is an open service mesh that provides a uniform way to connect, manage, and secure microservices. It supports managing traffic flows between services, enforcing access policies, and aggregating telemetry data, all without requiring changes to the microservice code.
Istio gives you:
Automatic load balancing for HTTP, gRPC, WebSocket, MongoDB, and TCP traffic.
Fine-grained control of traffic behavior with rich routing rules, retries, failovers, and fault injection.
A configurable policy layer and API supporting access controls, rate limits, and quotas.
Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress, and egress.
Secure service-to-service communication in a cluster with strong identity based authentication and authorization.
Istio on GKE is a tool that provides automated installation and upgrade of Istio in your GKE cluster. When you upgrade GKE, the add-on is automatically upgraded to the most recent GKE-supported version of Istio. This lets you easily manage the installation and upgrade of Istio as part of the GKE cluster lifecycle.

Please read the following references for more information
https://cloud.google.com/istio/docs/istio-on-gke/overview#what_is
https://istio.io/latest/docs/tasks/traffic-management/fault-injection/
Question 19: Incorrect
For a web application, how can you ensure that production deployment of application is linked to source code commits?
This deployment process should be fully auditable.
•	 
All developers must follow practice of tagging the code commit with commit timestamp.
•	 
All developers must follow practice of adding a comment to the commit that tells which commit is used for a particular commit.
(Incorrect)
•	 
Make sure that container tag match the source code commit hash
(Correct)
•	 
All developers must follow practice of tagging the code commits with :latest
Explanation
Option A , Option B and Option D are just adding a tag or comment on source code only. This way we can not establish the relationship between deployment (container image) and source code commit after which this deployment was built.
Relationship between deployment (container image) and source code commit can be established by adding a tag to container image with tag value as source code commit.
Question 20: 
Skipped
Your team needs to deploy application on Google Kubernetes Engine (GKE) cluster. This application need access to third-party services on the internet.
As per company policy, no Compute Engine VM instance can have a public IP address in GCP.
How can you create a deployment strategy that meets these guidelines?
•	 
Configure the GKE cluster as a private cluster.
Configure Cloud NAT Gateway for the cluster subnet.
(Correct)
•	 
Configure the GKE cluster as a private cluster.
Configure Private Google Access in the VPC in which cluster nodes are created.
•	 
Configure the Kubernetes cluster as a route-based cluster.
Configure Private Google Access on the VPC in which cluster nodes are created.
•	 
Install a NAT Proxy on one Compute Engine VM Instance.
Configure all workloads on Kubernetes in a way that all request for third-party services on internet pass through this proxy.
Explanation
Private clusters
By default, all nodes in a GKE cluster have public IP addresses. A good practice is to create private clusters, which gives all worker nodes only private RFC 1918 IP addresses. Private clusters enforce network isolation, reducing the risk exposure surface for your clusters. Using private clusters means that by default only clients inside your network can access services in the cluster. In order to allow external services to reach services in your cluster, you can use an HTTP(S) load balancer or a network load balancer.

Private clusters in GKE gives you the ability to isolate nodes from having inbound and outbound connectivity to the public internet. This isolation is achieved as the nodes have internal IP addresses only.
If you want to provide outbound internet access for certain private nodes, you can use Cloud NAT or manage your own NAT gateway.

 
Please read the following references for more information
https://cloud.google.com/architecture/prep-kubernetes-engine-for-prod
https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept#using_in_private_clusters
https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters
Question 21: 
Skipped
Your company wants to store some critical data in a GCS bucket. and your company needs to rotate the encryption key used to encrypt the data in the bucket. This data will be processed in Cloud Dataflow.
As per Google Cloud's recommendation for the security, what solution would you like to suggest for this use case?
•	 
Create a key with Cloud Key Management Service (KMS).
Use encrypt method of Cloud KMS to encrypt the data.
•	 
Create a key with Cloud Key Management Service (KMS).
Set the encryption key on the bucket to the Cloud KMS key.
(Correct)
•	 
Generate a symmetric key.
Encrypt the data using above key.
Upload the encrypted data to the GCS bucket.
Use same key to decrypt data at the time of data read.
•	 
Generate an AES-256 encryption key.
Encrypt the data in the bucket using the customer-supplied encryption key feature.
Explanation
All the data on Cloud Storage Bucket is encrypted. By default this encryption happens via Google Managed encryption keys.  But in place of using Google Managed encryption keys, you can choose to use keys generated by Cloud Key Management Service. Such keys are known as customer- managed encryption keys. You can use customer-managed encryption keys on individual objects, or configure your bucket to use a key by default on all new objects added to a bucket.

If you use a customer-managed encryption key, your encryption keys are stored within Cloud KMS. The project that holds your encryption keys can then be independent from the project that contains your buckets, thus allowing for better separation of duties.

Please read the following references for more information
https://cloud.google.com/storage/docs/encryption/customer-managed-keys
Question 22: 
Skipped
Your company wants to do outsourcing of operation functions. They want their developers to deploy new versions of application in staging environment for verification purpose. After verification in staging environment, they want to allow the outsourced operations team to promote this verified deployment version to production environment.
Which GCP products should you use to minimize the operation overhead of the solution?
•	 
App Engine
(Correct)
•	 
GKE On-Prem
•	 
Compute Engine
•	 
Google Kubernetes Engine
Explanation
App engine has less operation overhead as compare to any other option.
GKE and Compute engine services are more flexible but has more operation overhead.
Question 23: 
Skipped
Your company needs to design a Data Warehousing solution in GCP. Your team wants to store secured data in BigQuery. Your team requires you to have the encryption keys generated outside GCP.
As a cloud architect what solution would you like to suggest for this?
•	 
Generate a new key in Cloud Key Management Service (Cloud KMS).
Store all the data in GCS Bucket using the customer-managed key and select the key created in previous step.
Set up dataflow processing pipeline to read data from GCS bucket, decrypt this data and then store it in a new BigQuery dataset.
•	 
Generate a new key in Cloud Key Management Service (Cloud KMS).
Create a dataset in BigQuery using the customer-managed key and select the key created in previous step.
•	 
Import a key in Cloud Key Management Service (Cloud KMS).
Store all the data in GCS Bucket using the customer-managed key and select the key created in previous step.
Set up a Dataflow processing pipeline to read data from GCS bucket, decrypt this data and then store it in a new BigQuery dataset.
•	 
Import a key in Cloud Key Management Service (Cloud KMS).
Create a dataset in BigQuery using the customer-managed key and select the key created in previous step.
(Correct)
Explanation
BigQuery encrypts customer content stored at rest. BigQuery handles and manages this default encryption for you without any additional actions on your part.
If you want to control encryption yourself, you can use customer-managed encryption keys (CMEK) for BigQuery. Instead of Google managing the encryption keys that protect your data, you control and manage encryption keys in Cloud KMS

Please read the following reference for more information
https://cloud.google.com/bigquery/docs/customer-managed-encryption

Since key needs to be generated outside GCP, so we can import that key in KMS to be used by BigQuery.

Question 24: 
Skipped
Your company has a project in GCP with 3 VPCs. There is one VM instance in each VPC.
Network subnets inside VPC do not overlap with each other and these subnets must remain separated.
This network configuration is as following
 
For your company, Instance1 is a special case where this instance must communicate directly with other 2 instances via internal IPs.
As cloud architect, what solution would you like to suggest?
•	 
Add two additional (Network Interface Card) NICs to instance1 with the following configuration
1. NIC1
          VPC: VPC2
          Subnetwork: Subnet2
2. NIC2
          VPC: VPC3
          Subnetwork: Subnet3 
Update firewall rules in VPC to enable
1. traffic between instances1 and Instance 2 via NIC1.
2. traffic between instances1 and Instance 3 via NIC2.
(Correct)
•	 
Create one cloud router between subnet1 and subnet2 to advertise IP addresses of subnet2 to subnet#1
Create 2nd cloud router between subnet1 and subnet3 to advertise IP addresses of subnet3 to subnet#1
•	 
Create two VPN tunnels via Cloud VPN
1 Between VPC#1 and VPC#2
1 Between VPC#2 and VPC#3
Update firewall rules to enable traffic between instances. 
•	 
Establish VPC Peering connection as following
Peer VPC#1 with VPC#2.
Peer VPC#2 with VPC#3
Update firewall rules to allow traffic between instances. 
Explanation
Multiple NIC can be attached with a VM instance and each NIC can be assigned different IP. By adding 2 NIC as per Option A, instance 1 will also be considered as an instance in VPC#2 and VPC3.
Via NIC1 , instance# 1 and instance#2 can communicate via internal IP of subnet 2
Via NIC2 , instance# 1 and instance#3 can communicate via internal IP of subnet 3

Cloud Router option (Option B) is not right choice, because use of Cloud router alone is not sufficient. To enable communication between VM's via private IP, either they must be in same subnet of same VPC or they must be connected via VPN gateway or Cloud Interconnect if those VM's are in different VPC.  Since this question did not mention anything about VPN or Cloud Interconnect, so Option B is not right choice here
Question 25: 
Skipped
Your company has an application deployed in a GKE cluster. You have three clusters for development, staging and production environments.
Your operation team found that the team can do application deployment to the production cluster without first testing the application in lower environments like development and staging environment.
As operation team head, You want team to be autonomous but want to prevent the direct production deployment.
As Google Cloud architect, what solution you can suggest to achieve this quickly with less effort?
•	 
First, configure the binary authorization policies for all GKE clusters, development. staging and production clusters.
Set up a CI/CD pipeline that require the attestations as part of the pipeline processing.
(Correct)
•	 
Create an IAM group of all team leads
Create a policy to allow team leads in this group to do deployment on production GKE cluster
•	 
Create a GKE lifecycle hook to prevent the container from starting if container image is not approved for usage in the given environment.
•	 
Implement a corporate policy to prevent teams from deploying Docker images to an environment unless the Docker image was tested in an earlier environment.
Explanation
Binary Authorization is a deploy-time security control that ensures only trusted container images are deployed on Google Kubernetes Engine (GKE) or Cloud Run. With Binary Authorization, you can require images to be signed by trusted authorities during the development process and then enforce signature validation when deploying. By enforcing validation, you can gain tighter control over your container environment by ensuring that only verified images are integrated into the build-and-release process.

Please read the following reference for more information
https://cloud.google.com/binary-authorization
Question 26: 
Skipped
You company wants to use Cloud SQL MySQL database for business critical transaction data.
Which two features should you implement to ensure that the minimum amount of data is lost in case of disaster event?
•	 
Sharding
•	 
Read replicas
(Correct)
•	 
Binary logging
•	 
Automated backups
(Correct)
•	 
Semisynchronous replication
Explanation
Sharding is a way to distribute data among multiple database nodes in a way that each node contains subset of data. It makes data read and write faster. Sharding can not help in controlling data loss

In case of disaster, to ensure minimum data loss must to following things
1. Run Cloud SQL in HA mode. It will provide high availability in case zonal outage.
2. Take Regular backup of data. It will help in creating a new instance faster
3. Create cross region read replica. It will ensure minimum data loss in case of disaster event.

We can following architecture for disaster recovery in GCP
Before Disaster
 

After Disaster Recovery
 

Prerequisites for creating a read replica
1. Before you can create a read replica of a primary Cloud SQL instance, the instance must meet the following requirements:
Automated backups must be enabled.
2. Binary logging must be enabled which requires point-in-time recovery to be enabled. Learn more about the impact of these logs.
3. At least one backup must have been created after binary logging was enabled.

It means creating read replica already covers binary logging.

Please read the following references for more information
https://cloud.google.com/solutions/cloud-sql-mysql-disaster-recovery-complete-failover-fallback
https://medium.com/google-cloud/cloud-sql-recovering-from-regional-failure-in-10-minutes-or-less-mysql-fc055540a8f0
Replication in Cloud SQL | Cloud SQL for MySQL | Google Cloud
Question 27: 
Skipped
Your company is using BigQuery in Google Cloud Project. Company's on-premise environment is connected to Google Cloud via VPN tunnel.
What solution can you suggest to ensure security in this system to avoid data exfiltration and accidental oversharing?
•	 
Configure VPC Service Controls and configure Private Google Access
(Correct)
•	 
Configure Private Google Access for on-premises only
•	 
Perform the following task
1. Create a service account.
2. Give the role to run BigQuery Jobs to the service account.
3. Remove all other IAM access from the project
•	 
Use Private Google Access.
Explanation
VPC Service Control helps in following things
1. Mitigate exfiltration risks by isolating multi-tenant services
2. Ensure sensitive data can only be accessed from authorized networks
3. Restrict resource access to allowed IP addresses, identities, and trusted client devices
4. Control which Google Cloud services are accessible from a VPC network

Private Google Access
Private Google Access offers private connectivity to hosts either in a VPC network or on-premises network that use private IP addresses to access Google APIs and services.
VM instances that only have internal IP addresses (no external IP addresses) can use Private Google Access. They can reach the external IP addresses of Google APIs and services. The source IP address of the packet can be the primary internal IP address of the network interface or an address in an alias IP range that is assigned to the interface.

Please read the following references for more information
Private Google Access with VPC Service Controls
Private Google Access
VPC Service Controls

Question 28: 
Skipped
Your company has a simple application which is deployed as single pod on GKE cluster. This application pulls messages from Pub/Sub and does some heavy I/O operation while processing messaging.
Your found that the application is slow in processing these messages after analyzing Pub/Sub metrics. Application is not able to process the messages in real time. Most of messages have to wait before being pulled by application for processing.

What solution can you suggest to fix this issue?
•	 
Use the --enable-autoscaling flag while creating GKE cluster
•	 
Use subscription/push_request_latencies metric to configure autoscaling of GKE cluster deployment
•	 
Use subsription/num_undelivered_messages metric to configure autoscaling of GKE cluster deployment
(Correct)
•	 
Configure GKE cluster deployment autoscaling based on CPU utilization.
Also configure minimum and maximum limit on number of nodes in this cluster.
Explanation
subscription/push_request_latencies metric helps you in understanding your push endpoint's response latency distribution. Because of the limit on the number of outstanding messages, endpoint latency affects subscription throughput. If it takes 100 milliseconds to process each message, your throughput limit is likely to be 10 messages per second.
With this metrics we can not determine how much scaling is needed. Because processing time of each subscriber will still remain nearly same even after scaling.
Ideally our subscriber should consume message from subscription as soon as possible. Pub/Sub generally limits the number of outstanding messages. You should aim for fewer than 1000 outstanding messages in most situations. If we increase our subscriber count then number of undelivered messages count will go down accordingly. So Option C is better choice in this case.
 

Please read the following references for more information
Pub/Sub Monitoring
Question 29: 
Skipped
Your company's on-premises data center is connected with Google cloud via 1Gbps connection bandwidth. You need to migrate their 10TB on-premises database into GCS bucket.
What solution will you recommend to perform this data export.? Your goal is to minimize the data transfer cost and minimize load on database in on-premises data center.
•	 
Develop a dataflow job that will read data from database and write it into GCS bucket.
•	 
Use a Hadoop job in Dataproc cluster solution that will read data from database and write it into GCS bucket.
•	 
Do offline data migration by using Data Transfer appliance.
•	 
Zip the input data and upload it with gsutil -m command.
It will copy data faster by doing multi-threaded copy.
(Correct)
Explanation
The two main criteria to consider with Transfer Appliance are cost and speed. With reasonable network connectivity (for example, 1 Gbps), transferring 100 TB of data online takes over 10 days to complete. If this rate is acceptable, an online transfer is likely a good solution for your needs.

The expected turnaround time for a network appliance to be shipped, loaded with your data, shipped back, and rehydrated on Google Cloud is 50 days. If your calculated online transfer timeframe is substantially more than this timeframe, consider Transfer Appliance. The total cost for the 480 TB device process is less than $3,000.

 
As per this table it will take around 30 hours to migrate data with 1Gbps which very less as compare to turn around time of Transfer appliance.

Please read the following references for more information
Google Cloud Data Transfer Options
Question 30: 
Skipped
One team in your company has been running one application in Google Cloud.  This team has recently started using Cloud Pub/Sub with this application. Due to increase in load on the application servers, this application has started giving many timeout error. This application is not logging any Pub/Sub publishing errors. You want to improve publishing latency.
As Google Cloud architect what will you suggest in this case?
•	 
Create more Pub/Sub message queues
•	 
Use Push model of Pub/Sub for subscription
•	 
Increase the Pub/Sub total timeout retry value
•	 
Stop using Pub/Sub message batching
(Correct)
Explanation
Batching messages
The Pub/Sub client libraries batch multiple messages into a single call to the service. Larger batch sizes increase message throughput (rate of messages sent per CPU). The cost of batching is latency for individual messages, which are queued in memory until their corresponding batch is filled and ready to be sent over the network. To minimize latency, batching should be turned off. This is particularly important for applications that publish a single message as part of a request-response sequence.

Please read the following references for more information
Publishing Message to Cloud Pub/Sub
Question 31: 
Skipped
How can you ensure that use of Google Cloud for health care industry will comply with an upcoming privacy compliance audit?
•	 
Implement Prometheus to detect and prevent security breaches on your web-based applications.
•	 
Verify list of Google cloud products to be used in your application against the list of compliant product on the Google Cloud compliance page
(Correct)
•	 
Use Firebase Authentication for your user facing applications.
•	 
You should execute a Business Associate Agreement(BAA) with Google Cloud
(Correct)
•	 
Use Google Kubernetes Engine private cluster for all containerized applications.
Explanation
Under HIPAA, certain information about a person’s health or health care services is classified as Protected Health Information (PHI). Google Workspace and Cloud Identity customers who are subject to HIPAA and wish to use Google Workspace or Cloud Identity with PHI must sign a Business Associate Agreement (BAA) with Google.
Reference:
https://support.google.com/a/answer/3407054?hl=en

Customers that are subject to HIPAA and want to utilize any Google Cloud products in connection with PHI must review and accept Google's Business Associate Agreement (BAA). Google ensures that the Google products covered under the BAA meet the requirements under HIPAA and align with our ISO/IEC 27001, 27017, and 27018 certifications and SOC 2 report.
Reference
https://cloud.google.com/security/compliance/hipaa-compliance

For other compliances please check Google compliance page at following link
https://cloud.google.com/security/compliance


Question 32: 
Skipped
While deploying containerized application on GKE cluster, how can you ensure that only verified containers are deployed using Google Cloud services?
•	 
Use service account to create and deploy containers from container registry.
•	 
Set up a Jenkins job that will cryptographically sign a container as part of a CI/CD pipeline.
•	 
Enable Binary Authorization on GKE, and sign containers as part of a CI/CD pipeline.
(Correct)
•	 
Use Vulnerability Scanning feature of Container Registry before deploying the workload. Don't deploy the workload if any vulnerability is found during this scanning.
Explanation
Binary Authorization is a deploy-time security control that ensures only trusted container images are deployed on Google Kubernetes Engine (GKE) or Cloud Run. With Binary Authorization, you can require images to be signed by trusted authorities during the development process and then enforce signature validation when deploying. By enforcing validation, you can gain tighter control over your container environment by ensuring only verified images are integrated into the build-and-release process.

Please read the following references for more information
https://cloud.google.com/binary-authorization
https://cloud.google.com/architecture/binary-auth-with-cloud-build-and-gke
Question 33: 
Skipped
In your company, public IP addresses was assigned by mistake to backend application servers. This public IP address assignment made these backend servers accessible from the internet.
In your organization how can you ensure that public IP addresses can not be assigned to backend server by any one. External IP addresses should be assigned to frontend servers only.
•	 
Create an Organizational Policy with a constraint that will allow public IP addresses only on the frontend servers only.
(Correct)
•	 
Create VM instances using Deployment Manager and do manual review of deployment configuration yaml file to ensure that public IP is not assigned to VM instances
•	 
Create an identity and Access Management (IAM) policy that will revoke permission to create public IP address in your project
•	 
Create a custom Identity and Access Management (IAM) role that will revoke permission to create public IP address
Create a policy at project level that associate this role with all users in your project.
Explanation
Rules created at Organization Policy level have highest priority. These rules even have more priority than IAM policy defined at other resource level like Project, Folder and  Organization.
So if we define deny rule as Organization Policy with a constraint to allow external IP addresses only on the frontend Compute Engine instances then no one can assign public IP address to any other instance even by mistake.

Please read the following references for more information
Organization Policy Constraints
Question 34: 
Skipped
There is a single Dedicated Interconnect connection between your primary on-premises data center and Google Cloud. This connection should adheres to  following policies:
1. Any on-premise server with private IP should connect to cloud resources via private IP of cloud resources.
2. Egress traffic from production network management servers to GCE virtual machines should never use public internet.
3. Network should meet SLA (Service level Agreement) of any business critical application.
What solution will you suggest to meet these requirements.
•	 
Add a new Dedicated Interconnect connection.
(Correct)
•	 
Upgrade the bandwidth on the Dedicated Interconnect connection to 100G
•	 
Add four new Cloud VPN connections.
•	 
Use a new Carrier Peering connection.
Explanation
Dedicated Interconnect provides direct physical connections between your on-premises network and Google's network. Dedicated Interconnect enables you to transfer large amounts of data between networks, which can be more cost-effective than purchasing additional bandwidth over the public internet.

For the highest level availability, it is recommended to set up configuration for 99.99% availability as the base configuration, as shown in the following diagram. Clients in the on-premises network can reach the IP addresses of virtual machine (VM) instances in the us-central1 region through at least one of the redundant paths. If one path is unavailable, the other paths can continue to serve traffic.
      
As per Google recommendation:
With 100 Gbps Dedicated Interconnect, you can scale your connection capacity to meet your particular requirements. Connect up to 2x 100G transport circuits for private cloud traffic to Google Cloud at any of our POPs.

Please read the following references for more information
Dedicated Interconnect Overview
Hybrid Connectivity
Question 35: 
Skipped
Your company needs to design Google Cloud network architecture for GKE cluster. This cluster will be used to run business critical internet facing application. As per Google Cloud's best practices, how will you design this network so that you can manage this cluster from your data center and there is very less chances of attack from malicious attacker?
•	 
Create a public cluster with master authorized networks enabled and firewall rules.
•	 
Create a public cluster with firewall rules and Virtual Private Cloud (VPC) routes
•	 
Create a private cluster with a private endpoint with master authorized networks configured.
•	 
Create a private cluster with public endpoint with master authorized networks configured.
(Correct)
Explanation
In a private cluster, nodes only have internal IP addresses, which means that nodes and Pods are isolated from the internet by default.
In private clusters, the control plane (master) has a private and public endpoint. There are three configuration combinations to control access to the cluster endpoints:

1. Public endpoint access disabled: creates a private cluster with no client access to the public endpoint.
2. Public endpoint access enabled, authorized networks enabled: creates a private cluster with limited access to the public endpoint.
3. Public endpoint access enabled, authorized networks disabled: creates a private cluster with unrestricted access to the public endpoint.

As per this, Option D is the right choice because with this Option D, we can create cluster where all nodes and pods will be assigned private IP only and master control can be accessed only from authorized network (means inside Google Cloud VPC and from on-premises network)
Question 36: 
Skipped
Your company wants to scale existing business critical application. They have customers across the globe. For this application, GKE cluster will be used to host the application. Currently, company is not interested in migrating its existing MySQL on-premises database to cloud. So application servers need high speed, consistent and highly available connectivity between their on-premises data center and Google Cloud to access on-premises database.
As per Google's recommended practices for production-level applications, what solution will you suggest for hybrid connectivity between your company's on-premises systems and Google Cloud?
•	 
Configure two dedicated interconnect connections in one metro (City) and two connections in another metro.
Ensure that Interconnect connections are placed in different metro zones.
(Correct)
•	 
Configure two Partner interconnect connections in one metro (City).
Ensure that Interconnect connections are placed in different metro zones.
•	 
Configure Direct Peering between your data center and Google Cloud.
Ensure that you are peering at least two Google locations.
•	 
Configure two or more VPN connections from on-premises to Google Cloud.
Ensure that VPN devices in on-premises data center are in separate racks.
Explanation
For the highest level availability, Google recommend the configuration for 99.99% availability as the base configuration, as shown in the following diagram. Clients in the on-premises network can reach the IP addresses of virtual machine (VM) instances in the us-centra-1 region through at least one of the redundant paths. If one path is unavailable, the other paths can continue to serve traffic.

 

Only Option A follows this architecture. Also for consistency and high speed requirement, Dedicated connection is a preferred choice.

Please read the following references for more information
Dedicated Interconnect Overview
Question 37: 
Skipped
You want to build an application and you are interested in using serverless Cloud Functions for backend services.
In your application, you have requirements that cloud Function functionA should be able to invoke another Cloud Function functionB.
As per Google Cloud recommendation, what solution will you suggest to ensure that functionB accept request from functionA only.
•	 
Make functionB 'Require authentication'
Make functionB only accept internal traffic.
Create both Cloud Functions in the same VPC.
Create an ingress firewall for functionB to only allow traffic from functionA .
•	 
Make functionB 'Require authentication'.
Create a unique service account and associate it to functionA.
Grant the service account invoker role for functionB .
Create an id token in functionA and pass the token with request when invoking functionB.
(Correct)
•	 
Create a token and pass it in as an environment variable to functionA.
While invoking functionB, include the token in the request.
Pass the same token to functionB
Reject the invocation if the tokens are different.
•	 
Create two functions in the same project and VPC.
Make functionB only accept internal traffic.
Create an ingress firewall for functionB to only allow traffic from functionA.
Ensure that both functions use the same service account.
Explanation
There are two approaches to controlling access for Cloud Functions:
Identity-based
1. Evaluating credentials that the entity presents to ensure that it is who it says it is (Authentication).
2. Allowing that entity to access your resources based on what permissions that identity has been granted (Authorization).
Network-based
You can also limit access by specifying network settings for individual functions. This allows for fine-tuned control over the network ingress and egress to and from your functions.
Please read the following references for more information on this
https://cloud.google.com/functions/docs/securing
https://cloud.google.com/functions/docs/networking/network-settings

-------------------------------------------------------------------------------------------------------------------------------
Authenticating function to function calls
When building services that connect multiple functions, it's a good idea to ensure that each function can only send requests to a specific subset of your other functions. For instance, if you have a login function, it should be able to access the user profiles function, but it probably shouldn't be able to access the search function.

To configure the receiving function to accept requests from a specific calling function, you need to add the calling function's service account as a member on the receiving function and grant that member the Cloud Functions Invoker (roles/cloudfunctions.invoker) role.

Please read the following references for more information on this
Authenticating Function To Function Calls
------------------------------------------------------------------------------------------------------------------------------------------
As per above explanation, we can meet our requirement from. Option B alone.


Question 38: 
Skipped
Your company has a legacy web application running in on-premises data center. You want to use Google Cloud to monitor this application.
In case of application failure or during maintenance, you want to redirect users to a "Application is not available" page as soon as possible. Your Operation team should be able to receive a notification for the issue.
What solution will you suggest for this requirement? You also have goal to minimize the cost.
•	 
Use Cloud Error Reporting to check the application URL.
In case of failure, switch the URL to the "Application is not available" page
Notify the Operation team about this incident.
•	 
Create a Cloud Monitoring uptime check to validate the application URL.
In case of health check failure, send a message to Pub/Sub queue.
Create a Cloud Function that subscribe to this event.
This Cloud Function will switch the URL to the "Application is not available" page, and notify the Operation team.
(Correct)
•	 
Create a cron job on a GCE VM instance that runs every 30 seconds.
The cron job invokes a shell script to check the application URL.
In case of failure, switch the URL to the "Application is not available" page, and notify the Operation team.
•	 
Create a schedule job in Cloud Run to invoke a container every 30 seconds.
The container will check the application URL.
In case of failure, switch the URL to the "Application is not available" page, and notify the Operation team.
Explanation
An uptime check is a request sent to a resource to see if it responds. You can use uptime checks to determine the availability of a VM instance, an App Engine service, a URL, or an AWS load balancer.
You can monitor the availability of a resource by creating an alerting policy that creates an incident if the uptime check fails. The alerting policy can be configured to notify you by email or through a different channel, and that notification can include details about the resource that failed to respond. You also have the option to observe the results of uptime checks in the Monitoring uptime-check dashboards.

Please read the following references for more information
Uptime Check
Question 39: 
Skipped
Your company has about 1 petabyte (PB) of critical data in on-premises data center. You want to export this data to GCS bucket . There is a 1-Gbps interconnect link between on-premises data center and Google Cloud. Other team wants to use data stored in GCS bucket after one month.
What solution will you suggest for such data export?
•	 
Use Transfer Appliances from Google Cloud to export the data to Google's Cloud storage bucket.
(Correct)
•	 
Export files to an encrypted USB device
Send the device to Google
Request Google cloud for import of the data to Cloud Storage bucket from this USB device.
•	 
Ensure that there are no other users consuming the 1Gbps link
Use multi-thread transfer using gcloud -m command line utility to transfer data to Cloud Storage.
•	 
Use the Storage Transfer service from Google Cloud to export the data from on-premises data center to Cloud Storage bucket.
Explanation
With 1Gbps, it can take 124 or more number of days to transfer 1 PB of data, so gsutil and Storage Transfer service is not right choice here in this case because both are dependent on network bandwidth.

So only correct choice for this case is Option A, which suggest to request for Transfer Appliance.

Please read the following references for more information
Data Transfer Options
Question 40: 
Skipped
You want to migrate on-premises Linux-based virtual machines to Google Cloud. Your infrastructure team sent you several recent Linux vulnerabilities published by Common Vulnerabilities and Exposure (CVE).
How can you assess these Vulnerabilities that can affect your VM migration?
•	 
Open a support case regarding the CVE
Chat with the Google Cloud support team.
(Correct)
•	 
Post a query related to CVE in a Google Cloud discussion forum to get more information about this.
•	 
Check the Google Cloud Status Dashboard to understand about CVEs that can affect VM migration.
•	 
Check Google Cloud Platform Security Bulletins to understand about CVEs that can affect VM migration.
(Correct)
•	 
Post your CVE related query on other internet forums like Stack Overflow to understand it in better way.
Explanation
Google cloud publish CVE report from time to time on its security Bulletin

Please read the following references for more information
Google Cloud security Bulletin
Question 41: 
Skipped
Your company wants to build microservice based application. Your team want to deploy this application using Docker containers.
How would you like to set up pipeline that can store the build artifacts and can do CI/CD for your application?
•	 
Create a Cloud Build trigger for new source code changes.
The trigger invokes build jobs and build container images for the microservices.
Tag the images with a version number, and push them to Cloud Storage.
•	 
Create a Cloud Build trigger for new source code changes.
Invoke Cloud Build that will first build the container image, and then tag the image with the label 'latest;.
Push that image to Container Registry.
•	 
Create a Cloud Build trigger for new source code changes.
Invoke Cloud Build that will first build the container image, and then tag the image with the code commit hash.
Push that images to the Container Registry.
(Correct)
•	 
Create a Scheduler job to check the code repository every minute.
For any new change, invoke Cloud Build to build container images for the microservices.
Tag the images with current timestamp.
Push container image to Container Registry.
Explanation
Here Option B and Option C are very close but I will prefer Option C more.
With this I can directly map any deployment with source code commit used to build this deployment.

CI/CD pipeline for GKE
Question 42: 
Skipped
Your company wants to convert a monolithic application into RESTful microservices. Your team want to run those microservices on Cloud Run by using Docker containers. Your application has customer across the globe.
How can you ensure that these services are highly available? Also, customers expect low latency from your solution?
•	 
Deploy Cloud Run services in multiple regions.
Create serverless network endpoint groups pointing to the services.
Use serverless NEG's as a backend service for the global HTTP(S) Load Balancer.
(Correct)
•	 
Deploy Cloud Run services to multiple availability zones.
Create Cloud Endpoints that point for these services.
Create a global HTTP(S) Load Balancer.
Attach the Cloud Endpoints as backend service for above load balancer.
•	 
Deploy Cloud Run services in multiple regions.
Configure Cloud DNS to route traffic based on latency.
•	 
Deploy Cloud Run services in multiple availability zones.
Create a TCP/IP global load balancer.
Add the Cloud Run Endpoints as backend services for above load balancer.
Explanation
This is how a serverless NEG fits into the HTTP(S) Load Balancing model.
 

A network endpoint group (NEG) specifies a group of backend endpoints for a load balancer. A serverless NEG is a backend that points to a Cloud Run, App Engine, or Cloud Functions service.
A serverless NEG can represent one of the following:
1. A Cloud Run service or a group of services sharing the same URL pattern.
2. A Cloud Functions function or a group of functions sharing the same URL pattern.
3. An App Engine app (Standard or Flex), a specific service within an app, a specific version of an app, or a group of services sharing the same URL pattern.
When HTTP(S) Load Balancing is enabled for serverless apps, you can:
Configure your serverless app to serve from a dedicated IPv4 and/or IPv6 IP address that is not shared with other services.
Map a single URL to multiple functionally-identical serverless applications running in different regions, allowing requests to be routed to the region closest to the user. This is supported only for Cloud Run and Cloud Functions.
Reuse the same SSL certificates and private keys that you use for Compute Engine, Google Kubernetes Engine and Cloud Storage. This eliminates the need to manage separate certificates for serverless apps.

Endpoint types
Serverless NEGs do not have any network endpoints such as ports or IP addresses. They can only point to an existing Cloud Run, App Engine, or Cloud Functions service residing in the same region as the NEG.
When you create a serverless NEG, you specify the fully-qualified domain name (FQDN) of the Cloud Run, App Engine, or Cloud Functions service. The endpoint is of type SERVERLESS. Other endpoint types are not supported in a serverless NEG.
A serverless NEG cannot have more than one endpoint. Because only one endpoint is allowed in each serverless NEG, the load balancer serves as the frontend only, and proxies traffic to the specified serverless endpoint. However, if the backend service contains multiple serverless NEGs, the load balancer balances traffic between these NEGs, thus minimizing request latency.

Please read the following references for more information
Serverless Network EndPoint
Setting Up Serverless HTTP Load Balancing
Question 43: 
Skipped
Your company is hosting all its applications in single VPC and VPC has required firewall rules in place. Your company wants to use Firewall insights feature in the Google Network Intelligence Center to analyze the efficiency of the applied firewall ruleset. But network team did not find any row in Firewall Insight on Cloud Console.
How can this problem be fixed?
•	 
Use Google Cloud SDK to check Firewall logs.
•	 
Assigned the correct IAM role to users to check Firewall logs.
•	 
Enable Virtual Private Cloud (VPC) flow logging.
•	 
Enable Firewall Rules Logging for the firewall rules that you want to monitor.
(Correct)
Explanation
Firewall Insights enables you to better understand and safely optimize your firewall configurations. Firewall Insights provides reports that contain information about firewall usage and the impact of various firewall rules on your Virtual Private Cloud (VPC) network.
Firewall rule usage metrics are accurate only for the period of time during which Firewall Rules Logging is enabled.
Enabling Firewall Rules Logging
To see insights and usage metrics for firewall rules, you must enable Firewall Rules Logging for one or more firewall rules

Please read the following references for more information
Use Firewall Insight
Using Firewall Insights

Question 44: 
Skipped
Your team is developing an application that process and stores very sensitive documents. Team is considering Cloud Storage in Google Cloud to store these documents. Any change to these sensitive documents must be uploaded as a separate copy of the document.
For compliance requirements, how can to ensure that these documents cannot be deleted or overwritten for the next 7 years.
•	 
Generate customer-managed key in Cloud KMS.
Use above key for the encryption of the bucket data.
Rotate the key after 7 years.
•	 
Create the bucket with fine-grained access control.
Create a service account.
Assign role of Object Writer to above service account.
Use above created service account to upload new files.
•	 
Create a retention policy on the bucket for the duration of 7 years.
Create a lock on the retention policy.
(Correct)
•	 
Create the bucket with uniform bucket-level access control.
Create a service account.
Assign role of Object Writer to above service account.
Use above created service account to upload new files.
Explanation
Bucket Lock feature allows you to configure a data retention policy for a Cloud Storage bucket that governs how long objects in the bucket must be retained. The feature also allows you to lock the data retention policy, permanently preventing the policy from being reduced or removed.

This feature can provide immutable storage on Cloud Storage. In conjunction with Detailed audit logging mode, which logs Cloud Storage request and response details, Bucket Lock can help with regulatory and compliance requirements, such as those associated with FINRA, SEC, and CFTC. Bucket Lock may also help you address certain health care industry retention regulations.

You can add a retention policy to a bucket to specify a retention period.
1. If a bucket does not have a retention policy, you can delete or replace objects in the bucket at any time
2. If a bucket has a retention policy, objects in the bucket can only be deleted or replaced once their age is greater than the retention period.
3. A retention policy retroactively applies to existing objects in the bucket as well as new objects added to the bucket.

You can lock a retention policy to permanently set it on the bucket.
1. Once you lock a retention policy, you cannot remove it or reduce the retention period it has.
2. You cannot delete a bucket with a locked retention policy unless every object in the bucket has met the retention period.
3. You can increase the retention period of a locked retention policy.
4. Locking a retention policy can help your data comply with record retention regulations.

Please read the following references for more information
Bucket Lock
Question 45: 
Skipped
There is an existing Application which is running on Compute Engine instances which are managed via a Managed Instance Group.
Now you need to upgrade this application with some non critical update. You have created a new instance template for this non critical update.
How can you deploy this non critical update with no impact on existing VM instances? You need to ensure that only new VM instances has updated application running inside them.
•	 
Start a new rolling restart operation.
•	 
Start a new rolling replace operation.
•	 
Start a new rolling update with Proactive update mode.
•	 
Start a new rolling update with Opportunistic update mode.
(Correct)
Explanation
To automatically roll out new configuration to all or to a subset of the instances in a MIG, set the MIG's update type to PROACTIVE. If an automated update is potentially too disruptive, or you want more control over the update, set the MIG's update type to OPPORTUNISTIC then selectively update specific instances.
Opportunistic updates
When the update type is set to OPPORTUNISTIC, the MIG applies updates only when you selectively apply the update to specific instances or when new instances are created by the MIG. A MIG creates new instances when it is resized to add instances, either automatically or manually. Compute Engine does not actively initiate requests to apply opportunistic updates.
In certain scenarios, an opportunistic update is useful because you don't want to cause instability to the system if it can be avoided. For example, if you have a non-critical update that can be applied as necessary without any urgency and you have a MIG that is actively being autoscaled, perform an opportunistic update so that Compute Engine does not actively tear down your existing instances to apply the update. When resizing down, the autoscaler preferentially terminates instances with the old template as well as instances that are not yet in a RUNNING state.

Please read the following references for more information
Opportunistic Rolling Update
Choosing between automated and selective updates
Question 46: 
Skipped
One team in your company is developing a Web Application using Go 1.12. The expected load on this application is not predictable.
How would you like to deploy this application in GCP with minimum operation overhead? You also need to ensure availability of application during peak traffic as well.
•	 
Develop the application on App Engine standard environment.
(Correct)
•	 
Use a Managed Instance Group to deploy application.
•	 
Create Docker container for this application
Develop the application on App Engine flexible environment using above container.
•	 
Create Docker container for this application.
Deploy application on GKE cluster.
Explanation
Go1.12 is supported in App Engine standard environment so Option A is best choice to deploy application with minimum operational overhead for given use case.
Question 47: 
Skipped
One team in you company has deployed a business critical application on Athos cluster. This application must be highly available and should serve request with low latency.
What solution will you suggest to receive alerts if request latency of this critical application goes beyond a certain threshold value for specified period of time?
•	 
Enable the Cloud Trace API on your project.
Use Cloud Monitoring Alerts to send an alert based on the Cloud Trace metrics.
•	 
Configure Anthos Config Management on your cluster.
Create a yaml file that defines the SLO.
Create an alerting policy in your cluster.
•	 
Install Anthos Service Mesh on your cluster.
Define a Service Level Objective(SLO) on cloud console.
Create an alerting policy based on this SLO.
(Correct)
•	 
Use Cloud Profiler to track request latency.
Create a custom metric in Cloud Monitoring based on the request latency calculated in above step.
Create an Alerting policy in case this metric exceeds the threshold value.
Explanation
What is a service mesh?
A service mesh is an architecture that enables managed, observable, and secure communication across your services, letting you create robust enterprise applications made up of many microservices on your chosen infrastructure. Service meshes factor out all the common concerns of running a service such as monitoring, networking, and security, with consistent, powerful tools, making it easier for service developers and operators to focus on creating and managing great applications for their users.

The Anthos Service Mesh pages in the Google Cloud Console provides you to define Service level objectives (SLOs) that give you insight into the health of your services. You can easily define an SLO and alert on your own standards of service health.

Please read the following references for more information
Anthos Service Mesh
Question 48: 
Skipped
You are GCP practice head in your company. In your company, there are multiple departments and these departments need access to their own projects. Members within each department will have the same project responsibilities.
You want to set up your cloud environment in a way that require minimum maintenance.
You also need to ensure that IAM permissions are easy to set up as each department's project start and end.
What solution will you suggest for this requirement?
•	 
Create a folder for each department.
Assign the respective members of the department the required IAM permissions at the folder level.
Arrange all projects for each department under the respective folders.
•	 
Create a Google Group for each department.
Associate all department members with respective groups.
Assign each group the required IAM permissions for their respective projects.
•	 
Assign all department members the required IAM permissions for their respective projects.
•	 
Create a Google Group for each department.
Associate all department members to their respective groups.
Create a folder for each department.
Assign the respective group the required IAM permissions at the folder level.
Create the projects under the respective folders.
(Correct)
Explanation
Option A and Option C are incorrect because permissions are assigned to each member is individual member. Group is more recommended way to assign permission to members rather that giving permission to individual member.
Option B is incorrect because permission are granted at project level. Since all members in one department need same project responsibilities for all the projects so it make more sense to assign permission at folder level rather. So Option D is most appropriate choice for this.
Question 49: 
Skipped
One team in you company wants to develop a web application. This application has only few API which needs to be exposed to internet. User traffic will also be less for this application most of the time. Only on some rare occasions traffic cloud spike.
What solution will you suggest to deploy this application in cloud in cost effective way? You also need to ensure high availability of this application.
•	 
Store static content such as HTML and images in a GCS bucket.
Use Cloud Functions to host the APIs and save the user data in Firestore.
(Correct)
•	 
Use Cloud CDN to store static content such as HTML and images.
Host the APIs on App Engine.
Store the user data in Cloud SQL.
•	 
Store static content such as HTML and images in a GCS bucket.
Host the APIs on a zonal GKE cluster with worker nodes in multiple zones.
Save the user data in Cloud Spanner.
•	 
Use Cloud CDN to store static content such as HTML and images.
Use cloud Run to host the APIs.
Save the user data in Cloud SQL.
Explanation
Since traffic is very low, so serverless option is better choice in such case because in case of serverless deployment, there will be no charge if there is no traffic. So With this point, Both Option A and Option D looks like correct choice. Option B can also be correct choice if App Engine standard environment is used.
But I think Option A is best choice among all possible cases because only few number of API needs to be hosted. So Cloud Function will cost lesser in this case as compare to other choices.
Also Firestore is better option to store user data as compare to Cloud SQL.
Question 50: 
Skipped
Your company has developed an application for hospitals. This application need to store large amount of sensitive patient data in BigQuery.
As per some compliance requirements, you must delete this sensitive information upon request of the subject.
What solution will you suggest for this requirement?
•	 
Create a BigQuery view over the table that contains all data.
Exclude the rows that affect the subject's data from this view when deletion request is made.
Use this view instead of the source table for all analysis tasks.
•	 
Use a unique identifier for each individual.
When deletion request is received, overwrite the column with the unique identifier with a its hashed value.
•	 
When ingesting new data in BigQuery, run the data through the Data Loss Prevention (DLP) API to identify any personal information.
As part of the DLP scan, save the result to Data Catalog.
When deletion request is received, query Data Catalog to find the column with personal information.
(Correct)
•	 
Use a unique identifier for each individual.
When deletion request is received, delete all rows from BigQuery with this identifier.
Explanation
Cloud Data Loss Prevention (DLP) helps you find, understand, and manage the sensitive data that exists within your infrastructure.
DLP is a managed Google Cloud service designed to help you discover, classify, and protect your most sensitive data..
Once you've scanned your content for sensitive data using Cloud DLP, you have several options for what to do with that data intelligence like
1. Store Cloud DLP scan results directly in BigQuery.
2. Generate reports on where sensitive data resides in your infrastructure.
3.  Run rich SQL analytics to understand where sensitive data is stored and what kind it is.
4.  Automate alerts, or actions to trigger based on a single set or a combination of findings.

Please read the following references for more information
https://cloud.google.com/bigquery/docs/scan-with-dlp

